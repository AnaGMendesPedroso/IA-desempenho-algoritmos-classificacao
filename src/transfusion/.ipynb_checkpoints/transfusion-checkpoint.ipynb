{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Transfusion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "0                   2                 50                  12500   \n",
       "1                   0                 13                   3250   \n",
       "2                   1                 16                   4000   \n",
       "3                   2                 20                   5000   \n",
       "4                   1                 24                   6000   \n",
       "..                ...                ...                    ...   \n",
       "743                23                  2                    500   \n",
       "744                21                  2                    500   \n",
       "745                23                  3                    750   \n",
       "746                39                  1                    250   \n",
       "747                72                  1                    250   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "0               98                                           1  \n",
       "1               28                                           1  \n",
       "2               35                                           1  \n",
       "3               45                                           1  \n",
       "4               77                                           0  \n",
       "..             ...                                         ...  \n",
       "743             38                                           0  \n",
       "744             52                                           0  \n",
       "745             62                                           0  \n",
       "746             39                                           0  \n",
       "747             72                                           0  \n",
       "\n",
       "[748 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_transfusion = pd.read_table('/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/datasets/transfusion.data', sep=',')\n",
    "print(\"Dataset Transfusion\")\n",
    "dataset_transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Transfusion Normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.927899</td>\n",
       "      <td>7.623346</td>\n",
       "      <td>7.623346</td>\n",
       "      <td>2.615633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.175118</td>\n",
       "      <td>1.282738</td>\n",
       "      <td>1.282738</td>\n",
       "      <td>-0.257881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.051508</td>\n",
       "      <td>1.796842</td>\n",
       "      <td>1.796842</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.927899</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>0.439973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.051508</td>\n",
       "      <td>3.167784</td>\n",
       "      <td>3.167784</td>\n",
       "      <td>1.753579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>1.667904</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>0.152621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>1.420685</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>0.727324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>1.667904</td>\n",
       "      <td>-0.430940</td>\n",
       "      <td>-0.430940</td>\n",
       "      <td>1.137826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>3.645659</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>7.724778</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>1.548328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "0           -0.927899           7.623346               7.623346   \n",
       "1           -1.175118           1.282738               1.282738   \n",
       "2           -1.051508           1.796842               1.796842   \n",
       "3           -0.927899           2.482313               2.482313   \n",
       "4           -1.051508           3.167784               3.167784   \n",
       "..                ...                ...                    ...   \n",
       "743          1.667904          -0.602307              -0.602307   \n",
       "744          1.420685          -0.602307              -0.602307   \n",
       "745          1.667904          -0.430940              -0.430940   \n",
       "746          3.645659          -0.773675              -0.773675   \n",
       "747          7.724778          -0.773675              -0.773675   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "0         2.615633                                           1  \n",
       "1        -0.257881                                           1  \n",
       "2         0.029471                                           1  \n",
       "3         0.439973                                           1  \n",
       "4         1.753579                                           0  \n",
       "..             ...                                         ...  \n",
       "743       0.152621                                           0  \n",
       "744       0.727324                                           0  \n",
       "745       1.137826                                           0  \n",
       "746       0.193671                                           0  \n",
       "747       1.548328                                           0  \n",
       "\n",
       "[748 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfusion_normalised = dataset_transfusion.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "transfusion_normalised = normalizador.fit_transform(dataset_transfusion)\n",
    "dataset_transfusion['Recency (months)'] = transfusion_normalised[:,0]\n",
    "dataset_transfusion['Frequency (times)'] = transfusion_normalised[:,1]\n",
    "dataset_transfusion['Monetary (c.c. blood)'] = transfusion_normalised[:,2]\n",
    "dataset_transfusion['Time (months)'] = transfusion_normalised[:,3]\n",
    "\n",
    "print(\"Dataset Transfusion Normalized\")\n",
    "dataset_transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfusion features\n",
      "[[-0.92789873  7.62334626  7.62334626  2.61563344]\n",
      " [-1.17511806  1.28273826  1.28273826 -0.2578809 ]\n",
      " [-1.0515084   1.79684161  1.79684161  0.02947053]\n",
      " ...\n",
      " [ 1.66790417 -0.43093957 -0.43093957  1.13782607]\n",
      " [ 3.64565877 -0.77367514 -0.77367514  0.19367135]\n",
      " [ 7.72477762 -0.77367514 -0.77367514  1.54832812]]\n"
     ]
    }
   ],
   "source": [
    "transfusion_values = dataset_transfusion.iloc[:,0:4].values\n",
    "print(\"Transfusion features\")\n",
    "print(transfusion_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfusion classes\n",
      "[1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0\n",
      " 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Transfusion classes shape\n",
      "(748,)\n"
     ]
    }
   ],
   "source": [
    "transfusion_classes = dataset_transfusion.iloc[:,4].values\n",
    "print(\"Transfusion classes\")\n",
    "print(transfusion_classes)\n",
    "print(\"\\nTransfusion classes shape\")\n",
    "print(transfusion_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(transfusion_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60314242\n",
      "Iteration 2, loss = 0.59760617\n",
      "Iteration 3, loss = 0.58945039\n",
      "Iteration 4, loss = 0.58053324\n",
      "Iteration 5, loss = 0.57208552\n",
      "Iteration 6, loss = 0.56449890\n",
      "Iteration 7, loss = 0.55829882\n",
      "Iteration 8, loss = 0.55288764\n",
      "Iteration 9, loss = 0.54892256\n",
      "Iteration 10, loss = 0.54588041\n",
      "Iteration 11, loss = 0.54331607\n",
      "Iteration 12, loss = 0.54116776\n",
      "Iteration 13, loss = 0.53990635\n",
      "Iteration 14, loss = 0.53868107\n",
      "Iteration 15, loss = 0.53776121\n",
      "Iteration 16, loss = 0.53696658\n",
      "Iteration 17, loss = 0.53603485\n",
      "Iteration 18, loss = 0.53536417\n",
      "Iteration 19, loss = 0.53481328\n",
      "Iteration 20, loss = 0.53415467\n",
      "Iteration 21, loss = 0.53350419\n",
      "Iteration 22, loss = 0.53292362\n",
      "Iteration 23, loss = 0.53234291\n",
      "Iteration 24, loss = 0.53171529\n",
      "Iteration 25, loss = 0.53111413\n",
      "Iteration 26, loss = 0.53052101\n",
      "Iteration 27, loss = 0.52994036\n",
      "Iteration 28, loss = 0.52931881\n",
      "Iteration 29, loss = 0.52873340\n",
      "Iteration 30, loss = 0.52816669\n",
      "Iteration 31, loss = 0.52758564\n",
      "Iteration 32, loss = 0.52700795\n",
      "Iteration 33, loss = 0.52642894\n",
      "Iteration 34, loss = 0.52582479\n",
      "Iteration 35, loss = 0.52528273\n",
      "Iteration 36, loss = 0.52474402\n",
      "Iteration 37, loss = 0.52416239\n",
      "Iteration 38, loss = 0.52360359\n",
      "Iteration 39, loss = 0.52303350\n",
      "Iteration 40, loss = 0.52248597\n",
      "Iteration 41, loss = 0.52194757\n",
      "Iteration 42, loss = 0.52140531\n",
      "Iteration 43, loss = 0.52084695\n",
      "Iteration 44, loss = 0.52029610\n",
      "Iteration 45, loss = 0.51976421\n",
      "Iteration 46, loss = 0.51923976\n",
      "Iteration 47, loss = 0.51865891\n",
      "Iteration 48, loss = 0.51814063\n",
      "Iteration 49, loss = 0.51760154\n",
      "Iteration 50, loss = 0.51706198\n",
      "Iteration 51, loss = 0.51653066\n",
      "Iteration 52, loss = 0.51602560\n",
      "Iteration 53, loss = 0.51546494\n",
      "Iteration 54, loss = 0.51494774\n",
      "Iteration 55, loss = 0.51444815\n",
      "Iteration 56, loss = 0.51393458\n",
      "Iteration 57, loss = 0.51341356\n",
      "Iteration 58, loss = 0.51286291\n",
      "Iteration 59, loss = 0.51236027\n",
      "Iteration 60, loss = 0.51184497\n",
      "Iteration 61, loss = 0.51133733\n",
      "Iteration 62, loss = 0.51081876\n",
      "Iteration 63, loss = 0.51033743\n",
      "Iteration 64, loss = 0.50980244\n",
      "Iteration 65, loss = 0.50929188\n",
      "Iteration 66, loss = 0.50880732\n",
      "Iteration 67, loss = 0.50831210\n",
      "Iteration 68, loss = 0.50783267\n",
      "Iteration 69, loss = 0.50731005\n",
      "Iteration 70, loss = 0.50681134\n",
      "Iteration 71, loss = 0.50635319\n",
      "Iteration 72, loss = 0.50583000\n",
      "Iteration 73, loss = 0.50533607\n",
      "Iteration 74, loss = 0.50485491\n",
      "Iteration 75, loss = 0.50438723\n",
      "Iteration 76, loss = 0.50388866\n",
      "Iteration 77, loss = 0.50338000\n",
      "Iteration 78, loss = 0.50294029\n",
      "Iteration 79, loss = 0.50247584\n",
      "Iteration 80, loss = 0.50198044\n",
      "Iteration 81, loss = 0.50149204\n",
      "Iteration 82, loss = 0.50104376\n",
      "Iteration 83, loss = 0.50056803\n",
      "Iteration 84, loss = 0.50008379\n",
      "Iteration 85, loss = 0.49963884\n",
      "Iteration 86, loss = 0.49914845\n",
      "Iteration 87, loss = 0.49873299\n",
      "Iteration 88, loss = 0.49826907\n",
      "Iteration 89, loss = 0.49778114\n",
      "Iteration 90, loss = 0.49733863\n",
      "Iteration 91, loss = 0.49687449\n",
      "Iteration 92, loss = 0.49643845\n",
      "Iteration 93, loss = 0.49599740\n",
      "Iteration 94, loss = 0.49557484\n",
      "Iteration 95, loss = 0.49509147\n",
      "Iteration 96, loss = 0.49467487\n",
      "Iteration 97, loss = 0.49422324\n",
      "Iteration 98, loss = 0.49379813\n",
      "Iteration 99, loss = 0.49337935\n",
      "Iteration 100, loss = 0.49291156\n",
      "Iteration 101, loss = 0.49250062\n",
      "Iteration 102, loss = 0.49206108\n",
      "Iteration 103, loss = 0.49164003\n",
      "Iteration 104, loss = 0.49120227\n",
      "Iteration 105, loss = 0.49079222\n",
      "Iteration 106, loss = 0.49037874\n",
      "Iteration 107, loss = 0.48999384\n",
      "Iteration 108, loss = 0.48958438\n",
      "Iteration 109, loss = 0.48916439\n",
      "Iteration 110, loss = 0.48872707\n",
      "Iteration 111, loss = 0.48837204\n",
      "Iteration 112, loss = 0.48793241\n",
      "Iteration 113, loss = 0.48754386\n",
      "Iteration 114, loss = 0.48714550\n",
      "Iteration 115, loss = 0.48678023\n",
      "Iteration 116, loss = 0.48635727\n",
      "Iteration 117, loss = 0.48596641\n",
      "Iteration 118, loss = 0.48558016\n",
      "Iteration 119, loss = 0.48520740\n",
      "Iteration 120, loss = 0.48480491\n",
      "Iteration 121, loss = 0.48446852\n",
      "Iteration 122, loss = 0.48407157\n",
      "Iteration 123, loss = 0.48368943\n",
      "Iteration 124, loss = 0.48333912\n",
      "Iteration 125, loss = 0.48294917\n",
      "Iteration 126, loss = 0.48259802\n",
      "Iteration 127, loss = 0.48224351\n",
      "Iteration 128, loss = 0.48191169\n",
      "Iteration 129, loss = 0.48151132\n",
      "Iteration 130, loss = 0.48115428\n",
      "Iteration 131, loss = 0.48084548\n",
      "Iteration 132, loss = 0.48048473\n",
      "Iteration 133, loss = 0.48011977\n",
      "Iteration 134, loss = 0.47977383\n",
      "Iteration 135, loss = 0.47943888\n",
      "Iteration 136, loss = 0.47912333\n",
      "Iteration 137, loss = 0.47880202\n",
      "Iteration 138, loss = 0.47845809\n",
      "Iteration 139, loss = 0.47810117\n",
      "Iteration 140, loss = 0.47778576\n",
      "Iteration 141, loss = 0.47745479\n",
      "Iteration 142, loss = 0.47712226\n",
      "Iteration 143, loss = 0.47680269\n",
      "Iteration 144, loss = 0.47651360\n",
      "Iteration 145, loss = 0.47619744\n",
      "Iteration 146, loss = 0.47591678\n",
      "Iteration 147, loss = 0.47559385\n",
      "Iteration 148, loss = 0.47527731\n",
      "Iteration 149, loss = 0.47498685\n",
      "Iteration 150, loss = 0.47467171\n",
      "Iteration 151, loss = 0.47438487\n",
      "Iteration 152, loss = 0.47408088\n",
      "Iteration 153, loss = 0.47381872\n",
      "Iteration 154, loss = 0.47351868\n",
      "Iteration 155, loss = 0.47325603\n",
      "Iteration 156, loss = 0.47294809\n",
      "Iteration 157, loss = 0.47269650\n",
      "Iteration 158, loss = 0.47237211\n",
      "Iteration 159, loss = 0.47214960\n",
      "Iteration 160, loss = 0.47188308\n",
      "Iteration 161, loss = 0.47158478\n",
      "Iteration 162, loss = 0.47130247\n",
      "Iteration 163, loss = 0.47108216\n",
      "Iteration 164, loss = 0.47082626\n",
      "Iteration 165, loss = 0.47051957\n",
      "Iteration 166, loss = 0.47028417\n",
      "Iteration 167, loss = 0.47004593\n",
      "Iteration 168, loss = 0.46977460\n",
      "Iteration 169, loss = 0.46955220\n",
      "Iteration 170, loss = 0.46929028\n",
      "Iteration 171, loss = 0.46906956\n",
      "Iteration 172, loss = 0.46881384\n",
      "Iteration 173, loss = 0.46857999\n",
      "Iteration 174, loss = 0.46834620\n",
      "Iteration 175, loss = 0.46810913\n",
      "Iteration 176, loss = 0.46789709\n",
      "Iteration 177, loss = 0.46767747\n",
      "Iteration 178, loss = 0.46742196\n",
      "Iteration 179, loss = 0.46722395\n",
      "Iteration 180, loss = 0.46699643\n",
      "Iteration 181, loss = 0.46676106\n",
      "Iteration 182, loss = 0.46655031\n",
      "Iteration 183, loss = 0.46634564\n",
      "Iteration 184, loss = 0.46611606\n",
      "Iteration 185, loss = 0.46590538\n",
      "Iteration 186, loss = 0.46571945\n",
      "Iteration 187, loss = 0.46553035\n",
      "Iteration 188, loss = 0.46533552\n",
      "Iteration 189, loss = 0.46512945\n",
      "Iteration 190, loss = 0.46496630\n",
      "Iteration 191, loss = 0.46472009\n",
      "Iteration 192, loss = 0.46454704\n",
      "Iteration 193, loss = 0.46435778\n",
      "Iteration 194, loss = 0.46418646\n",
      "Iteration 195, loss = 0.46396478\n",
      "Iteration 196, loss = 0.46379954\n",
      "Iteration 197, loss = 0.46361561\n",
      "Iteration 198, loss = 0.46343033\n",
      "Iteration 199, loss = 0.46325621\n",
      "Iteration 200, loss = 0.46308346\n",
      "Iteration 201, loss = 0.46291917\n",
      "Iteration 202, loss = 0.46274772\n",
      "Iteration 203, loss = 0.46256195\n",
      "Iteration 204, loss = 0.46241270\n",
      "Iteration 205, loss = 0.46222199\n",
      "Iteration 206, loss = 0.46206755\n",
      "Iteration 207, loss = 0.46191706\n",
      "Iteration 208, loss = 0.46177849\n",
      "Iteration 209, loss = 0.46162488\n",
      "Iteration 210, loss = 0.46145875\n",
      "Iteration 211, loss = 0.46128953\n",
      "Iteration 212, loss = 0.46116400\n",
      "Iteration 213, loss = 0.46098591\n",
      "Iteration 214, loss = 0.46083297\n",
      "Iteration 215, loss = 0.46072316\n",
      "Iteration 216, loss = 0.46054516\n",
      "Iteration 217, loss = 0.46041039\n",
      "Iteration 218, loss = 0.46027429\n",
      "Iteration 219, loss = 0.46015856\n",
      "Iteration 220, loss = 0.45999349\n",
      "Iteration 221, loss = 0.45987186\n",
      "Iteration 222, loss = 0.45974124\n",
      "Iteration 223, loss = 0.45961164\n",
      "Iteration 224, loss = 0.45947126\n",
      "Iteration 225, loss = 0.45934222\n",
      "Iteration 226, loss = 0.45921652\n",
      "Iteration 227, loss = 0.45910659\n",
      "Iteration 228, loss = 0.45896624\n",
      "Iteration 229, loss = 0.45884128\n",
      "Iteration 230, loss = 0.45872556\n",
      "Iteration 231, loss = 0.45860895\n",
      "Iteration 232, loss = 0.45847991\n",
      "Iteration 233, loss = 0.45836757\n",
      "Iteration 234, loss = 0.45825443\n",
      "Iteration 235, loss = 0.45815153\n",
      "Iteration 236, loss = 0.45804914\n",
      "Iteration 237, loss = 0.45794331\n",
      "Iteration 238, loss = 0.45781876\n",
      "Iteration 239, loss = 0.45768797\n",
      "Iteration 240, loss = 0.45758818\n",
      "Iteration 241, loss = 0.45747855\n",
      "Iteration 242, loss = 0.45738630\n",
      "Iteration 243, loss = 0.45728168\n",
      "Iteration 244, loss = 0.45717288\n",
      "Iteration 245, loss = 0.45711736\n",
      "Iteration 246, loss = 0.45705167\n",
      "Iteration 247, loss = 0.45687906\n",
      "Iteration 248, loss = 0.45679344\n",
      "Iteration 249, loss = 0.45668765\n",
      "Iteration 250, loss = 0.45660294\n",
      "Iteration 251, loss = 0.45650122\n",
      "Iteration 252, loss = 0.45642716\n",
      "Iteration 253, loss = 0.45634828\n",
      "Iteration 254, loss = 0.45625608\n",
      "Iteration 255, loss = 0.45615196\n",
      "Iteration 256, loss = 0.45607781\n",
      "Iteration 257, loss = 0.45596557\n",
      "Iteration 258, loss = 0.45588033\n",
      "Iteration 259, loss = 0.45580241\n",
      "Iteration 260, loss = 0.45573723\n",
      "Iteration 261, loss = 0.45565759\n",
      "Iteration 262, loss = 0.45556118\n",
      "Iteration 263, loss = 0.45547889\n",
      "Iteration 264, loss = 0.45538947\n",
      "Iteration 265, loss = 0.45534010\n",
      "Iteration 266, loss = 0.45523818\n",
      "Iteration 267, loss = 0.45521225\n",
      "Iteration 268, loss = 0.45512351\n",
      "Iteration 269, loss = 0.45502203\n",
      "Iteration 270, loss = 0.45498267\n",
      "Iteration 271, loss = 0.45488006\n",
      "Iteration 272, loss = 0.45481180\n",
      "Iteration 273, loss = 0.45474964\n",
      "Iteration 274, loss = 0.45466770\n",
      "Iteration 275, loss = 0.45462451\n",
      "Iteration 276, loss = 0.45454515\n",
      "Iteration 277, loss = 0.45446033\n",
      "Iteration 278, loss = 0.45439625\n",
      "Iteration 279, loss = 0.45435064\n",
      "Iteration 280, loss = 0.45430622\n",
      "Iteration 281, loss = 0.45423735\n",
      "Iteration 282, loss = 0.45415951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93410525\n",
      "Iteration 2, loss = 0.89317717\n",
      "Iteration 3, loss = 0.83651242\n",
      "Iteration 4, loss = 0.77822304\n",
      "Iteration 5, loss = 0.72262571\n",
      "Iteration 6, loss = 0.67555075\n",
      "Iteration 7, loss = 0.63682865\n",
      "Iteration 8, loss = 0.61122078\n",
      "Iteration 9, loss = 0.59015790\n",
      "Iteration 10, loss = 0.57827196\n",
      "Iteration 11, loss = 0.56843631\n",
      "Iteration 12, loss = 0.56403628\n",
      "Iteration 13, loss = 0.55993894\n",
      "Iteration 14, loss = 0.55764576\n",
      "Iteration 15, loss = 0.55676290\n",
      "Iteration 16, loss = 0.55611272\n",
      "Iteration 17, loss = 0.55542436\n",
      "Iteration 18, loss = 0.55513332\n",
      "Iteration 19, loss = 0.55470237\n",
      "Iteration 20, loss = 0.55432841\n",
      "Iteration 21, loss = 0.55402255\n",
      "Iteration 22, loss = 0.55368053\n",
      "Iteration 23, loss = 0.55326145\n",
      "Iteration 24, loss = 0.55288502\n",
      "Iteration 25, loss = 0.55245181\n",
      "Iteration 26, loss = 0.55207717\n",
      "Iteration 27, loss = 0.55172552\n",
      "Iteration 28, loss = 0.55134647\n",
      "Iteration 29, loss = 0.55097799\n",
      "Iteration 30, loss = 0.55060593\n",
      "Iteration 31, loss = 0.55024581\n",
      "Iteration 32, loss = 0.54989009\n",
      "Iteration 33, loss = 0.54957765\n",
      "Iteration 34, loss = 0.54922366\n",
      "Iteration 35, loss = 0.54887587\n",
      "Iteration 36, loss = 0.54856943\n",
      "Iteration 37, loss = 0.54833490\n",
      "Iteration 38, loss = 0.54792961\n",
      "Iteration 39, loss = 0.54769035\n",
      "Iteration 40, loss = 0.54735215\n",
      "Iteration 41, loss = 0.54700564\n",
      "Iteration 42, loss = 0.54668000\n",
      "Iteration 43, loss = 0.54641920\n",
      "Iteration 44, loss = 0.54608917\n",
      "Iteration 45, loss = 0.54581898\n",
      "Iteration 46, loss = 0.54548935\n",
      "Iteration 47, loss = 0.54522139\n",
      "Iteration 48, loss = 0.54489880\n",
      "Iteration 49, loss = 0.54462097\n",
      "Iteration 50, loss = 0.54434240\n",
      "Iteration 51, loss = 0.54403142\n",
      "Iteration 52, loss = 0.54377184\n",
      "Iteration 53, loss = 0.54349396\n",
      "Iteration 54, loss = 0.54317770\n",
      "Iteration 55, loss = 0.54293177\n",
      "Iteration 56, loss = 0.54261848\n",
      "Iteration 57, loss = 0.54233698\n",
      "Iteration 58, loss = 0.54205245\n",
      "Iteration 59, loss = 0.54180189\n",
      "Iteration 60, loss = 0.54153508\n",
      "Iteration 61, loss = 0.54123872\n",
      "Iteration 62, loss = 0.54093484\n",
      "Iteration 63, loss = 0.54068597\n",
      "Iteration 64, loss = 0.54038420\n",
      "Iteration 65, loss = 0.54010089\n",
      "Iteration 66, loss = 0.53984317\n",
      "Iteration 67, loss = 0.53957426\n",
      "Iteration 68, loss = 0.53931540\n",
      "Iteration 69, loss = 0.53900209\n",
      "Iteration 70, loss = 0.53872043\n",
      "Iteration 71, loss = 0.53842168\n",
      "Iteration 72, loss = 0.53817153\n",
      "Iteration 73, loss = 0.53787229\n",
      "Iteration 74, loss = 0.53765442\n",
      "Iteration 75, loss = 0.53734127\n",
      "Iteration 76, loss = 0.53706604\n",
      "Iteration 77, loss = 0.53676434\n",
      "Iteration 78, loss = 0.53649404\n",
      "Iteration 79, loss = 0.53620743\n",
      "Iteration 80, loss = 0.53592238\n",
      "Iteration 81, loss = 0.53566350\n",
      "Iteration 82, loss = 0.53538525\n",
      "Iteration 83, loss = 0.53508803\n",
      "Iteration 84, loss = 0.53481389\n",
      "Iteration 85, loss = 0.53452659\n",
      "Iteration 86, loss = 0.53426221\n",
      "Iteration 87, loss = 0.53396177\n",
      "Iteration 88, loss = 0.53366678\n",
      "Iteration 89, loss = 0.53340817\n",
      "Iteration 90, loss = 0.53313661\n",
      "Iteration 91, loss = 0.53282786\n",
      "Iteration 92, loss = 0.53256217\n",
      "Iteration 93, loss = 0.53230313\n",
      "Iteration 94, loss = 0.53199699\n",
      "Iteration 95, loss = 0.53167681\n",
      "Iteration 96, loss = 0.53138664\n",
      "Iteration 97, loss = 0.53111388\n",
      "Iteration 98, loss = 0.53081205\n",
      "Iteration 99, loss = 0.53053617\n",
      "Iteration 100, loss = 0.53027026\n",
      "Iteration 101, loss = 0.52994332\n",
      "Iteration 102, loss = 0.52963969\n",
      "Iteration 103, loss = 0.52935361\n",
      "Iteration 104, loss = 0.52907740\n",
      "Iteration 105, loss = 0.52882513\n",
      "Iteration 106, loss = 0.52850329\n",
      "Iteration 107, loss = 0.52819981\n",
      "Iteration 108, loss = 0.52789576\n",
      "Iteration 109, loss = 0.52759856\n",
      "Iteration 110, loss = 0.52730660\n",
      "Iteration 111, loss = 0.52699059\n",
      "Iteration 112, loss = 0.52671134\n",
      "Iteration 113, loss = 0.52642472\n",
      "Iteration 114, loss = 0.52612873\n",
      "Iteration 115, loss = 0.52581181\n",
      "Iteration 116, loss = 0.52554539\n",
      "Iteration 117, loss = 0.52522777\n",
      "Iteration 118, loss = 0.52492817\n",
      "Iteration 119, loss = 0.52461895\n",
      "Iteration 120, loss = 0.52431909\n",
      "Iteration 121, loss = 0.52400832\n",
      "Iteration 122, loss = 0.52370509\n",
      "Iteration 123, loss = 0.52344656\n",
      "Iteration 124, loss = 0.52313588\n",
      "Iteration 125, loss = 0.52279937\n",
      "Iteration 126, loss = 0.52250574\n",
      "Iteration 127, loss = 0.52221174\n",
      "Iteration 128, loss = 0.52195817\n",
      "Iteration 129, loss = 0.52163859\n",
      "Iteration 130, loss = 0.52129902\n",
      "Iteration 131, loss = 0.52102947\n",
      "Iteration 132, loss = 0.52070905\n",
      "Iteration 133, loss = 0.52040120\n",
      "Iteration 134, loss = 0.52008552\n",
      "Iteration 135, loss = 0.51977224\n",
      "Iteration 136, loss = 0.51950315\n",
      "Iteration 137, loss = 0.51918367\n",
      "Iteration 138, loss = 0.51888696\n",
      "Iteration 139, loss = 0.51858259\n",
      "Iteration 140, loss = 0.51829397\n",
      "Iteration 141, loss = 0.51797465\n",
      "Iteration 142, loss = 0.51768462\n",
      "Iteration 143, loss = 0.51740390\n",
      "Iteration 144, loss = 0.51710842\n",
      "Iteration 145, loss = 0.51678088\n",
      "Iteration 146, loss = 0.51648916\n",
      "Iteration 147, loss = 0.51616278\n",
      "Iteration 148, loss = 0.51588356\n",
      "Iteration 149, loss = 0.51561296\n",
      "Iteration 150, loss = 0.51527393\n",
      "Iteration 151, loss = 0.51501043\n",
      "Iteration 152, loss = 0.51474353\n",
      "Iteration 153, loss = 0.51440183\n",
      "Iteration 154, loss = 0.51408442\n",
      "Iteration 155, loss = 0.51381241\n",
      "Iteration 156, loss = 0.51352814\n",
      "Iteration 157, loss = 0.51321883\n",
      "Iteration 158, loss = 0.51292611\n",
      "Iteration 159, loss = 0.51264302\n",
      "Iteration 160, loss = 0.51235598\n",
      "Iteration 161, loss = 0.51205175\n",
      "Iteration 162, loss = 0.51176392\n",
      "Iteration 163, loss = 0.51147752\n",
      "Iteration 164, loss = 0.51118688\n",
      "Iteration 165, loss = 0.51089311\n",
      "Iteration 166, loss = 0.51062103\n",
      "Iteration 167, loss = 0.51035498\n",
      "Iteration 168, loss = 0.51006459\n",
      "Iteration 169, loss = 0.50977136\n",
      "Iteration 170, loss = 0.50949034\n",
      "Iteration 171, loss = 0.50921490\n",
      "Iteration 172, loss = 0.50891643\n",
      "Iteration 173, loss = 0.50865170\n",
      "Iteration 174, loss = 0.50839180\n",
      "Iteration 175, loss = 0.50810695\n",
      "Iteration 176, loss = 0.50784313\n",
      "Iteration 177, loss = 0.50756251\n",
      "Iteration 178, loss = 0.50727532\n",
      "Iteration 179, loss = 0.50702083\n",
      "Iteration 180, loss = 0.50675629\n",
      "Iteration 181, loss = 0.50646805\n",
      "Iteration 182, loss = 0.50621878\n",
      "Iteration 183, loss = 0.50593466\n",
      "Iteration 184, loss = 0.50566313\n",
      "Iteration 185, loss = 0.50541522\n",
      "Iteration 186, loss = 0.50514108\n",
      "Iteration 187, loss = 0.50488996\n",
      "Iteration 188, loss = 0.50464355\n",
      "Iteration 189, loss = 0.50435745\n",
      "Iteration 190, loss = 0.50413514\n",
      "Iteration 191, loss = 0.50390619\n",
      "Iteration 192, loss = 0.50362972\n",
      "Iteration 193, loss = 0.50335987\n",
      "Iteration 194, loss = 0.50312796\n",
      "Iteration 195, loss = 0.50288447\n",
      "Iteration 196, loss = 0.50262956\n",
      "Iteration 197, loss = 0.50239484\n",
      "Iteration 198, loss = 0.50215629\n",
      "Iteration 199, loss = 0.50189960\n",
      "Iteration 200, loss = 0.50165323\n",
      "Iteration 201, loss = 0.50142676\n",
      "Iteration 202, loss = 0.50120203\n",
      "Iteration 203, loss = 0.50096198\n",
      "Iteration 204, loss = 0.50072817\n",
      "Iteration 205, loss = 0.50050565\n",
      "Iteration 206, loss = 0.50027003\n",
      "Iteration 207, loss = 0.50003691\n",
      "Iteration 208, loss = 0.49981962\n",
      "Iteration 209, loss = 0.49958941\n",
      "Iteration 210, loss = 0.49939616\n",
      "Iteration 211, loss = 0.49914401\n",
      "Iteration 212, loss = 0.49891376\n",
      "Iteration 213, loss = 0.49871694\n",
      "Iteration 214, loss = 0.49851064\n",
      "Iteration 215, loss = 0.49827822\n",
      "Iteration 216, loss = 0.49810594\n",
      "Iteration 217, loss = 0.49787229\n",
      "Iteration 218, loss = 0.49766993\n",
      "Iteration 219, loss = 0.49744640\n",
      "Iteration 220, loss = 0.49723458\n",
      "Iteration 221, loss = 0.49705439\n",
      "Iteration 222, loss = 0.49687215\n",
      "Iteration 223, loss = 0.49665205\n",
      "Iteration 224, loss = 0.49648051\n",
      "Iteration 225, loss = 0.49623650\n",
      "Iteration 226, loss = 0.49606494\n",
      "Iteration 227, loss = 0.49587497\n",
      "Iteration 228, loss = 0.49568887\n",
      "Iteration 229, loss = 0.49549916\n",
      "Iteration 230, loss = 0.49529139\n",
      "Iteration 231, loss = 0.49513030\n",
      "Iteration 232, loss = 0.49495218\n",
      "Iteration 233, loss = 0.49478546\n",
      "Iteration 234, loss = 0.49459758\n",
      "Iteration 235, loss = 0.49442715\n",
      "Iteration 236, loss = 0.49424365\n",
      "Iteration 237, loss = 0.49404547\n",
      "Iteration 238, loss = 0.49386232\n",
      "Iteration 239, loss = 0.49370745\n",
      "Iteration 240, loss = 0.49354389\n",
      "Iteration 241, loss = 0.49339323\n",
      "Iteration 242, loss = 0.49319793\n",
      "Iteration 243, loss = 0.49304920\n",
      "Iteration 244, loss = 0.49287555\n",
      "Iteration 245, loss = 0.49270893\n",
      "Iteration 246, loss = 0.49256350\n",
      "Iteration 247, loss = 0.49244896\n",
      "Iteration 248, loss = 0.49226524\n",
      "Iteration 249, loss = 0.49208512\n",
      "Iteration 250, loss = 0.49193420\n",
      "Iteration 251, loss = 0.49182030\n",
      "Iteration 252, loss = 0.49167704\n",
      "Iteration 253, loss = 0.49149821\n",
      "Iteration 254, loss = 0.49136378\n",
      "Iteration 255, loss = 0.49121408\n",
      "Iteration 256, loss = 0.49106401\n",
      "Iteration 257, loss = 0.49097104\n",
      "Iteration 258, loss = 0.49079815\n",
      "Iteration 259, loss = 0.49071000\n",
      "Iteration 260, loss = 0.49053390\n",
      "Iteration 261, loss = 0.49035940\n",
      "Iteration 262, loss = 0.49025513\n",
      "Iteration 263, loss = 0.49012011\n",
      "Iteration 264, loss = 0.48999309\n",
      "Iteration 265, loss = 0.48987907\n",
      "Iteration 266, loss = 0.48973449\n",
      "Iteration 267, loss = 0.48962093\n",
      "Iteration 268, loss = 0.48950024\n",
      "Iteration 269, loss = 0.48938272\n",
      "Iteration 270, loss = 0.48924592\n",
      "Iteration 271, loss = 0.48913347\n",
      "Iteration 272, loss = 0.48902097\n",
      "Iteration 273, loss = 0.48897868\n",
      "Iteration 274, loss = 0.48878094\n",
      "Iteration 275, loss = 0.48869427\n",
      "Iteration 276, loss = 0.48858987\n",
      "Iteration 277, loss = 0.48846174\n",
      "Iteration 278, loss = 0.48835861\n",
      "Iteration 279, loss = 0.48822868\n",
      "Iteration 280, loss = 0.48816646\n",
      "Iteration 281, loss = 0.48804599\n",
      "Iteration 282, loss = 0.48793226\n",
      "Iteration 283, loss = 0.48782891\n",
      "Iteration 284, loss = 0.48772591\n",
      "Iteration 285, loss = 0.48764536\n",
      "Iteration 286, loss = 0.48755785\n",
      "Iteration 287, loss = 0.48747650\n",
      "Iteration 288, loss = 0.48734625\n",
      "Iteration 289, loss = 0.48723448\n",
      "Iteration 290, loss = 0.48720092\n",
      "Iteration 291, loss = 0.48706458\n",
      "Iteration 292, loss = 0.48698157\n",
      "Iteration 293, loss = 0.48689161\n",
      "Iteration 294, loss = 0.48680615\n",
      "Iteration 295, loss = 0.48671624\n",
      "Iteration 296, loss = 0.48661463\n",
      "Iteration 297, loss = 0.48657303\n",
      "Iteration 298, loss = 0.48645069\n",
      "Iteration 299, loss = 0.48639613\n",
      "Iteration 300, loss = 0.48629116\n",
      "Iteration 1, loss = 0.57459642\n",
      "Iteration 2, loss = 0.57218949\n",
      "Iteration 3, loss = 0.56842407\n",
      "Iteration 4, loss = 0.56416387\n",
      "Iteration 5, loss = 0.56051226\n",
      "Iteration 6, loss = 0.55679010\n",
      "Iteration 7, loss = 0.55443289\n",
      "Iteration 8, loss = 0.55194076\n",
      "Iteration 9, loss = 0.55000616\n",
      "Iteration 10, loss = 0.54866253\n",
      "Iteration 11, loss = 0.54751525\n",
      "Iteration 12, loss = 0.54668323\n",
      "Iteration 13, loss = 0.54581326\n",
      "Iteration 14, loss = 0.54533145\n",
      "Iteration 15, loss = 0.54495432\n",
      "Iteration 16, loss = 0.54455684\n",
      "Iteration 17, loss = 0.54419397\n",
      "Iteration 18, loss = 0.54378563\n",
      "Iteration 19, loss = 0.54347638\n",
      "Iteration 20, loss = 0.54315512\n",
      "Iteration 21, loss = 0.54279371\n",
      "Iteration 22, loss = 0.54249515\n",
      "Iteration 23, loss = 0.54218274\n",
      "Iteration 24, loss = 0.54189911\n",
      "Iteration 25, loss = 0.54157644\n",
      "Iteration 26, loss = 0.54124217\n",
      "Iteration 27, loss = 0.54092961\n",
      "Iteration 28, loss = 0.54060666\n",
      "Iteration 29, loss = 0.54030620\n",
      "Iteration 30, loss = 0.54000992\n",
      "Iteration 31, loss = 0.53971698\n",
      "Iteration 32, loss = 0.53942040\n",
      "Iteration 33, loss = 0.53912331\n",
      "Iteration 34, loss = 0.53883207\n",
      "Iteration 35, loss = 0.53852708\n",
      "Iteration 36, loss = 0.53824998\n",
      "Iteration 37, loss = 0.53803256\n",
      "Iteration 38, loss = 0.53770964\n",
      "Iteration 39, loss = 0.53741635\n",
      "Iteration 40, loss = 0.53712185\n",
      "Iteration 41, loss = 0.53686498\n",
      "Iteration 42, loss = 0.53656934\n",
      "Iteration 43, loss = 0.53630690\n",
      "Iteration 44, loss = 0.53608818\n",
      "Iteration 45, loss = 0.53574456\n",
      "Iteration 46, loss = 0.53550632\n",
      "Iteration 47, loss = 0.53520263\n",
      "Iteration 48, loss = 0.53495944\n",
      "Iteration 49, loss = 0.53469303\n",
      "Iteration 50, loss = 0.53444372\n",
      "Iteration 51, loss = 0.53415232\n",
      "Iteration 52, loss = 0.53390566\n",
      "Iteration 53, loss = 0.53366218\n",
      "Iteration 54, loss = 0.53338794\n",
      "Iteration 55, loss = 0.53313093\n",
      "Iteration 56, loss = 0.53287391\n",
      "Iteration 57, loss = 0.53261151\n",
      "Iteration 58, loss = 0.53237409\n",
      "Iteration 59, loss = 0.53212500\n",
      "Iteration 60, loss = 0.53184670\n",
      "Iteration 61, loss = 0.53158656\n",
      "Iteration 62, loss = 0.53134679\n",
      "Iteration 63, loss = 0.53109485\n",
      "Iteration 64, loss = 0.53084390\n",
      "Iteration 65, loss = 0.53059546\n",
      "Iteration 66, loss = 0.53033991\n",
      "Iteration 67, loss = 0.53011601\n",
      "Iteration 68, loss = 0.52991533\n",
      "Iteration 69, loss = 0.52960185\n",
      "Iteration 70, loss = 0.52935799\n",
      "Iteration 71, loss = 0.52913229\n",
      "Iteration 72, loss = 0.52893375\n",
      "Iteration 73, loss = 0.52863354\n",
      "Iteration 74, loss = 0.52837682\n",
      "Iteration 75, loss = 0.52815913\n",
      "Iteration 76, loss = 0.52791307\n",
      "Iteration 77, loss = 0.52769172\n",
      "Iteration 78, loss = 0.52739997\n",
      "Iteration 79, loss = 0.52717733\n",
      "Iteration 80, loss = 0.52692774\n",
      "Iteration 81, loss = 0.52670166\n",
      "Iteration 82, loss = 0.52643747\n",
      "Iteration 83, loss = 0.52623222\n",
      "Iteration 84, loss = 0.52596395\n",
      "Iteration 85, loss = 0.52577634\n",
      "Iteration 86, loss = 0.52550743\n",
      "Iteration 87, loss = 0.52527850\n",
      "Iteration 88, loss = 0.52504407\n",
      "Iteration 89, loss = 0.52476992\n",
      "Iteration 90, loss = 0.52455153\n",
      "Iteration 91, loss = 0.52433022\n",
      "Iteration 92, loss = 0.52408016\n",
      "Iteration 93, loss = 0.52385260\n",
      "Iteration 94, loss = 0.52359700\n",
      "Iteration 95, loss = 0.52336111\n",
      "Iteration 96, loss = 0.52312673\n",
      "Iteration 97, loss = 0.52289973\n",
      "Iteration 98, loss = 0.52265307\n",
      "Iteration 99, loss = 0.52242627\n",
      "Iteration 100, loss = 0.52220688\n",
      "Iteration 101, loss = 0.52197622\n",
      "Iteration 102, loss = 0.52174571\n",
      "Iteration 103, loss = 0.52153410\n",
      "Iteration 104, loss = 0.52125761\n",
      "Iteration 105, loss = 0.52102127\n",
      "Iteration 106, loss = 0.52080270\n",
      "Iteration 107, loss = 0.52057321\n",
      "Iteration 108, loss = 0.52032712\n",
      "Iteration 109, loss = 0.52010296\n",
      "Iteration 110, loss = 0.51986787\n",
      "Iteration 111, loss = 0.51964145\n",
      "Iteration 112, loss = 0.51948540\n",
      "Iteration 113, loss = 0.51919449\n",
      "Iteration 114, loss = 0.51896655\n",
      "Iteration 115, loss = 0.51870780\n",
      "Iteration 116, loss = 0.51848534\n",
      "Iteration 117, loss = 0.51826888\n",
      "Iteration 118, loss = 0.51803545\n",
      "Iteration 119, loss = 0.51778972\n",
      "Iteration 120, loss = 0.51758554\n",
      "Iteration 121, loss = 0.51735447\n",
      "Iteration 122, loss = 0.51712670\n",
      "Iteration 123, loss = 0.51689605\n",
      "Iteration 124, loss = 0.51666771\n",
      "Iteration 125, loss = 0.51648066\n",
      "Iteration 126, loss = 0.51619986\n",
      "Iteration 127, loss = 0.51598821\n",
      "Iteration 128, loss = 0.51575359\n",
      "Iteration 129, loss = 0.51553979\n",
      "Iteration 130, loss = 0.51531063\n",
      "Iteration 131, loss = 0.51509243\n",
      "Iteration 132, loss = 0.51484520\n",
      "Iteration 133, loss = 0.51464448\n",
      "Iteration 134, loss = 0.51444268\n",
      "Iteration 135, loss = 0.51419738\n",
      "Iteration 136, loss = 0.51396901\n",
      "Iteration 137, loss = 0.51374205\n",
      "Iteration 138, loss = 0.51351630\n",
      "Iteration 139, loss = 0.51329268\n",
      "Iteration 140, loss = 0.51308610\n",
      "Iteration 141, loss = 0.51285866\n",
      "Iteration 142, loss = 0.51263779\n",
      "Iteration 143, loss = 0.51241208\n",
      "Iteration 144, loss = 0.51221137\n",
      "Iteration 145, loss = 0.51199250\n",
      "Iteration 146, loss = 0.51176162\n",
      "Iteration 147, loss = 0.51155978\n",
      "Iteration 148, loss = 0.51134377\n",
      "Iteration 149, loss = 0.51108226\n",
      "Iteration 150, loss = 0.51087566\n",
      "Iteration 151, loss = 0.51065452\n",
      "Iteration 152, loss = 0.51044350\n",
      "Iteration 153, loss = 0.51021494\n",
      "Iteration 154, loss = 0.51002628\n",
      "Iteration 155, loss = 0.50980810\n",
      "Iteration 156, loss = 0.50959076\n",
      "Iteration 157, loss = 0.50939887\n",
      "Iteration 158, loss = 0.50914226\n",
      "Iteration 159, loss = 0.50893246\n",
      "Iteration 160, loss = 0.50871140\n",
      "Iteration 161, loss = 0.50850134\n",
      "Iteration 162, loss = 0.50830649\n",
      "Iteration 163, loss = 0.50807054\n",
      "Iteration 164, loss = 0.50786664\n",
      "Iteration 165, loss = 0.50764467\n",
      "Iteration 166, loss = 0.50747639\n",
      "Iteration 167, loss = 0.50723770\n",
      "Iteration 168, loss = 0.50701256\n",
      "Iteration 169, loss = 0.50681786\n",
      "Iteration 170, loss = 0.50660929\n",
      "Iteration 171, loss = 0.50639642\n",
      "Iteration 172, loss = 0.50618530\n",
      "Iteration 173, loss = 0.50598847\n",
      "Iteration 174, loss = 0.50578707\n",
      "Iteration 175, loss = 0.50557969\n",
      "Iteration 176, loss = 0.50534995\n",
      "Iteration 177, loss = 0.50515954\n",
      "Iteration 178, loss = 0.50495481\n",
      "Iteration 179, loss = 0.50473765\n",
      "Iteration 180, loss = 0.50454475\n",
      "Iteration 181, loss = 0.50433911\n",
      "Iteration 182, loss = 0.50413407\n",
      "Iteration 183, loss = 0.50393203\n",
      "Iteration 184, loss = 0.50372837\n",
      "Iteration 185, loss = 0.50353501\n",
      "Iteration 186, loss = 0.50333060\n",
      "Iteration 187, loss = 0.50313555\n",
      "Iteration 188, loss = 0.50291422\n",
      "Iteration 189, loss = 0.50273621\n",
      "Iteration 190, loss = 0.50255435\n",
      "Iteration 191, loss = 0.50232526\n",
      "Iteration 192, loss = 0.50212333\n",
      "Iteration 193, loss = 0.50194202\n",
      "Iteration 194, loss = 0.50172091\n",
      "Iteration 195, loss = 0.50154952\n",
      "Iteration 196, loss = 0.50136174\n",
      "Iteration 197, loss = 0.50115817\n",
      "Iteration 198, loss = 0.50095703\n",
      "Iteration 199, loss = 0.50077643\n",
      "Iteration 200, loss = 0.50056376\n",
      "Iteration 201, loss = 0.50037788\n",
      "Iteration 202, loss = 0.50019292\n",
      "Iteration 203, loss = 0.50002690\n",
      "Iteration 204, loss = 0.49979118\n",
      "Iteration 205, loss = 0.49961874\n",
      "Iteration 206, loss = 0.49943659\n",
      "Iteration 207, loss = 0.49923410\n",
      "Iteration 208, loss = 0.49905517\n",
      "Iteration 209, loss = 0.49885447\n",
      "Iteration 210, loss = 0.49867633\n",
      "Iteration 211, loss = 0.49849969\n",
      "Iteration 212, loss = 0.49832215\n",
      "Iteration 213, loss = 0.49812020\n",
      "Iteration 214, loss = 0.49796235\n",
      "Iteration 215, loss = 0.49774775\n",
      "Iteration 216, loss = 0.49756287\n",
      "Iteration 217, loss = 0.49742333\n",
      "Iteration 218, loss = 0.49718879\n",
      "Iteration 219, loss = 0.49701841\n",
      "Iteration 220, loss = 0.49685628\n",
      "Iteration 221, loss = 0.49666124\n",
      "Iteration 222, loss = 0.49648567\n",
      "Iteration 223, loss = 0.49631643\n",
      "Iteration 224, loss = 0.49612109\n",
      "Iteration 225, loss = 0.49595315\n",
      "Iteration 226, loss = 0.49581272\n",
      "Iteration 227, loss = 0.49558899\n",
      "Iteration 228, loss = 0.49541111\n",
      "Iteration 229, loss = 0.49524295\n",
      "Iteration 230, loss = 0.49507787\n",
      "Iteration 231, loss = 0.49490893\n",
      "Iteration 232, loss = 0.49473355\n",
      "Iteration 233, loss = 0.49459364\n",
      "Iteration 234, loss = 0.49439021\n",
      "Iteration 235, loss = 0.49423685\n",
      "Iteration 236, loss = 0.49404075\n",
      "Iteration 237, loss = 0.49386011\n",
      "Iteration 238, loss = 0.49371326\n",
      "Iteration 239, loss = 0.49353512\n",
      "Iteration 240, loss = 0.49337989\n",
      "Iteration 241, loss = 0.49322269\n",
      "Iteration 242, loss = 0.49305532\n",
      "Iteration 243, loss = 0.49289552\n",
      "Iteration 244, loss = 0.49273133\n",
      "Iteration 245, loss = 0.49254154\n",
      "Iteration 246, loss = 0.49237102\n",
      "Iteration 247, loss = 0.49222375\n",
      "Iteration 248, loss = 0.49205311\n",
      "Iteration 249, loss = 0.49189797\n",
      "Iteration 250, loss = 0.49174824\n",
      "Iteration 251, loss = 0.49159479\n",
      "Iteration 252, loss = 0.49144150\n",
      "Iteration 253, loss = 0.49127029\n",
      "Iteration 254, loss = 0.49111098\n",
      "Iteration 255, loss = 0.49093383\n",
      "Iteration 256, loss = 0.49082587\n",
      "Iteration 257, loss = 0.49064625\n",
      "Iteration 258, loss = 0.49048523\n",
      "Iteration 259, loss = 0.49033319\n",
      "Iteration 260, loss = 0.49016583\n",
      "Iteration 261, loss = 0.49001952\n",
      "Iteration 262, loss = 0.48985657\n",
      "Iteration 263, loss = 0.48972214\n",
      "Iteration 264, loss = 0.48956547\n",
      "Iteration 265, loss = 0.48941025\n",
      "Iteration 266, loss = 0.48927204\n",
      "Iteration 267, loss = 0.48912476\n",
      "Iteration 268, loss = 0.48896840\n",
      "Iteration 269, loss = 0.48882528\n",
      "Iteration 270, loss = 0.48866985\n",
      "Iteration 271, loss = 0.48851898\n",
      "Iteration 272, loss = 0.48839442\n",
      "Iteration 273, loss = 0.48827852\n",
      "Iteration 274, loss = 0.48811124\n",
      "Iteration 275, loss = 0.48798486\n",
      "Iteration 276, loss = 0.48782211\n",
      "Iteration 277, loss = 0.48766210\n",
      "Iteration 278, loss = 0.48753938\n",
      "Iteration 279, loss = 0.48739682\n",
      "Iteration 280, loss = 0.48725386\n",
      "Iteration 281, loss = 0.48711230\n",
      "Iteration 282, loss = 0.48698902\n",
      "Iteration 283, loss = 0.48687190\n",
      "Iteration 284, loss = 0.48672748\n",
      "Iteration 285, loss = 0.48658572\n",
      "Iteration 286, loss = 0.48641487\n",
      "Iteration 287, loss = 0.48633563\n",
      "Iteration 288, loss = 0.48615904\n",
      "Iteration 289, loss = 0.48602906\n",
      "Iteration 290, loss = 0.48590984\n",
      "Iteration 291, loss = 0.48579776\n",
      "Iteration 292, loss = 0.48563939\n",
      "Iteration 293, loss = 0.48551832\n",
      "Iteration 294, loss = 0.48537945\n",
      "Iteration 295, loss = 0.48526171\n",
      "Iteration 296, loss = 0.48512897\n",
      "Iteration 297, loss = 0.48501023\n",
      "Iteration 298, loss = 0.48487264\n",
      "Iteration 299, loss = 0.48476040\n",
      "Iteration 300, loss = 0.48462709\n",
      "Iteration 1, loss = 0.72397506\n",
      "Iteration 2, loss = 0.70560601\n",
      "Iteration 3, loss = 0.67994544\n",
      "Iteration 4, loss = 0.65323559\n",
      "Iteration 5, loss = 0.62926038\n",
      "Iteration 6, loss = 0.60825833\n",
      "Iteration 7, loss = 0.59258313\n",
      "Iteration 8, loss = 0.58094730\n",
      "Iteration 9, loss = 0.57271168\n",
      "Iteration 10, loss = 0.56599119\n",
      "Iteration 11, loss = 0.56300154\n",
      "Iteration 12, loss = 0.56086141\n",
      "Iteration 13, loss = 0.55904736\n",
      "Iteration 14, loss = 0.55819833\n",
      "Iteration 15, loss = 0.55768933\n",
      "Iteration 16, loss = 0.55739180\n",
      "Iteration 17, loss = 0.55694869\n",
      "Iteration 18, loss = 0.55660910\n",
      "Iteration 19, loss = 0.55636468\n",
      "Iteration 20, loss = 0.55604300\n",
      "Iteration 21, loss = 0.55574689\n",
      "Iteration 22, loss = 0.55544236\n",
      "Iteration 23, loss = 0.55510964\n",
      "Iteration 24, loss = 0.55485113\n",
      "Iteration 25, loss = 0.55446144\n",
      "Iteration 26, loss = 0.55417676\n",
      "Iteration 27, loss = 0.55389574\n",
      "Iteration 28, loss = 0.55356691\n",
      "Iteration 29, loss = 0.55327290\n",
      "Iteration 30, loss = 0.55302938\n",
      "Iteration 31, loss = 0.55268855\n",
      "Iteration 32, loss = 0.55240522\n",
      "Iteration 33, loss = 0.55215168\n",
      "Iteration 34, loss = 0.55182984\n",
      "Iteration 35, loss = 0.55156470\n",
      "Iteration 36, loss = 0.55128828\n",
      "Iteration 37, loss = 0.55102434\n",
      "Iteration 38, loss = 0.55073451\n",
      "Iteration 39, loss = 0.55046324\n",
      "Iteration 40, loss = 0.55021457\n",
      "Iteration 41, loss = 0.54993220\n",
      "Iteration 42, loss = 0.54967684\n",
      "Iteration 43, loss = 0.54941486\n",
      "Iteration 44, loss = 0.54916194\n",
      "Iteration 45, loss = 0.54889405\n",
      "Iteration 46, loss = 0.54872111\n",
      "Iteration 47, loss = 0.54838637\n",
      "Iteration 48, loss = 0.54813193\n",
      "Iteration 49, loss = 0.54787581\n",
      "Iteration 50, loss = 0.54764336\n",
      "Iteration 51, loss = 0.54737042\n",
      "Iteration 52, loss = 0.54711328\n",
      "Iteration 53, loss = 0.54691458\n",
      "Iteration 54, loss = 0.54662316\n",
      "Iteration 55, loss = 0.54644676\n",
      "Iteration 56, loss = 0.54611504\n",
      "Iteration 57, loss = 0.54590017\n",
      "Iteration 58, loss = 0.54565886\n",
      "Iteration 59, loss = 0.54538525\n",
      "Iteration 60, loss = 0.54513045\n",
      "Iteration 61, loss = 0.54489703\n",
      "Iteration 62, loss = 0.54464627\n",
      "Iteration 63, loss = 0.54441159\n",
      "Iteration 64, loss = 0.54417410\n",
      "Iteration 65, loss = 0.54392520\n",
      "Iteration 66, loss = 0.54366674\n",
      "Iteration 67, loss = 0.54342443\n",
      "Iteration 68, loss = 0.54316293\n",
      "Iteration 69, loss = 0.54293032\n",
      "Iteration 70, loss = 0.54270400\n",
      "Iteration 71, loss = 0.54243464\n",
      "Iteration 72, loss = 0.54220146\n",
      "Iteration 73, loss = 0.54194361\n",
      "Iteration 74, loss = 0.54173052\n",
      "Iteration 75, loss = 0.54145201\n",
      "Iteration 76, loss = 0.54120106\n",
      "Iteration 77, loss = 0.54097652\n",
      "Iteration 78, loss = 0.54071272\n",
      "Iteration 79, loss = 0.54049088\n",
      "Iteration 80, loss = 0.54021328\n",
      "Iteration 81, loss = 0.53996492\n",
      "Iteration 82, loss = 0.53974191\n",
      "Iteration 83, loss = 0.53947300\n",
      "Iteration 84, loss = 0.53924766\n",
      "Iteration 85, loss = 0.53895682\n",
      "Iteration 86, loss = 0.53871278\n",
      "Iteration 87, loss = 0.53852194\n",
      "Iteration 88, loss = 0.53820993\n",
      "Iteration 89, loss = 0.53799639\n",
      "Iteration 90, loss = 0.53769214\n",
      "Iteration 91, loss = 0.53744587\n",
      "Iteration 92, loss = 0.53717533\n",
      "Iteration 93, loss = 0.53694063\n",
      "Iteration 94, loss = 0.53666514\n",
      "Iteration 95, loss = 0.53644598\n",
      "Iteration 96, loss = 0.53613787\n",
      "Iteration 97, loss = 0.53587591\n",
      "Iteration 98, loss = 0.53560063\n",
      "Iteration 99, loss = 0.53533740\n",
      "Iteration 100, loss = 0.53508778\n",
      "Iteration 101, loss = 0.53483515\n",
      "Iteration 102, loss = 0.53454875\n",
      "Iteration 103, loss = 0.53434159\n",
      "Iteration 104, loss = 0.53401536\n",
      "Iteration 105, loss = 0.53376296\n",
      "Iteration 106, loss = 0.53346136\n",
      "Iteration 107, loss = 0.53322439\n",
      "Iteration 108, loss = 0.53290756\n",
      "Iteration 109, loss = 0.53265085\n",
      "Iteration 110, loss = 0.53235957\n",
      "Iteration 111, loss = 0.53208581\n",
      "Iteration 112, loss = 0.53179534\n",
      "Iteration 113, loss = 0.53157403\n",
      "Iteration 114, loss = 0.53125722\n",
      "Iteration 115, loss = 0.53095378\n",
      "Iteration 116, loss = 0.53070057\n",
      "Iteration 117, loss = 0.53037424\n",
      "Iteration 118, loss = 0.53011695\n",
      "Iteration 119, loss = 0.52982696\n",
      "Iteration 120, loss = 0.52953026\n",
      "Iteration 121, loss = 0.52924128\n",
      "Iteration 122, loss = 0.52894270\n",
      "Iteration 123, loss = 0.52865734\n",
      "Iteration 124, loss = 0.52836357\n",
      "Iteration 125, loss = 0.52805562\n",
      "Iteration 126, loss = 0.52776224\n",
      "Iteration 127, loss = 0.52745868\n",
      "Iteration 128, loss = 0.52718131\n",
      "Iteration 129, loss = 0.52686014\n",
      "Iteration 130, loss = 0.52657739\n",
      "Iteration 131, loss = 0.52626088\n",
      "Iteration 132, loss = 0.52595814\n",
      "Iteration 133, loss = 0.52565442\n",
      "Iteration 134, loss = 0.52534239\n",
      "Iteration 135, loss = 0.52503501\n",
      "Iteration 136, loss = 0.52475270\n",
      "Iteration 137, loss = 0.52442276\n",
      "Iteration 138, loss = 0.52412584\n",
      "Iteration 139, loss = 0.52383146\n",
      "Iteration 140, loss = 0.52348205\n",
      "Iteration 141, loss = 0.52315271\n",
      "Iteration 142, loss = 0.52284386\n",
      "Iteration 143, loss = 0.52252827\n",
      "Iteration 144, loss = 0.52220898\n",
      "Iteration 145, loss = 0.52193682\n",
      "Iteration 146, loss = 0.52158630\n",
      "Iteration 147, loss = 0.52124770\n",
      "Iteration 148, loss = 0.52095083\n",
      "Iteration 149, loss = 0.52060791\n",
      "Iteration 150, loss = 0.52028345\n",
      "Iteration 151, loss = 0.51994771\n",
      "Iteration 152, loss = 0.51964629\n",
      "Iteration 153, loss = 0.51929598\n",
      "Iteration 154, loss = 0.51897560\n",
      "Iteration 155, loss = 0.51865903\n",
      "Iteration 156, loss = 0.51833724\n",
      "Iteration 157, loss = 0.51800992\n",
      "Iteration 158, loss = 0.51768847\n",
      "Iteration 159, loss = 0.51734791\n",
      "Iteration 160, loss = 0.51705036\n",
      "Iteration 161, loss = 0.51669561\n",
      "Iteration 162, loss = 0.51634075\n",
      "Iteration 163, loss = 0.51599360\n",
      "Iteration 164, loss = 0.51565812\n",
      "Iteration 165, loss = 0.51534197\n",
      "Iteration 166, loss = 0.51503287\n",
      "Iteration 167, loss = 0.51471649\n",
      "Iteration 168, loss = 0.51432074\n",
      "Iteration 169, loss = 0.51399742\n",
      "Iteration 170, loss = 0.51367576\n",
      "Iteration 171, loss = 0.51333334\n",
      "Iteration 172, loss = 0.51296961\n",
      "Iteration 173, loss = 0.51264160\n",
      "Iteration 174, loss = 0.51232717\n",
      "Iteration 175, loss = 0.51196677\n",
      "Iteration 176, loss = 0.51163205\n",
      "Iteration 177, loss = 0.51128926\n",
      "Iteration 178, loss = 0.51094784\n",
      "Iteration 179, loss = 0.51061293\n",
      "Iteration 180, loss = 0.51025999\n",
      "Iteration 181, loss = 0.50994637\n",
      "Iteration 182, loss = 0.50959773\n",
      "Iteration 183, loss = 0.50927389\n",
      "Iteration 184, loss = 0.50892598\n",
      "Iteration 185, loss = 0.50858752\n",
      "Iteration 186, loss = 0.50823740\n",
      "Iteration 187, loss = 0.50789842\n",
      "Iteration 188, loss = 0.50757745\n",
      "Iteration 189, loss = 0.50721358\n",
      "Iteration 190, loss = 0.50688957\n",
      "Iteration 191, loss = 0.50656560\n",
      "Iteration 192, loss = 0.50623066\n",
      "Iteration 193, loss = 0.50590235\n",
      "Iteration 194, loss = 0.50557378\n",
      "Iteration 195, loss = 0.50523456\n",
      "Iteration 196, loss = 0.50491309\n",
      "Iteration 197, loss = 0.50453546\n",
      "Iteration 198, loss = 0.50424375\n",
      "Iteration 199, loss = 0.50390158\n",
      "Iteration 200, loss = 0.50356664\n",
      "Iteration 201, loss = 0.50322338\n",
      "Iteration 202, loss = 0.50289093\n",
      "Iteration 203, loss = 0.50256639\n",
      "Iteration 204, loss = 0.50226958\n",
      "Iteration 205, loss = 0.50189318\n",
      "Iteration 206, loss = 0.50159065\n",
      "Iteration 207, loss = 0.50125812\n",
      "Iteration 208, loss = 0.50092107\n",
      "Iteration 209, loss = 0.50059484\n",
      "Iteration 210, loss = 0.50026801\n",
      "Iteration 211, loss = 0.49996023\n",
      "Iteration 212, loss = 0.49963549\n",
      "Iteration 213, loss = 0.49931206\n",
      "Iteration 214, loss = 0.49899534\n",
      "Iteration 215, loss = 0.49865754\n",
      "Iteration 216, loss = 0.49836115\n",
      "Iteration 217, loss = 0.49803598\n",
      "Iteration 218, loss = 0.49774332\n",
      "Iteration 219, loss = 0.49741633\n",
      "Iteration 220, loss = 0.49709758\n",
      "Iteration 221, loss = 0.49678961\n",
      "Iteration 222, loss = 0.49645361\n",
      "Iteration 223, loss = 0.49614557\n",
      "Iteration 224, loss = 0.49585360\n",
      "Iteration 225, loss = 0.49555687\n",
      "Iteration 226, loss = 0.49523537\n",
      "Iteration 227, loss = 0.49493182\n",
      "Iteration 228, loss = 0.49467645\n",
      "Iteration 229, loss = 0.49433006\n",
      "Iteration 230, loss = 0.49401064\n",
      "Iteration 231, loss = 0.49373103\n",
      "Iteration 232, loss = 0.49342176\n",
      "Iteration 233, loss = 0.49312964\n",
      "Iteration 234, loss = 0.49286236\n",
      "Iteration 235, loss = 0.49255420\n",
      "Iteration 236, loss = 0.49225440\n",
      "Iteration 237, loss = 0.49201346\n",
      "Iteration 238, loss = 0.49166806\n",
      "Iteration 239, loss = 0.49138018\n",
      "Iteration 240, loss = 0.49109190\n",
      "Iteration 241, loss = 0.49082722\n",
      "Iteration 242, loss = 0.49052628\n",
      "Iteration 243, loss = 0.49023121\n",
      "Iteration 244, loss = 0.48996098\n",
      "Iteration 245, loss = 0.48969195\n",
      "Iteration 246, loss = 0.48941594\n",
      "Iteration 247, loss = 0.48914368\n",
      "Iteration 248, loss = 0.48885148\n",
      "Iteration 249, loss = 0.48859186\n",
      "Iteration 250, loss = 0.48833286\n",
      "Iteration 251, loss = 0.48804515\n",
      "Iteration 252, loss = 0.48777147\n",
      "Iteration 253, loss = 0.48754657\n",
      "Iteration 254, loss = 0.48728527\n",
      "Iteration 255, loss = 0.48699940\n",
      "Iteration 256, loss = 0.48670562\n",
      "Iteration 257, loss = 0.48648433\n",
      "Iteration 258, loss = 0.48619594\n",
      "Iteration 259, loss = 0.48595822\n",
      "Iteration 260, loss = 0.48569015\n",
      "Iteration 261, loss = 0.48545696\n",
      "Iteration 262, loss = 0.48517673\n",
      "Iteration 263, loss = 0.48493984\n",
      "Iteration 264, loss = 0.48471357\n",
      "Iteration 265, loss = 0.48446035\n",
      "Iteration 266, loss = 0.48420755\n",
      "Iteration 267, loss = 0.48397722\n",
      "Iteration 268, loss = 0.48375776\n",
      "Iteration 269, loss = 0.48348727\n",
      "Iteration 270, loss = 0.48327545\n",
      "Iteration 271, loss = 0.48299679\n",
      "Iteration 272, loss = 0.48277636\n",
      "Iteration 273, loss = 0.48256488\n",
      "Iteration 274, loss = 0.48232010\n",
      "Iteration 275, loss = 0.48208729\n",
      "Iteration 276, loss = 0.48185548\n",
      "Iteration 277, loss = 0.48162981\n",
      "Iteration 278, loss = 0.48148078\n",
      "Iteration 279, loss = 0.48122404\n",
      "Iteration 280, loss = 0.48097328\n",
      "Iteration 281, loss = 0.48081917\n",
      "Iteration 282, loss = 0.48053540\n",
      "Iteration 283, loss = 0.48032674\n",
      "Iteration 284, loss = 0.48009935\n",
      "Iteration 285, loss = 0.47989971\n",
      "Iteration 286, loss = 0.47970653\n",
      "Iteration 287, loss = 0.47949087\n",
      "Iteration 288, loss = 0.47929034\n",
      "Iteration 289, loss = 0.47908304\n",
      "Iteration 290, loss = 0.47889411\n",
      "Iteration 291, loss = 0.47867300\n",
      "Iteration 292, loss = 0.47849106\n",
      "Iteration 293, loss = 0.47832591\n",
      "Iteration 294, loss = 0.47810017\n",
      "Iteration 295, loss = 0.47789432\n",
      "Iteration 296, loss = 0.47768587\n",
      "Iteration 297, loss = 0.47752038\n",
      "Iteration 298, loss = 0.47731631\n",
      "Iteration 299, loss = 0.47714678\n",
      "Iteration 300, loss = 0.47695081\n",
      "Iteration 1, loss = 0.75759703\n",
      "Iteration 2, loss = 0.73504372\n",
      "Iteration 3, loss = 0.70247873\n",
      "Iteration 4, loss = 0.66939544\n",
      "Iteration 5, loss = 0.63792525\n",
      "Iteration 6, loss = 0.61203717\n",
      "Iteration 7, loss = 0.58987736\n",
      "Iteration 8, loss = 0.57484426\n",
      "Iteration 9, loss = 0.56337448\n",
      "Iteration 10, loss = 0.55521105\n",
      "Iteration 11, loss = 0.54961601\n",
      "Iteration 12, loss = 0.54632083\n",
      "Iteration 13, loss = 0.54419297\n",
      "Iteration 14, loss = 0.54321034\n",
      "Iteration 15, loss = 0.54192761\n",
      "Iteration 16, loss = 0.54158630\n",
      "Iteration 17, loss = 0.54114996\n",
      "Iteration 18, loss = 0.54116949\n",
      "Iteration 19, loss = 0.54073663\n",
      "Iteration 20, loss = 0.54055655\n",
      "Iteration 21, loss = 0.54032433\n",
      "Iteration 22, loss = 0.54009324\n",
      "Iteration 23, loss = 0.53987034\n",
      "Iteration 24, loss = 0.53963948\n",
      "Iteration 25, loss = 0.53937198\n",
      "Iteration 26, loss = 0.53913052\n",
      "Iteration 27, loss = 0.53887997\n",
      "Iteration 28, loss = 0.53864180\n",
      "Iteration 29, loss = 0.53839265\n",
      "Iteration 30, loss = 0.53815073\n",
      "Iteration 31, loss = 0.53792088\n",
      "Iteration 32, loss = 0.53763059\n",
      "Iteration 33, loss = 0.53737282\n",
      "Iteration 34, loss = 0.53711806\n",
      "Iteration 35, loss = 0.53689417\n",
      "Iteration 36, loss = 0.53660737\n",
      "Iteration 37, loss = 0.53636192\n",
      "Iteration 38, loss = 0.53609269\n",
      "Iteration 39, loss = 0.53587031\n",
      "Iteration 40, loss = 0.53557670\n",
      "Iteration 41, loss = 0.53531185\n",
      "Iteration 42, loss = 0.53506808\n",
      "Iteration 43, loss = 0.53480293\n",
      "Iteration 44, loss = 0.53455422\n",
      "Iteration 45, loss = 0.53427161\n",
      "Iteration 46, loss = 0.53402537\n",
      "Iteration 47, loss = 0.53377616\n",
      "Iteration 48, loss = 0.53348119\n",
      "Iteration 49, loss = 0.53320198\n",
      "Iteration 50, loss = 0.53295742\n",
      "Iteration 51, loss = 0.53266830\n",
      "Iteration 52, loss = 0.53241265\n",
      "Iteration 53, loss = 0.53213613\n",
      "Iteration 54, loss = 0.53186867\n",
      "Iteration 55, loss = 0.53157563\n",
      "Iteration 56, loss = 0.53133213\n",
      "Iteration 57, loss = 0.53104431\n",
      "Iteration 58, loss = 0.53075636\n",
      "Iteration 59, loss = 0.53049553\n",
      "Iteration 60, loss = 0.53022476\n",
      "Iteration 61, loss = 0.52994835\n",
      "Iteration 62, loss = 0.52966212\n",
      "Iteration 63, loss = 0.52938038\n",
      "Iteration 64, loss = 0.52911413\n",
      "Iteration 65, loss = 0.52884288\n",
      "Iteration 66, loss = 0.52855346\n",
      "Iteration 67, loss = 0.52824231\n",
      "Iteration 68, loss = 0.52795579\n",
      "Iteration 69, loss = 0.52767781\n",
      "Iteration 70, loss = 0.52742048\n",
      "Iteration 71, loss = 0.52711749\n",
      "Iteration 72, loss = 0.52682686\n",
      "Iteration 73, loss = 0.52656605\n",
      "Iteration 74, loss = 0.52627016\n",
      "Iteration 75, loss = 0.52598215\n",
      "Iteration 76, loss = 0.52567880\n",
      "Iteration 77, loss = 0.52538369\n",
      "Iteration 78, loss = 0.52511375\n",
      "Iteration 79, loss = 0.52484153\n",
      "Iteration 80, loss = 0.52454734\n",
      "Iteration 81, loss = 0.52426574\n",
      "Iteration 82, loss = 0.52397197\n",
      "Iteration 83, loss = 0.52367627\n",
      "Iteration 84, loss = 0.52337849\n",
      "Iteration 85, loss = 0.52308137\n",
      "Iteration 86, loss = 0.52280931\n",
      "Iteration 87, loss = 0.52253097\n",
      "Iteration 88, loss = 0.52223679\n",
      "Iteration 89, loss = 0.52195687\n",
      "Iteration 90, loss = 0.52163768\n",
      "Iteration 91, loss = 0.52133820\n",
      "Iteration 92, loss = 0.52105917\n",
      "Iteration 93, loss = 0.52076736\n",
      "Iteration 94, loss = 0.52048099\n",
      "Iteration 95, loss = 0.52021635\n",
      "Iteration 96, loss = 0.51992182\n",
      "Iteration 97, loss = 0.51962136\n",
      "Iteration 98, loss = 0.51935215\n",
      "Iteration 99, loss = 0.51907138\n",
      "Iteration 100, loss = 0.51872933\n",
      "Iteration 101, loss = 0.51845428\n",
      "Iteration 102, loss = 0.51819447\n",
      "Iteration 103, loss = 0.51791338\n",
      "Iteration 104, loss = 0.51761305\n",
      "Iteration 105, loss = 0.51732937\n",
      "Iteration 106, loss = 0.51705369\n",
      "Iteration 107, loss = 0.51677089\n",
      "Iteration 108, loss = 0.51650468\n",
      "Iteration 109, loss = 0.51618672\n",
      "Iteration 110, loss = 0.51589704\n",
      "Iteration 111, loss = 0.51565067\n",
      "Iteration 112, loss = 0.51533641\n",
      "Iteration 113, loss = 0.51507938\n",
      "Iteration 114, loss = 0.51477334\n",
      "Iteration 115, loss = 0.51455256\n",
      "Iteration 116, loss = 0.51421972\n",
      "Iteration 117, loss = 0.51394800\n",
      "Iteration 118, loss = 0.51368493\n",
      "Iteration 119, loss = 0.51342356\n",
      "Iteration 120, loss = 0.51310992\n",
      "Iteration 121, loss = 0.51284377\n",
      "Iteration 122, loss = 0.51256412\n",
      "Iteration 123, loss = 0.51232315\n",
      "Iteration 124, loss = 0.51203898\n",
      "Iteration 125, loss = 0.51177627\n",
      "Iteration 126, loss = 0.51150043\n",
      "Iteration 127, loss = 0.51126009\n",
      "Iteration 128, loss = 0.51096627\n",
      "Iteration 129, loss = 0.51071145\n",
      "Iteration 130, loss = 0.51047565\n",
      "Iteration 131, loss = 0.51017998\n",
      "Iteration 132, loss = 0.50988952\n",
      "Iteration 133, loss = 0.50964369\n",
      "Iteration 134, loss = 0.50939066\n",
      "Iteration 135, loss = 0.50912826\n",
      "Iteration 136, loss = 0.50887568\n",
      "Iteration 137, loss = 0.50863431\n",
      "Iteration 138, loss = 0.50835365\n",
      "Iteration 139, loss = 0.50810266\n",
      "Iteration 140, loss = 0.50791379\n",
      "Iteration 141, loss = 0.50761317\n",
      "Iteration 142, loss = 0.50737714\n",
      "Iteration 143, loss = 0.50711097\n",
      "Iteration 144, loss = 0.50689233\n",
      "Iteration 145, loss = 0.50664750\n",
      "Iteration 146, loss = 0.50637552\n",
      "Iteration 147, loss = 0.50613970\n",
      "Iteration 148, loss = 0.50587649\n",
      "Iteration 149, loss = 0.50565487\n",
      "Iteration 150, loss = 0.50542929\n",
      "Iteration 151, loss = 0.50517911\n",
      "Iteration 152, loss = 0.50495045\n",
      "Iteration 153, loss = 0.50473174\n",
      "Iteration 154, loss = 0.50452058\n",
      "Iteration 155, loss = 0.50427694\n",
      "Iteration 156, loss = 0.50403093\n",
      "Iteration 157, loss = 0.50383905\n",
      "Iteration 158, loss = 0.50356414\n",
      "Iteration 159, loss = 0.50335135\n",
      "Iteration 160, loss = 0.50311389\n",
      "Iteration 161, loss = 0.50290825\n",
      "Iteration 162, loss = 0.50268812\n",
      "Iteration 163, loss = 0.50249916\n",
      "Iteration 164, loss = 0.50226009\n",
      "Iteration 165, loss = 0.50202929\n",
      "Iteration 166, loss = 0.50180515\n",
      "Iteration 167, loss = 0.50161556\n",
      "Iteration 168, loss = 0.50140531\n",
      "Iteration 169, loss = 0.50118907\n",
      "Iteration 170, loss = 0.50102820\n",
      "Iteration 171, loss = 0.50078697\n",
      "Iteration 172, loss = 0.50058193\n",
      "Iteration 173, loss = 0.50035430\n",
      "Iteration 174, loss = 0.50017266\n",
      "Iteration 175, loss = 0.49997438\n",
      "Iteration 176, loss = 0.49979242\n",
      "Iteration 177, loss = 0.49956728\n",
      "Iteration 178, loss = 0.49937029\n",
      "Iteration 179, loss = 0.49918125\n",
      "Iteration 180, loss = 0.49900261\n",
      "Iteration 181, loss = 0.49883432\n",
      "Iteration 182, loss = 0.49861578\n",
      "Iteration 183, loss = 0.49844153\n",
      "Iteration 184, loss = 0.49827292\n",
      "Iteration 185, loss = 0.49805221\n",
      "Iteration 186, loss = 0.49786875\n",
      "Iteration 187, loss = 0.49769500\n",
      "Iteration 188, loss = 0.49752435\n",
      "Iteration 189, loss = 0.49732402\n",
      "Iteration 190, loss = 0.49715745\n",
      "Iteration 191, loss = 0.49697935\n",
      "Iteration 192, loss = 0.49684315\n",
      "Iteration 193, loss = 0.49668882\n",
      "Iteration 194, loss = 0.49648786\n",
      "Iteration 195, loss = 0.49631519\n",
      "Iteration 196, loss = 0.49611248\n",
      "Iteration 197, loss = 0.49597867\n",
      "Iteration 198, loss = 0.49580139\n",
      "Iteration 199, loss = 0.49564580\n",
      "Iteration 200, loss = 0.49546786\n",
      "Iteration 201, loss = 0.49531324\n",
      "Iteration 202, loss = 0.49516828\n",
      "Iteration 203, loss = 0.49501823\n",
      "Iteration 204, loss = 0.49485674\n",
      "Iteration 205, loss = 0.49468953\n",
      "Iteration 206, loss = 0.49453078\n",
      "Iteration 207, loss = 0.49440030\n",
      "Iteration 208, loss = 0.49423227\n",
      "Iteration 209, loss = 0.49410877\n",
      "Iteration 210, loss = 0.49396472\n",
      "Iteration 211, loss = 0.49378263\n",
      "Iteration 212, loss = 0.49366830\n",
      "Iteration 213, loss = 0.49351572\n",
      "Iteration 214, loss = 0.49336763\n",
      "Iteration 215, loss = 0.49321862\n",
      "Iteration 216, loss = 0.49310591\n",
      "Iteration 217, loss = 0.49297048\n",
      "Iteration 218, loss = 0.49281661\n",
      "Iteration 219, loss = 0.49268002\n",
      "Iteration 220, loss = 0.49256533\n",
      "Iteration 221, loss = 0.49240796\n",
      "Iteration 222, loss = 0.49227594\n",
      "Iteration 223, loss = 0.49219276\n",
      "Iteration 224, loss = 0.49201552\n",
      "Iteration 225, loss = 0.49190016\n",
      "Iteration 226, loss = 0.49177082\n",
      "Iteration 227, loss = 0.49162729\n",
      "Iteration 228, loss = 0.49151971\n",
      "Iteration 229, loss = 0.49139472\n",
      "Iteration 230, loss = 0.49128941\n",
      "Iteration 231, loss = 0.49116181\n",
      "Iteration 232, loss = 0.49103103\n",
      "Iteration 233, loss = 0.49092103\n",
      "Iteration 234, loss = 0.49081537\n",
      "Iteration 235, loss = 0.49067358\n",
      "Iteration 236, loss = 0.49055902\n",
      "Iteration 237, loss = 0.49045211\n",
      "Iteration 238, loss = 0.49036400\n",
      "Iteration 239, loss = 0.49024364\n",
      "Iteration 240, loss = 0.49011238\n",
      "Iteration 241, loss = 0.48999832\n",
      "Iteration 242, loss = 0.48989471\n",
      "Iteration 243, loss = 0.48981215\n",
      "Iteration 244, loss = 0.48967701\n",
      "Iteration 245, loss = 0.48958965\n",
      "Iteration 246, loss = 0.48946784\n",
      "Iteration 247, loss = 0.48940543\n",
      "Iteration 248, loss = 0.48926361\n",
      "Iteration 249, loss = 0.48917599\n",
      "Iteration 250, loss = 0.48908662\n",
      "Iteration 251, loss = 0.48898286\n",
      "Iteration 252, loss = 0.48886612\n",
      "Iteration 253, loss = 0.48878278\n",
      "Iteration 254, loss = 0.48868845\n",
      "Iteration 255, loss = 0.48860387\n",
      "Iteration 256, loss = 0.48848593\n",
      "Iteration 257, loss = 0.48840626\n",
      "Iteration 258, loss = 0.48831822\n",
      "Iteration 259, loss = 0.48828388\n",
      "Iteration 260, loss = 0.48814043\n",
      "Iteration 261, loss = 0.48803975\n",
      "Iteration 262, loss = 0.48797302\n",
      "Iteration 263, loss = 0.48787105\n",
      "Iteration 264, loss = 0.48776903\n",
      "Iteration 265, loss = 0.48771959\n",
      "Iteration 266, loss = 0.48761454\n",
      "Iteration 267, loss = 0.48755456\n",
      "Iteration 268, loss = 0.48743588\n",
      "Iteration 269, loss = 0.48740053\n",
      "Iteration 270, loss = 0.48728186\n",
      "Iteration 271, loss = 0.48721546\n",
      "Iteration 272, loss = 0.48712616\n",
      "Iteration 273, loss = 0.48703162\n",
      "Iteration 274, loss = 0.48698083\n",
      "Iteration 275, loss = 0.48688537\n",
      "Iteration 276, loss = 0.48680929\n",
      "Iteration 277, loss = 0.48678700\n",
      "Iteration 278, loss = 0.48667982\n",
      "Iteration 279, loss = 0.48660399\n",
      "Iteration 280, loss = 0.48653513\n",
      "Iteration 281, loss = 0.48644902\n",
      "Iteration 282, loss = 0.48637614\n",
      "Iteration 283, loss = 0.48633495\n",
      "Iteration 284, loss = 0.48622957\n",
      "Iteration 285, loss = 0.48615880\n",
      "Iteration 286, loss = 0.48609751\n",
      "Iteration 287, loss = 0.48603570\n",
      "Iteration 288, loss = 0.48598983\n",
      "Iteration 289, loss = 0.48589717\n",
      "Iteration 290, loss = 0.48583532\n",
      "Iteration 291, loss = 0.48578350\n",
      "Iteration 292, loss = 0.48570783\n",
      "Iteration 293, loss = 0.48563299\n",
      "Iteration 294, loss = 0.48556991\n",
      "Iteration 295, loss = 0.48553344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(transfusion_values, transfusion_classes):\n",
    "    data_train, target_train = transfusion_values[train], transfusion_classes[train]\n",
    "    data_test, target_test = transfusion_values[test], transfusion_classes[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       570\n",
      "           1       0.26      0.28      0.27       178\n",
      "\n",
      "    accuracy                           0.64       748\n",
      "   macro avg       0.52      0.52      0.52       748\n",
      "weighted avg       0.65      0.64      0.64       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[428 142]\n",
      " [128  50]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       570\n",
      "           1       0.24      0.29      0.26       178\n",
      "\n",
      "    accuracy                           0.62       748\n",
      "   macro avg       0.50      0.50      0.50       748\n",
      "weighted avg       0.64      0.62      0.63       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[411 159]\n",
      " [127  51]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       570\n",
      "           1       0.45      0.22      0.30       178\n",
      "\n",
      "    accuracy                           0.75       748\n",
      "   macro avg       0.62      0.57      0.57       748\n",
      "weighted avg       0.71      0.75      0.72       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[522  48]\n",
      " [138  40]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.87       570\n",
      "           1       0.59      0.27      0.37       178\n",
      "\n",
      "    accuracy                           0.78       748\n",
      "   macro avg       0.70      0.61      0.62       748\n",
      "weighted avg       0.75      0.78      0.75       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[536  34]\n",
      " [130  48]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.86       570\n",
      "           1       0.56      0.11      0.18       178\n",
      "\n",
      "    accuracy                           0.77       748\n",
      "   macro avg       0.67      0.54      0.52       748\n",
      "weighted avg       0.73      0.77      0.70       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[555  15]\n",
      " [159  19]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(transfusion_classes, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusÃ£o: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(transfusion_classes, predicted_classes[classificador]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
