{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Balance Scale:\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Class-Name  Left-Weight  Left-Distance  Light-Weight  Light-Distance\n0             2            1              1             1               1\n1             1            1              1             1               2\n2             1            1              1             1               3\n3             1            1              1             1               4\n4             1            1              1             1               5\n..          ...          ...            ...           ...             ...\n620           3            5              5             5               1\n621           3            5              5             5               2\n622           3            5              5             5               3\n623           3            5              5             5               4\n624           2            5              5             5               5\n\n[625 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class-Name</th>\n      <th>Left-Weight</th>\n      <th>Left-Distance</th>\n      <th>Light-Weight</th>\n      <th>Light-Distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>621</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>624</th>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>625 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "balanceScale = pd.read_table('balance-scale.data', sep=',')\n",
    "print(\"\\nDataset Balance Scale:\")\n",
    "balanceScale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Class-Name', 'Left-Weight', 'Left-Distance', 'Light-Weight',\n       'Light-Distance'],\n      dtype='object')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanceScale.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "     Class-Name  Left-Weight  Left-Distance  Light-Weight  Light-Distance\n0             2            1              1             1               1\n1             1            1              1             1               2\n2             1            1              1             1               3\n3             1            1              1             1               4\n4             1            1              1             1               5\n..          ...          ...            ...           ...             ...\n620           3            5              5             5               1\n621           3            5              5             5               2\n622           3            5              5             5               3\n623           3            5              5             5               4\n624           2            5              5             5               5\n\n[625 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class-Name</th>\n      <th>Left-Weight</th>\n      <th>Left-Distance</th>\n      <th>Light-Weight</th>\n      <th>Light-Distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>621</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>624</th>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>625 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanceScaleNormalized = balanceScale.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "# Codificar coluna que tenha caractere para numero e depois remover a coluna\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc_df = pd.DataFrame(enc.fit_transform(balanceScale[['Class-Name']]).toarray())\n",
    "# balanceScale = balanceScale.join(enc_df)\n",
    "# balanceScale = balanceScale.drop(columns = ['Class-Name'])\n",
    "# balanceScale\n",
    "\n",
    "# balanceScaleNormalized = normalizador.fit_transform(balanceScale)\n",
    "# balanceScale['Class-Name'] = balanceScaleNormalized[:,0]\n",
    "# balanceScale['Left-Weight'] = balanceScaleNormalized[:,1]\n",
    "# balanceScale['Left-Distance'] = balanceScaleNormalized[:,2]\n",
    "# balanceScale['Right-Weight'] = balanceScaleNormalized[:,3]\n",
    "# balanceScale['Right-Distance'] = balanceScaleNormalized[:,4]\n",
    "#\n",
    "# print(\"\\nDataset Balance Scale Normalized:\")\n",
    "balanceScale\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balance Scale features:\n",
      "\n",
      "[[2 1 1 1 1]\n",
      " [1 1 1 1 2]\n",
      " [1 1 1 1 3]\n",
      " ...\n",
      " [3 5 5 5 3]\n",
      " [3 5 5 5 4]\n",
      " [2 5 5 5 5]]\n"
     ]
    }
   ],
   "source": [
    "balanceScaleValues = balanceScale.iloc[:,0:5].values\n",
    "print(\"\\nBalance Scale features:\\n\")\n",
    "print(balanceScaleValues)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balance Scale classes:\n",
      "\n",
      "[1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2\n",
      " 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4\n",
      " 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1\n",
      " 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3\n",
      " 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n",
      " 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2\n",
      " 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4\n",
      " 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1\n",
      " 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3\n",
      " 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n",
      " 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2\n",
      " 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4\n",
      " 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1\n",
      " 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3\n",
      " 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n",
      " 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2\n",
      " 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5]\n",
      "\n",
      "Balance Scale classes shape:\n",
      "(625,)\n"
     ]
    }
   ],
   "source": [
    "balanceScaleClasses = balanceScale.iloc[:,4].values\n",
    "print(\"\\nBalance Scale classes:\\n\")\n",
    "print(balanceScaleClasses)\n",
    "print(\"\\nBalance Scale classes shape:\")\n",
    "print(balanceScaleClasses.shape)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(balanceScaleClasses.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68298971\n",
      "Iteration 2, loss = 1.67652829\n",
      "Iteration 3, loss = 1.66671590\n",
      "Iteration 4, loss = 1.65727770\n",
      "Iteration 5, loss = 1.64665213\n",
      "Iteration 6, loss = 1.63854895\n",
      "Iteration 7, loss = 1.63065888\n",
      "Iteration 8, loss = 1.62491272\n",
      "Iteration 9, loss = 1.61945917\n",
      "Iteration 10, loss = 1.61584000\n",
      "Iteration 11, loss = 1.61222134\n",
      "Iteration 12, loss = 1.60964282\n",
      "Iteration 13, loss = 1.60702550\n",
      "Iteration 14, loss = 1.60536307\n",
      "Iteration 15, loss = 1.60321115\n",
      "Iteration 16, loss = 1.60148061\n",
      "Iteration 17, loss = 1.59971245\n",
      "Iteration 18, loss = 1.59807684\n",
      "Iteration 19, loss = 1.59660742\n",
      "Iteration 20, loss = 1.59484174\n",
      "Iteration 21, loss = 1.59304620\n",
      "Iteration 22, loss = 1.59104497\n",
      "Iteration 23, loss = 1.58903737\n",
      "Iteration 24, loss = 1.58690715\n",
      "Iteration 25, loss = 1.58460567\n",
      "Iteration 26, loss = 1.58197501\n",
      "Iteration 27, loss = 1.57930816\n",
      "Iteration 28, loss = 1.57630947\n",
      "Iteration 29, loss = 1.57316832\n",
      "Iteration 30, loss = 1.56978689\n",
      "Iteration 31, loss = 1.56622535\n",
      "Iteration 32, loss = 1.56236757\n",
      "Iteration 33, loss = 1.55837297\n",
      "Iteration 34, loss = 1.55439238\n",
      "Iteration 35, loss = 1.55013706\n",
      "Iteration 36, loss = 1.54560873\n",
      "Iteration 37, loss = 1.54103176\n",
      "Iteration 38, loss = 1.53635956\n",
      "Iteration 39, loss = 1.53156856\n",
      "Iteration 40, loss = 1.52673373\n",
      "Iteration 41, loss = 1.52168816\n",
      "Iteration 42, loss = 1.51625345\n",
      "Iteration 43, loss = 1.51088670\n",
      "Iteration 44, loss = 1.50526678\n",
      "Iteration 45, loss = 1.49953873\n",
      "Iteration 46, loss = 1.49367752\n",
      "Iteration 47, loss = 1.48754448\n",
      "Iteration 48, loss = 1.48127676\n",
      "Iteration 49, loss = 1.47492948\n",
      "Iteration 50, loss = 1.46846583\n",
      "Iteration 51, loss = 1.46186321\n",
      "Iteration 52, loss = 1.45507349\n",
      "Iteration 53, loss = 1.44819564\n",
      "Iteration 54, loss = 1.44119078\n",
      "Iteration 55, loss = 1.43428635\n",
      "Iteration 56, loss = 1.42704461\n",
      "Iteration 57, loss = 1.41988277\n",
      "Iteration 58, loss = 1.41253805\n",
      "Iteration 59, loss = 1.40536662\n",
      "Iteration 60, loss = 1.39801483\n",
      "Iteration 61, loss = 1.39066136\n",
      "Iteration 62, loss = 1.38322609\n",
      "Iteration 63, loss = 1.37621444\n",
      "Iteration 64, loss = 1.36874173\n",
      "Iteration 65, loss = 1.36132237\n",
      "Iteration 66, loss = 1.35411107\n",
      "Iteration 67, loss = 1.34699291\n",
      "Iteration 68, loss = 1.33972270\n",
      "Iteration 69, loss = 1.33271287\n",
      "Iteration 70, loss = 1.32562623\n",
      "Iteration 71, loss = 1.31856364\n",
      "Iteration 72, loss = 1.31179401\n",
      "Iteration 73, loss = 1.30492350\n",
      "Iteration 74, loss = 1.29820025\n",
      "Iteration 75, loss = 1.29155206\n",
      "Iteration 76, loss = 1.28496011\n",
      "Iteration 77, loss = 1.27844845\n",
      "Iteration 78, loss = 1.27216000\n",
      "Iteration 79, loss = 1.26572256\n",
      "Iteration 80, loss = 1.25954225\n",
      "Iteration 81, loss = 1.25349602\n",
      "Iteration 82, loss = 1.24750295\n",
      "Iteration 83, loss = 1.24161212\n",
      "Iteration 84, loss = 1.23589663\n",
      "Iteration 85, loss = 1.23009682\n",
      "Iteration 86, loss = 1.22455166\n",
      "Iteration 87, loss = 1.21905112\n",
      "Iteration 88, loss = 1.21368847\n",
      "Iteration 89, loss = 1.20826691\n",
      "Iteration 90, loss = 1.20319617\n",
      "Iteration 91, loss = 1.19806744\n",
      "Iteration 92, loss = 1.19302657\n",
      "Iteration 93, loss = 1.18812191\n",
      "Iteration 94, loss = 1.18323617\n",
      "Iteration 95, loss = 1.17851873\n",
      "Iteration 96, loss = 1.17385048\n",
      "Iteration 97, loss = 1.16927875\n",
      "Iteration 98, loss = 1.16480095\n",
      "Iteration 99, loss = 1.16038699\n",
      "Iteration 100, loss = 1.15600505\n",
      "Iteration 101, loss = 1.15174962\n",
      "Iteration 102, loss = 1.14761441\n",
      "Iteration 103, loss = 1.14351551\n",
      "Iteration 104, loss = 1.13949417\n",
      "Iteration 105, loss = 1.13550662\n",
      "Iteration 106, loss = 1.13168676\n",
      "Iteration 107, loss = 1.12780362\n",
      "Iteration 108, loss = 1.12403991\n",
      "Iteration 109, loss = 1.12033187\n",
      "Iteration 110, loss = 1.11674223\n",
      "Iteration 111, loss = 1.11310437\n",
      "Iteration 112, loss = 1.10962633\n",
      "Iteration 113, loss = 1.10610180\n",
      "Iteration 114, loss = 1.10270653\n",
      "Iteration 115, loss = 1.09935893\n",
      "Iteration 116, loss = 1.09599708\n",
      "Iteration 117, loss = 1.09275665\n",
      "Iteration 118, loss = 1.08954452\n",
      "Iteration 119, loss = 1.08630478\n",
      "Iteration 120, loss = 1.08314350\n",
      "Iteration 121, loss = 1.08009804\n",
      "Iteration 122, loss = 1.07701795\n",
      "Iteration 123, loss = 1.07399912\n",
      "Iteration 124, loss = 1.07101044\n",
      "Iteration 125, loss = 1.06808516\n",
      "Iteration 126, loss = 1.06516807\n",
      "Iteration 127, loss = 1.06224374\n",
      "Iteration 128, loss = 1.05940807\n",
      "Iteration 129, loss = 1.05664187\n",
      "Iteration 130, loss = 1.05386860\n",
      "Iteration 131, loss = 1.05112721\n",
      "Iteration 132, loss = 1.04847065\n",
      "Iteration 133, loss = 1.04565648\n",
      "Iteration 134, loss = 1.04306331\n",
      "Iteration 135, loss = 1.04040869\n",
      "Iteration 136, loss = 1.03787826\n",
      "Iteration 137, loss = 1.03535317\n",
      "Iteration 138, loss = 1.03263978\n",
      "Iteration 139, loss = 1.03010738\n",
      "Iteration 140, loss = 1.02769592\n",
      "Iteration 141, loss = 1.02508020\n",
      "Iteration 142, loss = 1.02271002\n",
      "Iteration 143, loss = 1.02018341\n",
      "Iteration 144, loss = 1.01775653\n",
      "Iteration 145, loss = 1.01544730\n",
      "Iteration 146, loss = 1.01303451\n",
      "Iteration 147, loss = 1.01064998\n",
      "Iteration 148, loss = 1.00827886\n",
      "Iteration 149, loss = 1.00603585\n",
      "Iteration 150, loss = 1.00364357\n",
      "Iteration 151, loss = 1.00137599\n",
      "Iteration 152, loss = 0.99902192\n",
      "Iteration 153, loss = 0.99687194\n",
      "Iteration 154, loss = 0.99452188\n",
      "Iteration 155, loss = 0.99235270\n",
      "Iteration 156, loss = 0.99000060\n",
      "Iteration 157, loss = 0.98776246\n",
      "Iteration 158, loss = 0.98558083\n",
      "Iteration 159, loss = 0.98342783\n",
      "Iteration 160, loss = 0.98123575\n",
      "Iteration 161, loss = 0.97899895\n",
      "Iteration 162, loss = 0.97683906\n",
      "Iteration 163, loss = 0.97466338\n",
      "Iteration 164, loss = 0.97252222\n",
      "Iteration 165, loss = 0.97039229\n",
      "Iteration 166, loss = 0.96817556\n",
      "Iteration 167, loss = 0.96608275\n",
      "Iteration 168, loss = 0.96403762\n",
      "Iteration 169, loss = 0.96201005\n",
      "Iteration 170, loss = 0.95980591\n",
      "Iteration 171, loss = 0.95773663\n",
      "Iteration 172, loss = 0.95570130\n",
      "Iteration 173, loss = 0.95363274\n",
      "Iteration 174, loss = 0.95168020\n",
      "Iteration 175, loss = 0.94967174\n",
      "Iteration 176, loss = 0.94760791\n",
      "Iteration 177, loss = 0.94556103\n",
      "Iteration 178, loss = 0.94364737\n",
      "Iteration 179, loss = 0.94156257\n",
      "Iteration 180, loss = 0.93954297\n",
      "Iteration 181, loss = 0.93759688\n",
      "Iteration 182, loss = 0.93563937\n",
      "Iteration 183, loss = 0.93363935\n",
      "Iteration 184, loss = 0.93169161\n",
      "Iteration 185, loss = 0.92978221\n",
      "Iteration 186, loss = 0.92787054\n",
      "Iteration 187, loss = 0.92585087\n",
      "Iteration 188, loss = 0.92397165\n",
      "Iteration 189, loss = 0.92221304\n",
      "Iteration 190, loss = 0.92008890\n",
      "Iteration 191, loss = 0.91818274\n",
      "Iteration 192, loss = 0.91629648\n",
      "Iteration 193, loss = 0.91436855\n",
      "Iteration 194, loss = 0.91257058\n",
      "Iteration 195, loss = 0.91071158\n",
      "Iteration 196, loss = 0.90878417\n",
      "Iteration 197, loss = 0.90692861\n",
      "Iteration 198, loss = 0.90509238\n",
      "Iteration 199, loss = 0.90335846\n",
      "Iteration 200, loss = 0.90139741\n",
      "Iteration 201, loss = 0.89963313\n",
      "Iteration 202, loss = 0.89782943\n",
      "Iteration 203, loss = 0.89590060\n",
      "Iteration 204, loss = 0.89420294\n",
      "Iteration 205, loss = 0.89236189\n",
      "Iteration 206, loss = 0.89056918\n",
      "Iteration 207, loss = 0.88875710\n",
      "Iteration 208, loss = 0.88704065\n",
      "Iteration 209, loss = 0.88533303\n",
      "Iteration 210, loss = 0.88356800\n",
      "Iteration 211, loss = 0.88189536\n",
      "Iteration 212, loss = 0.88005388\n",
      "Iteration 213, loss = 0.87825220\n",
      "Iteration 214, loss = 0.87649669\n",
      "Iteration 215, loss = 0.87486360\n",
      "Iteration 216, loss = 0.87315986\n",
      "Iteration 217, loss = 0.87137023\n",
      "Iteration 218, loss = 0.86964669\n",
      "Iteration 219, loss = 0.86797159\n",
      "Iteration 220, loss = 0.86630685\n",
      "Iteration 221, loss = 0.86458009\n",
      "Iteration 222, loss = 0.86296270\n",
      "Iteration 223, loss = 0.86125744\n",
      "Iteration 224, loss = 0.85957036\n",
      "Iteration 225, loss = 0.85790455\n",
      "Iteration 226, loss = 0.85624434\n",
      "Iteration 227, loss = 0.85460965\n",
      "Iteration 228, loss = 0.85302411\n",
      "Iteration 229, loss = 0.85134289\n",
      "Iteration 230, loss = 0.84975948\n",
      "Iteration 231, loss = 0.84808649\n",
      "Iteration 232, loss = 0.84657317\n",
      "Iteration 233, loss = 0.84496179\n",
      "Iteration 234, loss = 0.84330632\n",
      "Iteration 235, loss = 0.84170549\n",
      "Iteration 236, loss = 0.84017052\n",
      "Iteration 237, loss = 0.83860444\n",
      "Iteration 238, loss = 0.83692175\n",
      "Iteration 239, loss = 0.83536301\n",
      "Iteration 240, loss = 0.83381424\n",
      "Iteration 241, loss = 0.83230478\n",
      "Iteration 242, loss = 0.83065692\n",
      "Iteration 243, loss = 0.82923530\n",
      "Iteration 244, loss = 0.82760291\n",
      "Iteration 245, loss = 0.82618110\n",
      "Iteration 246, loss = 0.82456632\n",
      "Iteration 247, loss = 0.82306515\n",
      "Iteration 248, loss = 0.82159825\n",
      "Iteration 249, loss = 0.82006457\n",
      "Iteration 250, loss = 0.81854867\n",
      "Iteration 251, loss = 0.81708626\n",
      "Iteration 252, loss = 0.81561967\n",
      "Iteration 253, loss = 0.81401713\n",
      "Iteration 254, loss = 0.81254437\n",
      "Iteration 255, loss = 0.81108044\n",
      "Iteration 256, loss = 0.80975655\n",
      "Iteration 257, loss = 0.80835181\n",
      "Iteration 258, loss = 0.80681577\n",
      "Iteration 259, loss = 0.80533691\n",
      "Iteration 260, loss = 0.80389542\n",
      "Iteration 261, loss = 0.80245620\n",
      "Iteration 262, loss = 0.80119379\n",
      "Iteration 263, loss = 0.79960980\n",
      "Iteration 264, loss = 0.79824784\n",
      "Iteration 265, loss = 0.79689648\n",
      "Iteration 266, loss = 0.79537985\n",
      "Iteration 267, loss = 0.79399941\n",
      "Iteration 268, loss = 0.79254414\n",
      "Iteration 269, loss = 0.79129535\n",
      "Iteration 270, loss = 0.78990115\n",
      "Iteration 271, loss = 0.78848936\n",
      "Iteration 272, loss = 0.78708291\n",
      "Iteration 273, loss = 0.78568015\n",
      "Iteration 274, loss = 0.78435952\n",
      "Iteration 275, loss = 0.78293395\n",
      "Iteration 276, loss = 0.78160773\n",
      "Iteration 277, loss = 0.78033466\n",
      "Iteration 278, loss = 0.77893837\n",
      "Iteration 279, loss = 0.77756868\n",
      "Iteration 280, loss = 0.77625751\n",
      "Iteration 281, loss = 0.77502072\n",
      "Iteration 282, loss = 0.77355374\n",
      "Iteration 283, loss = 0.77232756\n",
      "Iteration 284, loss = 0.77112855\n",
      "Iteration 285, loss = 0.76972077\n",
      "Iteration 286, loss = 0.76849429\n",
      "Iteration 287, loss = 0.76720758\n",
      "Iteration 288, loss = 0.76592495\n",
      "Iteration 289, loss = 0.76467285\n",
      "Iteration 290, loss = 0.76332417\n",
      "Iteration 291, loss = 0.76207096\n",
      "Iteration 292, loss = 0.76098894\n",
      "Iteration 293, loss = 0.75954796\n",
      "Iteration 294, loss = 0.75826608\n",
      "Iteration 295, loss = 0.75703935\n",
      "Iteration 296, loss = 0.75578759\n",
      "Iteration 297, loss = 0.75452988\n",
      "Iteration 298, loss = 0.75330481\n",
      "Iteration 299, loss = 0.75208500\n",
      "Iteration 300, loss = 0.75091124\n",
      "Iteration 1, loss = 1.70186435\n",
      "Iteration 2, loss = 1.69396416\n",
      "Iteration 3, loss = 1.68280129\n",
      "Iteration 4, loss = 1.67089530\n",
      "Iteration 5, loss = 1.66055045\n",
      "Iteration 6, loss = 1.65025876\n",
      "Iteration 7, loss = 1.64186850\n",
      "Iteration 8, loss = 1.63537657\n",
      "Iteration 9, loss = 1.63033893\n",
      "Iteration 10, loss = 1.62606239\n",
      "Iteration 11, loss = 1.62236129\n",
      "Iteration 12, loss = 1.61980660\n",
      "Iteration 13, loss = 1.61740905\n",
      "Iteration 14, loss = 1.61531490\n",
      "Iteration 15, loss = 1.61349059\n",
      "Iteration 16, loss = 1.61178888\n",
      "Iteration 17, loss = 1.61014315\n",
      "Iteration 18, loss = 1.60863269\n",
      "Iteration 19, loss = 1.60713345\n",
      "Iteration 20, loss = 1.60562201\n",
      "Iteration 21, loss = 1.60409929\n",
      "Iteration 22, loss = 1.60266706\n",
      "Iteration 23, loss = 1.60121332\n",
      "Iteration 24, loss = 1.59972845\n",
      "Iteration 25, loss = 1.59832592\n",
      "Iteration 26, loss = 1.59672389\n",
      "Iteration 27, loss = 1.59518568\n",
      "Iteration 28, loss = 1.59359259\n",
      "Iteration 29, loss = 1.59200375\n",
      "Iteration 30, loss = 1.59020340\n",
      "Iteration 31, loss = 1.58849403\n",
      "Iteration 32, loss = 1.58663233\n",
      "Iteration 33, loss = 1.58473048\n",
      "Iteration 34, loss = 1.58272684\n",
      "Iteration 35, loss = 1.58068764\n",
      "Iteration 36, loss = 1.57862373\n",
      "Iteration 37, loss = 1.57638036\n",
      "Iteration 38, loss = 1.57406843\n",
      "Iteration 39, loss = 1.57165517\n",
      "Iteration 40, loss = 1.56916273\n",
      "Iteration 41, loss = 1.56656652\n",
      "Iteration 42, loss = 1.56384809\n",
      "Iteration 43, loss = 1.56093213\n",
      "Iteration 44, loss = 1.55797705\n",
      "Iteration 45, loss = 1.55479626\n",
      "Iteration 46, loss = 1.55152459\n",
      "Iteration 47, loss = 1.54809568\n",
      "Iteration 48, loss = 1.54468873\n",
      "Iteration 49, loss = 1.54082780\n",
      "Iteration 50, loss = 1.53703891\n",
      "Iteration 51, loss = 1.53306260\n",
      "Iteration 52, loss = 1.52890865\n",
      "Iteration 53, loss = 1.52459148\n",
      "Iteration 54, loss = 1.52012512\n",
      "Iteration 55, loss = 1.51561092\n",
      "Iteration 56, loss = 1.51081807\n",
      "Iteration 57, loss = 1.50581166\n",
      "Iteration 58, loss = 1.50086869\n",
      "Iteration 59, loss = 1.49556031\n",
      "Iteration 60, loss = 1.49015856\n",
      "Iteration 61, loss = 1.48466355\n",
      "Iteration 62, loss = 1.47892234\n",
      "Iteration 63, loss = 1.47322471\n",
      "Iteration 64, loss = 1.46729671\n",
      "Iteration 65, loss = 1.46121892\n",
      "Iteration 66, loss = 1.45500855\n",
      "Iteration 67, loss = 1.44871199\n",
      "Iteration 68, loss = 1.44232886\n",
      "Iteration 69, loss = 1.43586739\n",
      "Iteration 70, loss = 1.42926638\n",
      "Iteration 71, loss = 1.42267418\n",
      "Iteration 72, loss = 1.41599624\n",
      "Iteration 73, loss = 1.40923991\n",
      "Iteration 74, loss = 1.40242751\n",
      "Iteration 75, loss = 1.39559495\n",
      "Iteration 76, loss = 1.38869662\n",
      "Iteration 77, loss = 1.38180541\n",
      "Iteration 78, loss = 1.37479440\n",
      "Iteration 79, loss = 1.36793262\n",
      "Iteration 80, loss = 1.36099169\n",
      "Iteration 81, loss = 1.35401950\n",
      "Iteration 82, loss = 1.34713975\n",
      "Iteration 83, loss = 1.34036206\n",
      "Iteration 84, loss = 1.33354179\n",
      "Iteration 85, loss = 1.32689457\n",
      "Iteration 86, loss = 1.32000118\n",
      "Iteration 87, loss = 1.31340459\n",
      "Iteration 88, loss = 1.30673473\n",
      "Iteration 89, loss = 1.30026636\n",
      "Iteration 90, loss = 1.29379346\n",
      "Iteration 91, loss = 1.28740185\n",
      "Iteration 92, loss = 1.28097715\n",
      "Iteration 93, loss = 1.27480528\n",
      "Iteration 94, loss = 1.26860448\n",
      "Iteration 95, loss = 1.26257922\n",
      "Iteration 96, loss = 1.25666830\n",
      "Iteration 97, loss = 1.25067770\n",
      "Iteration 98, loss = 1.24488037\n",
      "Iteration 99, loss = 1.23923886\n",
      "Iteration 100, loss = 1.23359923\n",
      "Iteration 101, loss = 1.22809252\n",
      "Iteration 102, loss = 1.22275638\n",
      "Iteration 103, loss = 1.21734405\n",
      "Iteration 104, loss = 1.21211022\n",
      "Iteration 105, loss = 1.20700532\n",
      "Iteration 106, loss = 1.20191564\n",
      "Iteration 107, loss = 1.19691112\n",
      "Iteration 108, loss = 1.19208173\n",
      "Iteration 109, loss = 1.18728877\n",
      "Iteration 110, loss = 1.18261046\n",
      "Iteration 111, loss = 1.17800246\n",
      "Iteration 112, loss = 1.17340187\n",
      "Iteration 113, loss = 1.16897777\n",
      "Iteration 114, loss = 1.16453965\n",
      "Iteration 115, loss = 1.16021611\n",
      "Iteration 116, loss = 1.15607242\n",
      "Iteration 117, loss = 1.15184492\n",
      "Iteration 118, loss = 1.14784002\n",
      "Iteration 119, loss = 1.14377189\n",
      "Iteration 120, loss = 1.13986036\n",
      "Iteration 121, loss = 1.13596585\n",
      "Iteration 122, loss = 1.13220162\n",
      "Iteration 123, loss = 1.12846762\n",
      "Iteration 124, loss = 1.12483879\n",
      "Iteration 125, loss = 1.12114632\n",
      "Iteration 126, loss = 1.11768288\n",
      "Iteration 127, loss = 1.11415881\n",
      "Iteration 128, loss = 1.11075270\n",
      "Iteration 129, loss = 1.10743427\n",
      "Iteration 130, loss = 1.10406044\n",
      "Iteration 131, loss = 1.10078399\n",
      "Iteration 132, loss = 1.09763744\n",
      "Iteration 133, loss = 1.09450809\n",
      "Iteration 134, loss = 1.09151859\n",
      "Iteration 135, loss = 1.08834136\n",
      "Iteration 136, loss = 1.08531272\n",
      "Iteration 137, loss = 1.08237144\n",
      "Iteration 138, loss = 1.07946104\n",
      "Iteration 139, loss = 1.07654786\n",
      "Iteration 140, loss = 1.07367219\n",
      "Iteration 141, loss = 1.07088716\n",
      "Iteration 142, loss = 1.06809933\n",
      "Iteration 143, loss = 1.06538681\n",
      "Iteration 144, loss = 1.06263722\n",
      "Iteration 145, loss = 1.06003504\n",
      "Iteration 146, loss = 1.05736434\n",
      "Iteration 147, loss = 1.05487606\n",
      "Iteration 148, loss = 1.05230580\n",
      "Iteration 149, loss = 1.04969689\n",
      "Iteration 150, loss = 1.04720840\n",
      "Iteration 151, loss = 1.04478739\n",
      "Iteration 152, loss = 1.04236407\n",
      "Iteration 153, loss = 1.03992954\n",
      "Iteration 154, loss = 1.03756034\n",
      "Iteration 155, loss = 1.03514294\n",
      "Iteration 156, loss = 1.03287892\n",
      "Iteration 157, loss = 1.03056961\n",
      "Iteration 158, loss = 1.02828605\n",
      "Iteration 159, loss = 1.02606239\n",
      "Iteration 160, loss = 1.02381554\n",
      "Iteration 161, loss = 1.02166686\n",
      "Iteration 162, loss = 1.01944198\n",
      "Iteration 163, loss = 1.01724145\n",
      "Iteration 164, loss = 1.01517945\n",
      "Iteration 165, loss = 1.01304628\n",
      "Iteration 166, loss = 1.01092519\n",
      "Iteration 167, loss = 1.00887256\n",
      "Iteration 168, loss = 1.00686077\n",
      "Iteration 169, loss = 1.00478361\n",
      "Iteration 170, loss = 1.00277587\n",
      "Iteration 171, loss = 1.00077097\n",
      "Iteration 172, loss = 0.99877885\n",
      "Iteration 173, loss = 0.99687240\n",
      "Iteration 174, loss = 0.99490660\n",
      "Iteration 175, loss = 0.99301160\n",
      "Iteration 176, loss = 0.99104970\n",
      "Iteration 177, loss = 0.98912190\n",
      "Iteration 178, loss = 0.98729192\n",
      "Iteration 179, loss = 0.98537798\n",
      "Iteration 180, loss = 0.98365035\n",
      "Iteration 181, loss = 0.98178523\n",
      "Iteration 182, loss = 0.98006215\n",
      "Iteration 183, loss = 0.97816716\n",
      "Iteration 184, loss = 0.97637623\n",
      "Iteration 185, loss = 0.97453272\n",
      "Iteration 186, loss = 0.97276787\n",
      "Iteration 187, loss = 0.97104612\n",
      "Iteration 188, loss = 0.96925303\n",
      "Iteration 189, loss = 0.96756654\n",
      "Iteration 190, loss = 0.96581251\n",
      "Iteration 191, loss = 0.96408258\n",
      "Iteration 192, loss = 0.96240994\n",
      "Iteration 193, loss = 0.96077127\n",
      "Iteration 194, loss = 0.95908200\n",
      "Iteration 195, loss = 0.95739991\n",
      "Iteration 196, loss = 0.95577138\n",
      "Iteration 197, loss = 0.95409842\n",
      "Iteration 198, loss = 0.95249060\n",
      "Iteration 199, loss = 0.95087167\n",
      "Iteration 200, loss = 0.94927906\n",
      "Iteration 201, loss = 0.94764651\n",
      "Iteration 202, loss = 0.94609274\n",
      "Iteration 203, loss = 0.94455933\n",
      "Iteration 204, loss = 0.94304002\n",
      "Iteration 205, loss = 0.94137542\n",
      "Iteration 206, loss = 0.93986879\n",
      "Iteration 207, loss = 0.93837094\n",
      "Iteration 208, loss = 0.93681348\n",
      "Iteration 209, loss = 0.93528023\n",
      "Iteration 210, loss = 0.93372543\n",
      "Iteration 211, loss = 0.93226149\n",
      "Iteration 212, loss = 0.93078238\n",
      "Iteration 213, loss = 0.92922234\n",
      "Iteration 214, loss = 0.92774990\n",
      "Iteration 215, loss = 0.92622587\n",
      "Iteration 216, loss = 0.92480128\n",
      "Iteration 217, loss = 0.92330594\n",
      "Iteration 218, loss = 0.92183959\n",
      "Iteration 219, loss = 0.92040131\n",
      "Iteration 220, loss = 0.91899986\n",
      "Iteration 221, loss = 0.91765109\n",
      "Iteration 222, loss = 0.91610383\n",
      "Iteration 223, loss = 0.91482303\n",
      "Iteration 224, loss = 0.91331137\n",
      "Iteration 225, loss = 0.91189391\n",
      "Iteration 226, loss = 0.91049658\n",
      "Iteration 227, loss = 0.90908825\n",
      "Iteration 228, loss = 0.90772719\n",
      "Iteration 229, loss = 0.90635747\n",
      "Iteration 230, loss = 0.90497462\n",
      "Iteration 231, loss = 0.90363512\n",
      "Iteration 232, loss = 0.90224635\n",
      "Iteration 233, loss = 0.90100379\n",
      "Iteration 234, loss = 0.89960440\n",
      "Iteration 235, loss = 0.89826543\n",
      "Iteration 236, loss = 0.89704082\n",
      "Iteration 237, loss = 0.89565152\n",
      "Iteration 238, loss = 0.89444097\n",
      "Iteration 239, loss = 0.89307008\n",
      "Iteration 240, loss = 0.89176754\n",
      "Iteration 241, loss = 0.89040686\n",
      "Iteration 242, loss = 0.88922631\n",
      "Iteration 243, loss = 0.88782058\n",
      "Iteration 244, loss = 0.88653450\n",
      "Iteration 245, loss = 0.88528655\n",
      "Iteration 246, loss = 0.88399811\n",
      "Iteration 247, loss = 0.88283030\n",
      "Iteration 248, loss = 0.88158825\n",
      "Iteration 249, loss = 0.88030430\n",
      "Iteration 250, loss = 0.87906609\n",
      "Iteration 251, loss = 0.87791264\n",
      "Iteration 252, loss = 0.87650797\n",
      "Iteration 253, loss = 0.87529150\n",
      "Iteration 254, loss = 0.87410798\n",
      "Iteration 255, loss = 0.87288983\n",
      "Iteration 256, loss = 0.87172631\n",
      "Iteration 257, loss = 0.87040518\n",
      "Iteration 258, loss = 0.86920620\n",
      "Iteration 259, loss = 0.86810281\n",
      "Iteration 260, loss = 0.86686628\n",
      "Iteration 261, loss = 0.86568693\n",
      "Iteration 262, loss = 0.86442612\n",
      "Iteration 263, loss = 0.86332531\n",
      "Iteration 264, loss = 0.86214443\n",
      "Iteration 265, loss = 0.86100482\n",
      "Iteration 266, loss = 0.85978100\n",
      "Iteration 267, loss = 0.85864432\n",
      "Iteration 268, loss = 0.85746762\n",
      "Iteration 269, loss = 0.85633302\n",
      "Iteration 270, loss = 0.85521631\n",
      "Iteration 271, loss = 0.85403896\n",
      "Iteration 272, loss = 0.85289821\n",
      "Iteration 273, loss = 0.85188151\n",
      "Iteration 274, loss = 0.85088241\n",
      "Iteration 275, loss = 0.84970661\n",
      "Iteration 276, loss = 0.84848564\n",
      "Iteration 277, loss = 0.84737526\n",
      "Iteration 278, loss = 0.84624419\n",
      "Iteration 279, loss = 0.84500840\n",
      "Iteration 280, loss = 0.84403609\n",
      "Iteration 281, loss = 0.84289370\n",
      "Iteration 282, loss = 0.84179698\n",
      "Iteration 283, loss = 0.84070661\n",
      "Iteration 284, loss = 0.83977808\n",
      "Iteration 285, loss = 0.83878718\n",
      "Iteration 286, loss = 0.83769895\n",
      "Iteration 287, loss = 0.83656308\n",
      "Iteration 288, loss = 0.83531234\n",
      "Iteration 289, loss = 0.83422709\n",
      "Iteration 290, loss = 0.83319956\n",
      "Iteration 291, loss = 0.83219803\n",
      "Iteration 292, loss = 0.83107057\n",
      "Iteration 293, loss = 0.83008282\n",
      "Iteration 294, loss = 0.82909577\n",
      "Iteration 295, loss = 0.82805535\n",
      "Iteration 296, loss = 0.82712628\n",
      "Iteration 297, loss = 0.82599391\n",
      "Iteration 298, loss = 0.82489343\n",
      "Iteration 299, loss = 0.82392483\n",
      "Iteration 300, loss = 0.82286101\n",
      "Iteration 1, loss = 1.74931138\n",
      "Iteration 2, loss = 1.72931653\n",
      "Iteration 3, loss = 1.70385549\n",
      "Iteration 4, loss = 1.67902821\n",
      "Iteration 5, loss = 1.65665307\n",
      "Iteration 6, loss = 1.64045672\n",
      "Iteration 7, loss = 1.62940987\n",
      "Iteration 8, loss = 1.62063378\n",
      "Iteration 9, loss = 1.61541437\n",
      "Iteration 10, loss = 1.61169284\n",
      "Iteration 11, loss = 1.60902830\n",
      "Iteration 12, loss = 1.60693896\n",
      "Iteration 13, loss = 1.60454920\n",
      "Iteration 14, loss = 1.60265390\n",
      "Iteration 15, loss = 1.60077411\n",
      "Iteration 16, loss = 1.59896711\n",
      "Iteration 17, loss = 1.59710824\n",
      "Iteration 18, loss = 1.59509624\n",
      "Iteration 19, loss = 1.59305314\n",
      "Iteration 20, loss = 1.59106508\n",
      "Iteration 21, loss = 1.58894341\n",
      "Iteration 22, loss = 1.58686545\n",
      "Iteration 23, loss = 1.58463471\n",
      "Iteration 24, loss = 1.58246842\n",
      "Iteration 25, loss = 1.58021446\n",
      "Iteration 26, loss = 1.57780833\n",
      "Iteration 27, loss = 1.57537718\n",
      "Iteration 28, loss = 1.57282996\n",
      "Iteration 29, loss = 1.57023682\n",
      "Iteration 30, loss = 1.56741138\n",
      "Iteration 31, loss = 1.56447965\n",
      "Iteration 32, loss = 1.56132704\n",
      "Iteration 33, loss = 1.55803332\n",
      "Iteration 34, loss = 1.55472149\n",
      "Iteration 35, loss = 1.55125393\n",
      "Iteration 36, loss = 1.54718021\n",
      "Iteration 37, loss = 1.54321845\n",
      "Iteration 38, loss = 1.53924344\n",
      "Iteration 39, loss = 1.53482139\n",
      "Iteration 40, loss = 1.53033250\n",
      "Iteration 41, loss = 1.52568879\n",
      "Iteration 42, loss = 1.52077585\n",
      "Iteration 43, loss = 1.51573129\n",
      "Iteration 44, loss = 1.51049568\n",
      "Iteration 45, loss = 1.50516241\n",
      "Iteration 46, loss = 1.49948522\n",
      "Iteration 47, loss = 1.49371739\n",
      "Iteration 48, loss = 1.48783764\n",
      "Iteration 49, loss = 1.48175955\n",
      "Iteration 50, loss = 1.47549906\n",
      "Iteration 51, loss = 1.46915162\n",
      "Iteration 52, loss = 1.46267940\n",
      "Iteration 53, loss = 1.45609121\n",
      "Iteration 54, loss = 1.44931587\n",
      "Iteration 55, loss = 1.44246322\n",
      "Iteration 56, loss = 1.43544289\n",
      "Iteration 57, loss = 1.42842972\n",
      "Iteration 58, loss = 1.42142285\n",
      "Iteration 59, loss = 1.41435649\n",
      "Iteration 60, loss = 1.40712807\n",
      "Iteration 61, loss = 1.39982456\n",
      "Iteration 62, loss = 1.39263260\n",
      "Iteration 63, loss = 1.38540595\n",
      "Iteration 64, loss = 1.37829041\n",
      "Iteration 65, loss = 1.37091189\n",
      "Iteration 66, loss = 1.36372362\n",
      "Iteration 67, loss = 1.35646975\n",
      "Iteration 68, loss = 1.34929954\n",
      "Iteration 69, loss = 1.34216521\n",
      "Iteration 70, loss = 1.33506421\n",
      "Iteration 71, loss = 1.32801687\n",
      "Iteration 72, loss = 1.32111336\n",
      "Iteration 73, loss = 1.31421589\n",
      "Iteration 74, loss = 1.30743663\n",
      "Iteration 75, loss = 1.30070351\n",
      "Iteration 76, loss = 1.29399679\n",
      "Iteration 77, loss = 1.28746845\n",
      "Iteration 78, loss = 1.28103674\n",
      "Iteration 79, loss = 1.27470511\n",
      "Iteration 80, loss = 1.26844046\n",
      "Iteration 81, loss = 1.26217328\n",
      "Iteration 82, loss = 1.25618485\n",
      "Iteration 83, loss = 1.25025196\n",
      "Iteration 84, loss = 1.24415638\n",
      "Iteration 85, loss = 1.23839443\n",
      "Iteration 86, loss = 1.23270284\n",
      "Iteration 87, loss = 1.22712522\n",
      "Iteration 88, loss = 1.22166809\n",
      "Iteration 89, loss = 1.21626231\n",
      "Iteration 90, loss = 1.21097105\n",
      "Iteration 91, loss = 1.20587993\n",
      "Iteration 92, loss = 1.20066774\n",
      "Iteration 93, loss = 1.19568942\n",
      "Iteration 94, loss = 1.19081002\n",
      "Iteration 95, loss = 1.18594083\n",
      "Iteration 96, loss = 1.18120774\n",
      "Iteration 97, loss = 1.17645064\n",
      "Iteration 98, loss = 1.17187692\n",
      "Iteration 99, loss = 1.16746221\n",
      "Iteration 100, loss = 1.16303886\n",
      "Iteration 101, loss = 1.15873918\n",
      "Iteration 102, loss = 1.15452973\n",
      "Iteration 103, loss = 1.15045121\n",
      "Iteration 104, loss = 1.14627533\n",
      "Iteration 105, loss = 1.14225018\n",
      "Iteration 106, loss = 1.13821956\n",
      "Iteration 107, loss = 1.13439887\n",
      "Iteration 108, loss = 1.13066432\n",
      "Iteration 109, loss = 1.12690286\n",
      "Iteration 110, loss = 1.12317499\n",
      "Iteration 111, loss = 1.11948803\n",
      "Iteration 112, loss = 1.11597735\n",
      "Iteration 113, loss = 1.11255978\n",
      "Iteration 114, loss = 1.10903091\n",
      "Iteration 115, loss = 1.10571082\n",
      "Iteration 116, loss = 1.10237326\n",
      "Iteration 117, loss = 1.09905918\n",
      "Iteration 118, loss = 1.09582910\n",
      "Iteration 119, loss = 1.09264337\n",
      "Iteration 120, loss = 1.08964801\n",
      "Iteration 121, loss = 1.08650745\n",
      "Iteration 122, loss = 1.08349522\n",
      "Iteration 123, loss = 1.08044958\n",
      "Iteration 124, loss = 1.07762576\n",
      "Iteration 125, loss = 1.07468832\n",
      "Iteration 126, loss = 1.07185227\n",
      "Iteration 127, loss = 1.06926048\n",
      "Iteration 128, loss = 1.06619252\n",
      "Iteration 129, loss = 1.06352672\n",
      "Iteration 130, loss = 1.06090171\n",
      "Iteration 131, loss = 1.05811651\n",
      "Iteration 132, loss = 1.05563167\n",
      "Iteration 133, loss = 1.05297429\n",
      "Iteration 134, loss = 1.05043152\n",
      "Iteration 135, loss = 1.04791878\n",
      "Iteration 136, loss = 1.04542247\n",
      "Iteration 137, loss = 1.04314235\n",
      "Iteration 138, loss = 1.04048786\n",
      "Iteration 139, loss = 1.03818821\n",
      "Iteration 140, loss = 1.03588372\n",
      "Iteration 141, loss = 1.03347585\n",
      "Iteration 142, loss = 1.03106617\n",
      "Iteration 143, loss = 1.02880908\n",
      "Iteration 144, loss = 1.02665782\n",
      "Iteration 145, loss = 1.02434657\n",
      "Iteration 146, loss = 1.02222660\n",
      "Iteration 147, loss = 1.01994325\n",
      "Iteration 148, loss = 1.01776024\n",
      "Iteration 149, loss = 1.01565664\n",
      "Iteration 150, loss = 1.01343003\n",
      "Iteration 151, loss = 1.01137744\n",
      "Iteration 152, loss = 1.00916512\n",
      "Iteration 153, loss = 1.00707378\n",
      "Iteration 154, loss = 1.00505615\n",
      "Iteration 155, loss = 1.00296138\n",
      "Iteration 156, loss = 1.00091026\n",
      "Iteration 157, loss = 0.99892084\n",
      "Iteration 158, loss = 0.99697799\n",
      "Iteration 159, loss = 0.99503624\n",
      "Iteration 160, loss = 0.99302739\n",
      "Iteration 161, loss = 0.99106597\n",
      "Iteration 162, loss = 0.98931838\n",
      "Iteration 163, loss = 0.98740394\n",
      "Iteration 164, loss = 0.98555731\n",
      "Iteration 165, loss = 0.98359228\n",
      "Iteration 166, loss = 0.98172230\n",
      "Iteration 167, loss = 0.97993513\n",
      "Iteration 168, loss = 0.97802076\n",
      "Iteration 169, loss = 0.97633545\n",
      "Iteration 170, loss = 0.97440412\n",
      "Iteration 171, loss = 0.97272771\n",
      "Iteration 172, loss = 0.97088614\n",
      "Iteration 173, loss = 0.96924296\n",
      "Iteration 174, loss = 0.96733180\n",
      "Iteration 175, loss = 0.96574493\n",
      "Iteration 176, loss = 0.96386911\n",
      "Iteration 177, loss = 0.96216770\n",
      "Iteration 178, loss = 0.96043685\n",
      "Iteration 179, loss = 0.95866876\n",
      "Iteration 180, loss = 0.95695937\n",
      "Iteration 181, loss = 0.95527512\n",
      "Iteration 182, loss = 0.95368250\n",
      "Iteration 183, loss = 0.95195768\n",
      "Iteration 184, loss = 0.95031181\n",
      "Iteration 185, loss = 0.94867029\n",
      "Iteration 186, loss = 0.94700608\n",
      "Iteration 187, loss = 0.94538523\n",
      "Iteration 188, loss = 0.94378937\n",
      "Iteration 189, loss = 0.94220443\n",
      "Iteration 190, loss = 0.94061708\n",
      "Iteration 191, loss = 0.93915423\n",
      "Iteration 192, loss = 0.93743981\n",
      "Iteration 193, loss = 0.93587721\n",
      "Iteration 194, loss = 0.93434595\n",
      "Iteration 195, loss = 0.93279158\n",
      "Iteration 196, loss = 0.93131627\n",
      "Iteration 197, loss = 0.92972665\n",
      "Iteration 198, loss = 0.92828648\n",
      "Iteration 199, loss = 0.92670228\n",
      "Iteration 200, loss = 0.92525827\n",
      "Iteration 201, loss = 0.92368496\n",
      "Iteration 202, loss = 0.92223799\n",
      "Iteration 203, loss = 0.92071385\n",
      "Iteration 204, loss = 0.91917191\n",
      "Iteration 205, loss = 0.91769026\n",
      "Iteration 206, loss = 0.91622687\n",
      "Iteration 207, loss = 0.91484685\n",
      "Iteration 208, loss = 0.91326562\n",
      "Iteration 209, loss = 0.91185750\n",
      "Iteration 210, loss = 0.91038852\n",
      "Iteration 211, loss = 0.90898217\n",
      "Iteration 212, loss = 0.90741754\n",
      "Iteration 213, loss = 0.90612034\n",
      "Iteration 214, loss = 0.90467427\n",
      "Iteration 215, loss = 0.90321150\n",
      "Iteration 216, loss = 0.90188547\n",
      "Iteration 217, loss = 0.90055518\n",
      "Iteration 218, loss = 0.89919682\n",
      "Iteration 219, loss = 0.89775871\n",
      "Iteration 220, loss = 0.89638607\n",
      "Iteration 221, loss = 0.89494172\n",
      "Iteration 222, loss = 0.89368970\n",
      "Iteration 223, loss = 0.89224019\n",
      "Iteration 224, loss = 0.89088633\n",
      "Iteration 225, loss = 0.88966499\n",
      "Iteration 226, loss = 0.88815269\n",
      "Iteration 227, loss = 0.88677747\n",
      "Iteration 228, loss = 0.88555694\n",
      "Iteration 229, loss = 0.88411519\n",
      "Iteration 230, loss = 0.88280505\n",
      "Iteration 231, loss = 0.88150544\n",
      "Iteration 232, loss = 0.88027765\n",
      "Iteration 233, loss = 0.87890215\n",
      "Iteration 234, loss = 0.87760736\n",
      "Iteration 235, loss = 0.87622658\n",
      "Iteration 236, loss = 0.87492067\n",
      "Iteration 237, loss = 0.87361554\n",
      "Iteration 238, loss = 0.87235293\n",
      "Iteration 239, loss = 0.87103371\n",
      "Iteration 240, loss = 0.86976955\n",
      "Iteration 241, loss = 0.86851030\n",
      "Iteration 242, loss = 0.86721349\n",
      "Iteration 243, loss = 0.86610527\n",
      "Iteration 244, loss = 0.86478976\n",
      "Iteration 245, loss = 0.86361795\n",
      "Iteration 246, loss = 0.86238250\n",
      "Iteration 247, loss = 0.86107004\n",
      "Iteration 248, loss = 0.85984936\n",
      "Iteration 249, loss = 0.85868193\n",
      "Iteration 250, loss = 0.85736890\n",
      "Iteration 251, loss = 0.85607328\n",
      "Iteration 252, loss = 0.85490642\n",
      "Iteration 253, loss = 0.85388018\n",
      "Iteration 254, loss = 0.85249331\n",
      "Iteration 255, loss = 0.85126535\n",
      "Iteration 256, loss = 0.85028890\n",
      "Iteration 257, loss = 0.84893607\n",
      "Iteration 258, loss = 0.84765136\n",
      "Iteration 259, loss = 0.84652735\n",
      "Iteration 260, loss = 0.84530542\n",
      "Iteration 261, loss = 0.84416133\n",
      "Iteration 262, loss = 0.84296098\n",
      "Iteration 263, loss = 0.84183530\n",
      "Iteration 264, loss = 0.84068218\n",
      "Iteration 265, loss = 0.83957211\n",
      "Iteration 266, loss = 0.83831940\n",
      "Iteration 267, loss = 0.83719458\n",
      "Iteration 268, loss = 0.83615604\n",
      "Iteration 269, loss = 0.83495493\n",
      "Iteration 270, loss = 0.83386831\n",
      "Iteration 271, loss = 0.83259185\n",
      "Iteration 272, loss = 0.83150055\n",
      "Iteration 273, loss = 0.83034366\n",
      "Iteration 274, loss = 0.82930546\n",
      "Iteration 275, loss = 0.82811807\n",
      "Iteration 276, loss = 0.82697891\n",
      "Iteration 277, loss = 0.82585357\n",
      "Iteration 278, loss = 0.82476554\n",
      "Iteration 279, loss = 0.82362145\n",
      "Iteration 280, loss = 0.82269233\n",
      "Iteration 281, loss = 0.82153528\n",
      "Iteration 282, loss = 0.82038062\n",
      "Iteration 283, loss = 0.81925883\n",
      "Iteration 284, loss = 0.81823445\n",
      "Iteration 285, loss = 0.81712281\n",
      "Iteration 286, loss = 0.81608093\n",
      "Iteration 287, loss = 0.81499565\n",
      "Iteration 288, loss = 0.81396504\n",
      "Iteration 289, loss = 0.81280896\n",
      "Iteration 290, loss = 0.81178444\n",
      "Iteration 291, loss = 0.81068614\n",
      "Iteration 292, loss = 0.80959641\n",
      "Iteration 293, loss = 0.80864481\n",
      "Iteration 294, loss = 0.80755789\n",
      "Iteration 295, loss = 0.80652915\n",
      "Iteration 296, loss = 0.80540258\n",
      "Iteration 297, loss = 0.80452625\n",
      "Iteration 298, loss = 0.80352748\n",
      "Iteration 299, loss = 0.80243370\n",
      "Iteration 300, loss = 0.80135819\n",
      "Iteration 1, loss = 1.72836434\n",
      "Iteration 2, loss = 1.71376794\n",
      "Iteration 3, loss = 1.69489652\n",
      "Iteration 4, loss = 1.67268935\n",
      "Iteration 5, loss = 1.65366242\n",
      "Iteration 6, loss = 1.63764994\n",
      "Iteration 7, loss = 1.62570712\n",
      "Iteration 8, loss = 1.61760248\n",
      "Iteration 9, loss = 1.61080347\n",
      "Iteration 10, loss = 1.60837648\n",
      "Iteration 11, loss = 1.60588701\n",
      "Iteration 12, loss = 1.60392258\n",
      "Iteration 13, loss = 1.60252394\n",
      "Iteration 14, loss = 1.60133076\n",
      "Iteration 15, loss = 1.60000759\n",
      "Iteration 16, loss = 1.59844187\n",
      "Iteration 17, loss = 1.59680670\n",
      "Iteration 18, loss = 1.59497162\n",
      "Iteration 19, loss = 1.59327114\n",
      "Iteration 20, loss = 1.59134003\n",
      "Iteration 21, loss = 1.58958902\n",
      "Iteration 22, loss = 1.58769600\n",
      "Iteration 23, loss = 1.58584483\n",
      "Iteration 24, loss = 1.58400499\n",
      "Iteration 25, loss = 1.58211895\n",
      "Iteration 26, loss = 1.57986175\n",
      "Iteration 27, loss = 1.57768723\n",
      "Iteration 28, loss = 1.57536044\n",
      "Iteration 29, loss = 1.57299004\n",
      "Iteration 30, loss = 1.57090139\n",
      "Iteration 31, loss = 1.56802919\n",
      "Iteration 32, loss = 1.56533858\n",
      "Iteration 33, loss = 1.56272671\n",
      "Iteration 34, loss = 1.55988773\n",
      "Iteration 35, loss = 1.55713143\n",
      "Iteration 36, loss = 1.55429556\n",
      "Iteration 37, loss = 1.55097424\n",
      "Iteration 38, loss = 1.54785998\n",
      "Iteration 39, loss = 1.54455489\n",
      "Iteration 40, loss = 1.54128278\n",
      "Iteration 41, loss = 1.53770229\n",
      "Iteration 42, loss = 1.53405956\n",
      "Iteration 43, loss = 1.53047701\n",
      "Iteration 44, loss = 1.52682648\n",
      "Iteration 45, loss = 1.52286156\n",
      "Iteration 46, loss = 1.51899675\n",
      "Iteration 47, loss = 1.51506027\n",
      "Iteration 48, loss = 1.51077467\n",
      "Iteration 49, loss = 1.50692410\n",
      "Iteration 50, loss = 1.50234952\n",
      "Iteration 51, loss = 1.49790822\n",
      "Iteration 52, loss = 1.49349628\n",
      "Iteration 53, loss = 1.48895656\n",
      "Iteration 54, loss = 1.48430260\n",
      "Iteration 55, loss = 1.47978315\n",
      "Iteration 56, loss = 1.47504831\n",
      "Iteration 57, loss = 1.47026314\n",
      "Iteration 58, loss = 1.46552355\n",
      "Iteration 59, loss = 1.46063670\n",
      "Iteration 60, loss = 1.45567445\n",
      "Iteration 61, loss = 1.45085886\n",
      "Iteration 62, loss = 1.44569151\n",
      "Iteration 63, loss = 1.44071398\n",
      "Iteration 64, loss = 1.43554698\n",
      "Iteration 65, loss = 1.43040605\n",
      "Iteration 66, loss = 1.42531781\n",
      "Iteration 67, loss = 1.42021672\n",
      "Iteration 68, loss = 1.41490399\n",
      "Iteration 69, loss = 1.40978479\n",
      "Iteration 70, loss = 1.40452651\n",
      "Iteration 71, loss = 1.39934325\n",
      "Iteration 72, loss = 1.39390855\n",
      "Iteration 73, loss = 1.38897192\n",
      "Iteration 74, loss = 1.38334464\n",
      "Iteration 75, loss = 1.37803083\n",
      "Iteration 76, loss = 1.37268477\n",
      "Iteration 77, loss = 1.36727257\n",
      "Iteration 78, loss = 1.36201915\n",
      "Iteration 79, loss = 1.35678815\n",
      "Iteration 80, loss = 1.35134393\n",
      "Iteration 81, loss = 1.34603665\n",
      "Iteration 82, loss = 1.34086254\n",
      "Iteration 83, loss = 1.33546468\n",
      "Iteration 84, loss = 1.33014603\n",
      "Iteration 85, loss = 1.32502965\n",
      "Iteration 86, loss = 1.31956948\n",
      "Iteration 87, loss = 1.31426231\n",
      "Iteration 88, loss = 1.30913721\n",
      "Iteration 89, loss = 1.30389811\n",
      "Iteration 90, loss = 1.29877523\n",
      "Iteration 91, loss = 1.29376413\n",
      "Iteration 92, loss = 1.28863223\n",
      "Iteration 93, loss = 1.28344429\n",
      "Iteration 94, loss = 1.27852844\n",
      "Iteration 95, loss = 1.27344669\n",
      "Iteration 96, loss = 1.26862920\n",
      "Iteration 97, loss = 1.26360209\n",
      "Iteration 98, loss = 1.25885670\n",
      "Iteration 99, loss = 1.25397881\n",
      "Iteration 100, loss = 1.24918466\n",
      "Iteration 101, loss = 1.24444357\n",
      "Iteration 102, loss = 1.23975067\n",
      "Iteration 103, loss = 1.23510700\n",
      "Iteration 104, loss = 1.23058402\n",
      "Iteration 105, loss = 1.22606796\n",
      "Iteration 106, loss = 1.22162926\n",
      "Iteration 107, loss = 1.21716069\n",
      "Iteration 108, loss = 1.21285788\n",
      "Iteration 109, loss = 1.20852503\n",
      "Iteration 110, loss = 1.20440925\n",
      "Iteration 111, loss = 1.20007482\n",
      "Iteration 112, loss = 1.19596308\n",
      "Iteration 113, loss = 1.19186568\n",
      "Iteration 114, loss = 1.18785708\n",
      "Iteration 115, loss = 1.18397321\n",
      "Iteration 116, loss = 1.18000453\n",
      "Iteration 117, loss = 1.17612710\n",
      "Iteration 118, loss = 1.17230074\n",
      "Iteration 119, loss = 1.16855334\n",
      "Iteration 120, loss = 1.16476805\n",
      "Iteration 121, loss = 1.16127688\n",
      "Iteration 122, loss = 1.15755100\n",
      "Iteration 123, loss = 1.15406674\n",
      "Iteration 124, loss = 1.15050604\n",
      "Iteration 125, loss = 1.14713267\n",
      "Iteration 126, loss = 1.14367063\n",
      "Iteration 127, loss = 1.14043330\n",
      "Iteration 128, loss = 1.13712134\n",
      "Iteration 129, loss = 1.13386577\n",
      "Iteration 130, loss = 1.13059950\n",
      "Iteration 131, loss = 1.12742669\n",
      "Iteration 132, loss = 1.12431510\n",
      "Iteration 133, loss = 1.12129633\n",
      "Iteration 134, loss = 1.11816404\n",
      "Iteration 135, loss = 1.11523739\n",
      "Iteration 136, loss = 1.11230822\n",
      "Iteration 137, loss = 1.10942015\n",
      "Iteration 138, loss = 1.10653851\n",
      "Iteration 139, loss = 1.10377893\n",
      "Iteration 140, loss = 1.10096525\n",
      "Iteration 141, loss = 1.09829195\n",
      "Iteration 142, loss = 1.09560704\n",
      "Iteration 143, loss = 1.09299190\n",
      "Iteration 144, loss = 1.09032782\n",
      "Iteration 145, loss = 1.08771571\n",
      "Iteration 146, loss = 1.08511870\n",
      "Iteration 147, loss = 1.08261961\n",
      "Iteration 148, loss = 1.08015118\n",
      "Iteration 149, loss = 1.07763845\n",
      "Iteration 150, loss = 1.07513506\n",
      "Iteration 151, loss = 1.07287459\n",
      "Iteration 152, loss = 1.07038691\n",
      "Iteration 153, loss = 1.06807568\n",
      "Iteration 154, loss = 1.06576379\n",
      "Iteration 155, loss = 1.06354573\n",
      "Iteration 156, loss = 1.06122772\n",
      "Iteration 157, loss = 1.05902884\n",
      "Iteration 158, loss = 1.05679098\n",
      "Iteration 159, loss = 1.05464410\n",
      "Iteration 160, loss = 1.05251052\n",
      "Iteration 161, loss = 1.05047828\n",
      "Iteration 162, loss = 1.04836980\n",
      "Iteration 163, loss = 1.04626341\n",
      "Iteration 164, loss = 1.04413740\n",
      "Iteration 165, loss = 1.04210670\n",
      "Iteration 166, loss = 1.04011827\n",
      "Iteration 167, loss = 1.03809713\n",
      "Iteration 168, loss = 1.03601367\n",
      "Iteration 169, loss = 1.03411464\n",
      "Iteration 170, loss = 1.03213776\n",
      "Iteration 171, loss = 1.03013580\n",
      "Iteration 172, loss = 1.02829707\n",
      "Iteration 173, loss = 1.02635866\n",
      "Iteration 174, loss = 1.02468222\n",
      "Iteration 175, loss = 1.02266878\n",
      "Iteration 176, loss = 1.02082233\n",
      "Iteration 177, loss = 1.01894970\n",
      "Iteration 178, loss = 1.01713036\n",
      "Iteration 179, loss = 1.01537568\n",
      "Iteration 180, loss = 1.01352847\n",
      "Iteration 181, loss = 1.01167777\n",
      "Iteration 182, loss = 1.00990445\n",
      "Iteration 183, loss = 1.00824022\n",
      "Iteration 184, loss = 1.00658287\n",
      "Iteration 185, loss = 1.00474059\n",
      "Iteration 186, loss = 1.00297271\n",
      "Iteration 187, loss = 1.00127079\n",
      "Iteration 188, loss = 0.99952495\n",
      "Iteration 189, loss = 0.99784581\n",
      "Iteration 190, loss = 0.99621019\n",
      "Iteration 191, loss = 0.99455536\n",
      "Iteration 192, loss = 0.99310112\n",
      "Iteration 193, loss = 0.99129360\n",
      "Iteration 194, loss = 0.98964758\n",
      "Iteration 195, loss = 0.98818781\n",
      "Iteration 196, loss = 0.98644397\n",
      "Iteration 197, loss = 0.98482917\n",
      "Iteration 198, loss = 0.98326528\n",
      "Iteration 199, loss = 0.98164874\n",
      "Iteration 200, loss = 0.98008104\n",
      "Iteration 201, loss = 0.97856420\n",
      "Iteration 202, loss = 0.97683045\n",
      "Iteration 203, loss = 0.97530962\n",
      "Iteration 204, loss = 0.97379621\n",
      "Iteration 205, loss = 0.97220918\n",
      "Iteration 206, loss = 0.97076402\n",
      "Iteration 207, loss = 0.96925051\n",
      "Iteration 208, loss = 0.96765630\n",
      "Iteration 209, loss = 0.96611626\n",
      "Iteration 210, loss = 0.96452367\n",
      "Iteration 211, loss = 0.96302080\n",
      "Iteration 212, loss = 0.96147641\n",
      "Iteration 213, loss = 0.96002482\n",
      "Iteration 214, loss = 0.95851679\n",
      "Iteration 215, loss = 0.95706119\n",
      "Iteration 216, loss = 0.95553947\n",
      "Iteration 217, loss = 0.95410050\n",
      "Iteration 218, loss = 0.95250721\n",
      "Iteration 219, loss = 0.95109429\n",
      "Iteration 220, loss = 0.94958549\n",
      "Iteration 221, loss = 0.94827242\n",
      "Iteration 222, loss = 0.94668601\n",
      "Iteration 223, loss = 0.94523044\n",
      "Iteration 224, loss = 0.94371966\n",
      "Iteration 225, loss = 0.94233683\n",
      "Iteration 226, loss = 0.94093734\n",
      "Iteration 227, loss = 0.93948564\n",
      "Iteration 228, loss = 0.93807540\n",
      "Iteration 229, loss = 0.93655977\n",
      "Iteration 230, loss = 0.93514086\n",
      "Iteration 231, loss = 0.93365046\n",
      "Iteration 232, loss = 0.93219451\n",
      "Iteration 233, loss = 0.93078539\n",
      "Iteration 234, loss = 0.92938269\n",
      "Iteration 235, loss = 0.92797504\n",
      "Iteration 236, loss = 0.92648823\n",
      "Iteration 237, loss = 0.92507948\n",
      "Iteration 238, loss = 0.92366632\n",
      "Iteration 239, loss = 0.92219904\n",
      "Iteration 240, loss = 0.92086038\n",
      "Iteration 241, loss = 0.91943035\n",
      "Iteration 242, loss = 0.91797532\n",
      "Iteration 243, loss = 0.91657105\n",
      "Iteration 244, loss = 0.91528596\n",
      "Iteration 245, loss = 0.91388319\n",
      "Iteration 246, loss = 0.91248668\n",
      "Iteration 247, loss = 0.91104099\n",
      "Iteration 248, loss = 0.90991119\n",
      "Iteration 249, loss = 0.90824417\n",
      "Iteration 250, loss = 0.90689169\n",
      "Iteration 251, loss = 0.90552793\n",
      "Iteration 252, loss = 0.90407075\n",
      "Iteration 253, loss = 0.90267904\n",
      "Iteration 254, loss = 0.90128473\n",
      "Iteration 255, loss = 0.89997201\n",
      "Iteration 256, loss = 0.89861555\n",
      "Iteration 257, loss = 0.89726510\n",
      "Iteration 258, loss = 0.89583139\n",
      "Iteration 259, loss = 0.89447652\n",
      "Iteration 260, loss = 0.89310932\n",
      "Iteration 261, loss = 0.89193636\n",
      "Iteration 262, loss = 0.89043901\n",
      "Iteration 263, loss = 0.88903326\n",
      "Iteration 264, loss = 0.88772654\n",
      "Iteration 265, loss = 0.88639197\n",
      "Iteration 266, loss = 0.88506036\n",
      "Iteration 267, loss = 0.88363963\n",
      "Iteration 268, loss = 0.88241093\n",
      "Iteration 269, loss = 0.88108616\n",
      "Iteration 270, loss = 0.87963736\n",
      "Iteration 271, loss = 0.87830510\n",
      "Iteration 272, loss = 0.87700613\n",
      "Iteration 273, loss = 0.87567888\n",
      "Iteration 274, loss = 0.87435729\n",
      "Iteration 275, loss = 0.87301148\n",
      "Iteration 276, loss = 0.87167654\n",
      "Iteration 277, loss = 0.87040008\n",
      "Iteration 278, loss = 0.86902836\n",
      "Iteration 279, loss = 0.86776797\n",
      "Iteration 280, loss = 0.86638465\n",
      "Iteration 281, loss = 0.86514774\n",
      "Iteration 282, loss = 0.86376658\n",
      "Iteration 283, loss = 0.86252643\n",
      "Iteration 284, loss = 0.86116740\n",
      "Iteration 285, loss = 0.86008591\n",
      "Iteration 286, loss = 0.85872957\n",
      "Iteration 287, loss = 0.85733583\n",
      "Iteration 288, loss = 0.85603749\n",
      "Iteration 289, loss = 0.85476954\n",
      "Iteration 290, loss = 0.85352987\n",
      "Iteration 291, loss = 0.85224042\n",
      "Iteration 292, loss = 0.85104210\n",
      "Iteration 293, loss = 0.84972193\n",
      "Iteration 294, loss = 0.84849790\n",
      "Iteration 295, loss = 0.84728987\n",
      "Iteration 296, loss = 0.84607914\n",
      "Iteration 297, loss = 0.84480598\n",
      "Iteration 298, loss = 0.84344158\n",
      "Iteration 299, loss = 0.84223735\n",
      "Iteration 300, loss = 0.84101365\n",
      "Iteration 1, loss = 1.68416532\n",
      "Iteration 2, loss = 1.67837430\n",
      "Iteration 3, loss = 1.66962991\n",
      "Iteration 4, loss = 1.66048861\n",
      "Iteration 5, loss = 1.65053156\n",
      "Iteration 6, loss = 1.64196310\n",
      "Iteration 7, loss = 1.63517015\n",
      "Iteration 8, loss = 1.62833448\n",
      "Iteration 9, loss = 1.62321146\n",
      "Iteration 10, loss = 1.61904426\n",
      "Iteration 11, loss = 1.61572510\n",
      "Iteration 12, loss = 1.61312092\n",
      "Iteration 13, loss = 1.61079668\n",
      "Iteration 14, loss = 1.60936487\n",
      "Iteration 15, loss = 1.60781247\n",
      "Iteration 16, loss = 1.60610667\n",
      "Iteration 17, loss = 1.60495845\n",
      "Iteration 18, loss = 1.60385780\n",
      "Iteration 19, loss = 1.60273261\n",
      "Iteration 20, loss = 1.60153609\n",
      "Iteration 21, loss = 1.60027433\n",
      "Iteration 22, loss = 1.59903586\n",
      "Iteration 23, loss = 1.59766886\n",
      "Iteration 24, loss = 1.59634442\n",
      "Iteration 25, loss = 1.59492530\n",
      "Iteration 26, loss = 1.59347652\n",
      "Iteration 27, loss = 1.59208203\n",
      "Iteration 28, loss = 1.59046888\n",
      "Iteration 29, loss = 1.58898349\n",
      "Iteration 30, loss = 1.58739624\n",
      "Iteration 31, loss = 1.58573121\n",
      "Iteration 32, loss = 1.58402614\n",
      "Iteration 33, loss = 1.58228759\n",
      "Iteration 34, loss = 1.58049975\n",
      "Iteration 35, loss = 1.57855919\n",
      "Iteration 36, loss = 1.57664509\n",
      "Iteration 37, loss = 1.57469122\n",
      "Iteration 38, loss = 1.57269712\n",
      "Iteration 39, loss = 1.57043933\n",
      "Iteration 40, loss = 1.56808220\n",
      "Iteration 41, loss = 1.56572535\n",
      "Iteration 42, loss = 1.56327393\n",
      "Iteration 43, loss = 1.56065219\n",
      "Iteration 44, loss = 1.55796796\n",
      "Iteration 45, loss = 1.55524716\n",
      "Iteration 46, loss = 1.55234465\n",
      "Iteration 47, loss = 1.54924762\n",
      "Iteration 48, loss = 1.54614621\n",
      "Iteration 49, loss = 1.54300928\n",
      "Iteration 50, loss = 1.53962062\n",
      "Iteration 51, loss = 1.53626970\n",
      "Iteration 52, loss = 1.53286765\n",
      "Iteration 53, loss = 1.52939941\n",
      "Iteration 54, loss = 1.52560104\n",
      "Iteration 55, loss = 1.52191912\n",
      "Iteration 56, loss = 1.51810285\n",
      "Iteration 57, loss = 1.51419745\n",
      "Iteration 58, loss = 1.51020258\n",
      "Iteration 59, loss = 1.50599530\n",
      "Iteration 60, loss = 1.50194738\n",
      "Iteration 61, loss = 1.49765662\n",
      "Iteration 62, loss = 1.49339006\n",
      "Iteration 63, loss = 1.48913142\n",
      "Iteration 64, loss = 1.48482982\n",
      "Iteration 65, loss = 1.48016387\n",
      "Iteration 66, loss = 1.47575672\n",
      "Iteration 67, loss = 1.47114195\n",
      "Iteration 68, loss = 1.46661353\n",
      "Iteration 69, loss = 1.46201310\n",
      "Iteration 70, loss = 1.45730935\n",
      "Iteration 71, loss = 1.45263713\n",
      "Iteration 72, loss = 1.44799834\n",
      "Iteration 73, loss = 1.44338805\n",
      "Iteration 74, loss = 1.43851137\n",
      "Iteration 75, loss = 1.43378940\n",
      "Iteration 76, loss = 1.42912106\n",
      "Iteration 77, loss = 1.42431299\n",
      "Iteration 78, loss = 1.41954080\n",
      "Iteration 79, loss = 1.41479297\n",
      "Iteration 80, loss = 1.41002808\n",
      "Iteration 81, loss = 1.40516243\n",
      "Iteration 82, loss = 1.40039900\n",
      "Iteration 83, loss = 1.39562633\n",
      "Iteration 84, loss = 1.39083908\n",
      "Iteration 85, loss = 1.38605506\n",
      "Iteration 86, loss = 1.38128338\n",
      "Iteration 87, loss = 1.37663015\n",
      "Iteration 88, loss = 1.37185506\n",
      "Iteration 89, loss = 1.36722116\n",
      "Iteration 90, loss = 1.36247396\n",
      "Iteration 91, loss = 1.35775433\n",
      "Iteration 92, loss = 1.35309464\n",
      "Iteration 93, loss = 1.34852506\n",
      "Iteration 94, loss = 1.34385509\n",
      "Iteration 95, loss = 1.33929349\n",
      "Iteration 96, loss = 1.33478290\n",
      "Iteration 97, loss = 1.33021581\n",
      "Iteration 98, loss = 1.32576872\n",
      "Iteration 99, loss = 1.32134265\n",
      "Iteration 100, loss = 1.31683281\n",
      "Iteration 101, loss = 1.31252411\n",
      "Iteration 102, loss = 1.30807233\n",
      "Iteration 103, loss = 1.30377379\n",
      "Iteration 104, loss = 1.29949150\n",
      "Iteration 105, loss = 1.29519091\n",
      "Iteration 106, loss = 1.29099034\n",
      "Iteration 107, loss = 1.28685515\n",
      "Iteration 108, loss = 1.28261186\n",
      "Iteration 109, loss = 1.27849490\n",
      "Iteration 110, loss = 1.27444332\n",
      "Iteration 111, loss = 1.27040680\n",
      "Iteration 112, loss = 1.26635093\n",
      "Iteration 113, loss = 1.26238374\n",
      "Iteration 114, loss = 1.25855157\n",
      "Iteration 115, loss = 1.25463371\n",
      "Iteration 116, loss = 1.25072447\n",
      "Iteration 117, loss = 1.24694341\n",
      "Iteration 118, loss = 1.24321449\n",
      "Iteration 119, loss = 1.23938658\n",
      "Iteration 120, loss = 1.23580498\n",
      "Iteration 121, loss = 1.23208212\n",
      "Iteration 122, loss = 1.22857153\n",
      "Iteration 123, loss = 1.22484618\n",
      "Iteration 124, loss = 1.22126694\n",
      "Iteration 125, loss = 1.21775638\n",
      "Iteration 126, loss = 1.21437139\n",
      "Iteration 127, loss = 1.21080479\n",
      "Iteration 128, loss = 1.20745770\n",
      "Iteration 129, loss = 1.20405892\n",
      "Iteration 130, loss = 1.20076747\n",
      "Iteration 131, loss = 1.19743456\n",
      "Iteration 132, loss = 1.19411109\n",
      "Iteration 133, loss = 1.19090127\n",
      "Iteration 134, loss = 1.18766341\n",
      "Iteration 135, loss = 1.18467363\n",
      "Iteration 136, loss = 1.18131055\n",
      "Iteration 137, loss = 1.17822751\n",
      "Iteration 138, loss = 1.17515401\n",
      "Iteration 139, loss = 1.17206827\n",
      "Iteration 140, loss = 1.16905285\n",
      "Iteration 141, loss = 1.16607189\n",
      "Iteration 142, loss = 1.16306592\n",
      "Iteration 143, loss = 1.16008745\n",
      "Iteration 144, loss = 1.15717558\n",
      "Iteration 145, loss = 1.15427185\n",
      "Iteration 146, loss = 1.15140161\n",
      "Iteration 147, loss = 1.14856169\n",
      "Iteration 148, loss = 1.14572071\n",
      "Iteration 149, loss = 1.14293666\n",
      "Iteration 150, loss = 1.14014759\n",
      "Iteration 151, loss = 1.13742151\n",
      "Iteration 152, loss = 1.13468616\n",
      "Iteration 153, loss = 1.13192412\n",
      "Iteration 154, loss = 1.12923944\n",
      "Iteration 155, loss = 1.12654235\n",
      "Iteration 156, loss = 1.12398021\n",
      "Iteration 157, loss = 1.12121926\n",
      "Iteration 158, loss = 1.11865313\n",
      "Iteration 159, loss = 1.11606115\n",
      "Iteration 160, loss = 1.11342014\n",
      "Iteration 161, loss = 1.11095000\n",
      "Iteration 162, loss = 1.10835818\n",
      "Iteration 163, loss = 1.10598840\n",
      "Iteration 164, loss = 1.10339589\n",
      "Iteration 165, loss = 1.10093863\n",
      "Iteration 166, loss = 1.09848033\n",
      "Iteration 167, loss = 1.09596061\n",
      "Iteration 168, loss = 1.09362731\n",
      "Iteration 169, loss = 1.09116888\n",
      "Iteration 170, loss = 1.08875078\n",
      "Iteration 171, loss = 1.08646503\n",
      "Iteration 172, loss = 1.08413047\n",
      "Iteration 173, loss = 1.08178430\n",
      "Iteration 174, loss = 1.07948597\n",
      "Iteration 175, loss = 1.07718105\n",
      "Iteration 176, loss = 1.07481409\n",
      "Iteration 177, loss = 1.07260507\n",
      "Iteration 178, loss = 1.07035344\n",
      "Iteration 179, loss = 1.06812901\n",
      "Iteration 180, loss = 1.06598538\n",
      "Iteration 181, loss = 1.06369563\n",
      "Iteration 182, loss = 1.06149350\n",
      "Iteration 183, loss = 1.05925001\n",
      "Iteration 184, loss = 1.05714552\n",
      "Iteration 185, loss = 1.05490765\n",
      "Iteration 186, loss = 1.05295781\n",
      "Iteration 187, loss = 1.05062947\n",
      "Iteration 188, loss = 1.04860826\n",
      "Iteration 189, loss = 1.04649140\n",
      "Iteration 190, loss = 1.04436456\n",
      "Iteration 191, loss = 1.04225934\n",
      "Iteration 192, loss = 1.04017154\n",
      "Iteration 193, loss = 1.03818122\n",
      "Iteration 194, loss = 1.03609329\n",
      "Iteration 195, loss = 1.03405562\n",
      "Iteration 196, loss = 1.03202963\n",
      "Iteration 197, loss = 1.02993719\n",
      "Iteration 198, loss = 1.02797653\n",
      "Iteration 199, loss = 1.02591796\n",
      "Iteration 200, loss = 1.02393512\n",
      "Iteration 201, loss = 1.02191246\n",
      "Iteration 202, loss = 1.01993723\n",
      "Iteration 203, loss = 1.01798902\n",
      "Iteration 204, loss = 1.01591167\n",
      "Iteration 205, loss = 1.01406057\n",
      "Iteration 206, loss = 1.01203859\n",
      "Iteration 207, loss = 1.01001761\n",
      "Iteration 208, loss = 1.00803948\n",
      "Iteration 209, loss = 1.00608376\n",
      "Iteration 210, loss = 1.00414929\n",
      "Iteration 211, loss = 1.00220519\n",
      "Iteration 212, loss = 1.00024108\n",
      "Iteration 213, loss = 0.99830281\n",
      "Iteration 214, loss = 0.99635410\n",
      "Iteration 215, loss = 0.99446757\n",
      "Iteration 216, loss = 0.99259127\n",
      "Iteration 217, loss = 0.99064726\n",
      "Iteration 218, loss = 0.98874725\n",
      "Iteration 219, loss = 0.98684780\n",
      "Iteration 220, loss = 0.98498698\n",
      "Iteration 221, loss = 0.98310648\n",
      "Iteration 222, loss = 0.98130410\n",
      "Iteration 223, loss = 0.97932189\n",
      "Iteration 224, loss = 0.97752592\n",
      "Iteration 225, loss = 0.97559406\n",
      "Iteration 226, loss = 0.97378376\n",
      "Iteration 227, loss = 0.97191283\n",
      "Iteration 228, loss = 0.97002201\n",
      "Iteration 229, loss = 0.96820251\n",
      "Iteration 230, loss = 0.96630162\n",
      "Iteration 231, loss = 0.96453693\n",
      "Iteration 232, loss = 0.96277028\n",
      "Iteration 233, loss = 0.96076574\n",
      "Iteration 234, loss = 0.95894540\n",
      "Iteration 235, loss = 0.95710029\n",
      "Iteration 236, loss = 0.95530775\n",
      "Iteration 237, loss = 0.95349340\n",
      "Iteration 238, loss = 0.95168259\n",
      "Iteration 239, loss = 0.94999355\n",
      "Iteration 240, loss = 0.94805968\n",
      "Iteration 241, loss = 0.94627246\n",
      "Iteration 242, loss = 0.94445112\n",
      "Iteration 243, loss = 0.94270479\n",
      "Iteration 244, loss = 0.94092668\n",
      "Iteration 245, loss = 0.93909263\n",
      "Iteration 246, loss = 0.93732671\n",
      "Iteration 247, loss = 0.93558181\n",
      "Iteration 248, loss = 0.93383310\n",
      "Iteration 249, loss = 0.93209663\n",
      "Iteration 250, loss = 0.93032903\n",
      "Iteration 251, loss = 0.92863399\n",
      "Iteration 252, loss = 0.92681568\n",
      "Iteration 253, loss = 0.92512370\n",
      "Iteration 254, loss = 0.92335517\n",
      "Iteration 255, loss = 0.92170882\n",
      "Iteration 256, loss = 0.91999674\n",
      "Iteration 257, loss = 0.91837548\n",
      "Iteration 258, loss = 0.91659207\n",
      "Iteration 259, loss = 0.91482707\n",
      "Iteration 260, loss = 0.91309531\n",
      "Iteration 261, loss = 0.91133097\n",
      "Iteration 262, loss = 0.90972834\n",
      "Iteration 263, loss = 0.90800963\n",
      "Iteration 264, loss = 0.90636719\n",
      "Iteration 265, loss = 0.90467104\n",
      "Iteration 266, loss = 0.90300012\n",
      "Iteration 267, loss = 0.90134287\n",
      "Iteration 268, loss = 0.89968830\n",
      "Iteration 269, loss = 0.89810216\n",
      "Iteration 270, loss = 0.89635499\n",
      "Iteration 271, loss = 0.89473206\n",
      "Iteration 272, loss = 0.89310687\n",
      "Iteration 273, loss = 0.89144793\n",
      "Iteration 274, loss = 0.88985364\n",
      "Iteration 275, loss = 0.88821014\n",
      "Iteration 276, loss = 0.88657912\n",
      "Iteration 277, loss = 0.88495577\n",
      "Iteration 278, loss = 0.88328139\n",
      "Iteration 279, loss = 0.88174953\n",
      "Iteration 280, loss = 0.88020371\n",
      "Iteration 281, loss = 0.87862424\n",
      "Iteration 282, loss = 0.87698929\n",
      "Iteration 283, loss = 0.87540900\n",
      "Iteration 284, loss = 0.87388154\n",
      "Iteration 285, loss = 0.87228038\n",
      "Iteration 286, loss = 0.87072426\n",
      "Iteration 287, loss = 0.86922415\n",
      "Iteration 288, loss = 0.86767755\n",
      "Iteration 289, loss = 0.86623282\n",
      "Iteration 290, loss = 0.86455339\n",
      "Iteration 291, loss = 0.86306413\n",
      "Iteration 292, loss = 0.86157518\n",
      "Iteration 293, loss = 0.85995251\n",
      "Iteration 294, loss = 0.85853278\n",
      "Iteration 295, loss = 0.85698040\n",
      "Iteration 296, loss = 0.85550630\n",
      "Iteration 297, loss = 0.85401778\n",
      "Iteration 298, loss = 0.85245809\n",
      "Iteration 299, loss = 0.85104581\n",
      "Iteration 300, loss = 0.84965915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(balanceScaleValues, balanceScaleClasses):\n",
    "    data_train, target_train = balanceScaleValues[train], balanceScaleClasses[train]\n",
    "    data_test, target_test = balanceScaleValues[test], balanceScaleClasses[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       1.00      1.00      1.00       125\n",
      "           3       1.00      1.00      1.00       125\n",
      "           4       1.00      1.00      1.00       125\n",
      "           5       1.00      1.00      1.00       125\n",
      "\n",
      "    accuracy                           1.00       625\n",
      "   macro avg       1.00      1.00      1.00       625\n",
      "weighted avg       1.00      1.00      1.00       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[125   0   0   0   0]\n",
      " [  0 125   0   0   0]\n",
      " [  0   0 125   0   0]\n",
      " [  0   0   0 125   0]\n",
      " [  0   0   0   0 125]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.97      0.96       125\n",
      "           2       0.91      0.94      0.93       125\n",
      "           3       0.97      0.92      0.94       125\n",
      "           4       0.98      0.98      0.98       125\n",
      "           5       1.00      1.00      1.00       125\n",
      "\n",
      "    accuracy                           0.96       625\n",
      "   macro avg       0.96      0.96      0.96       625\n",
      "weighted avg       0.96      0.96      0.96       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[121   4   0   0   0]\n",
      " [  5 118   2   0   0]\n",
      " [  0   7 115   3   0]\n",
      " [  0   0   2 123   0]\n",
      " [  0   0   0   0 125]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       1.00      1.00      1.00       125\n",
      "           3       1.00      1.00      1.00       125\n",
      "           4       1.00      1.00      1.00       125\n",
      "           5       1.00      1.00      1.00       125\n",
      "\n",
      "    accuracy                           1.00       625\n",
      "   macro avg       1.00      1.00      1.00       625\n",
      "weighted avg       1.00      1.00      1.00       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[125   0   0   0   0]\n",
      " [  0 125   0   0   0]\n",
      " [  0   0 125   0   0]\n",
      " [  0   0   0 125   0]\n",
      " [  0   0   0   0 125]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.94      0.97       125\n",
      "           2       0.91      0.92      0.92       125\n",
      "           3       0.89      0.87      0.88       125\n",
      "           4       0.90      0.89      0.90       125\n",
      "           5       0.93      1.00      0.96       125\n",
      "\n",
      "    accuracy                           0.92       625\n",
      "   macro avg       0.93      0.92      0.92       625\n",
      "weighted avg       0.93      0.92      0.92       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[118   7   0   0   0]\n",
      " [  0 115  10   0   0]\n",
      " [  0   4 109  12   0]\n",
      " [  0   0   4 111  10]\n",
      " [  0   0   0   0 125]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.98      0.91       125\n",
      "           2       0.74      0.66      0.70       125\n",
      "           3       0.64      0.50      0.57       125\n",
      "           4       0.57      0.31      0.40       125\n",
      "           5       0.61      0.98      0.75       125\n",
      "\n",
      "    accuracy                           0.69       625\n",
      "   macro avg       0.68      0.69      0.67       625\n",
      "weighted avg       0.68      0.69      0.67       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[122   3   0   0   0]\n",
      " [ 21  83  20   1   0]\n",
      " [  0  25  63  27  10]\n",
      " [  0   1  15  39  70]\n",
      " [  0   0   0   2 123]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(balanceScaleClasses, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusão: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(balanceScaleClasses, predicted_classes[classificador]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}