{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset teaching Assistant Evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nativeEnglishSpeaker</th>\n",
       "      <th>courseInstructor</th>\n",
       "      <th>course</th>\n",
       "      <th>summerOrRegularSemester</th>\n",
       "      <th>classSize</th>\n",
       "      <th>resultado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nativeEnglishSpeaker  courseInstructor  course  summerOrRegularSemester  \\\n",
       "0                       1                23       3                        1   \n",
       "1                       2                15       3                        1   \n",
       "2                       1                23       3                        2   \n",
       "3                       1                 5       2                        2   \n",
       "4                       2                 7      11                        2   \n",
       "5                       2                23       3                        1   \n",
       "6                       2                 9       5                        2   \n",
       "7                       2                10       3                        2   \n",
       "8                       1                22       3                        1   \n",
       "9                       2                15       3                        1   \n",
       "10                      2                10      22                        2   \n",
       "11                      2                13       1                        2   \n",
       "12                      2                18      21                        2   \n",
       "13                      2                 6      17                        2   \n",
       "14                      2                 6      17                        2   \n",
       "15                      2                 6      17                        2   \n",
       "16                      2                 7      11                        2   \n",
       "17                      2                22       3                        2   \n",
       "18                      2                13       3                        1   \n",
       "19                      2                 7      25                        2   \n",
       "20                      2                25       7                        2   \n",
       "21                      2                25       7                        2   \n",
       "22                      2                 2       9                        2   \n",
       "23                      2                 1      15                        1   \n",
       "24                      2                15      13                        2   \n",
       "25                      2                 7      11                        2   \n",
       "26                      2                 8       3                        2   \n",
       "27                      2                14      15                        2   \n",
       "28                      2                21       2                        2   \n",
       "29                      2                22       3                        2   \n",
       "..                    ...               ...     ...                      ...   \n",
       "121                     2                13      14                        2   \n",
       "122                     2                 9       6                        2   \n",
       "123                     1                10       3                        2   \n",
       "124                     2                14      15                        2   \n",
       "125                     1                13       1                        2   \n",
       "126                     1                 8       3                        2   \n",
       "127                     2                20       2                        2   \n",
       "128                     2                22       1                        2   \n",
       "129                     2                18      12                        2   \n",
       "130                     2                20      15                        2   \n",
       "131                     1                17      18                        2   \n",
       "132                     2                14      23                        2   \n",
       "133                     2                24      26                        2   \n",
       "134                     2                 9      24                        2   \n",
       "135                     2                12       8                        2   \n",
       "136                     2                 9       6                        2   \n",
       "137                     2                22       1                        2   \n",
       "138                     2                 7      11                        2   \n",
       "139                     2                10       3                        2   \n",
       "140                     2                23       3                        2   \n",
       "141                     2                17      18                        2   \n",
       "142                     2                16      20                        2   \n",
       "143                     2                 3       2                        2   \n",
       "144                     2                19       4                        2   \n",
       "145                     2                23       3                        2   \n",
       "146                     2                 3       2                        2   \n",
       "147                     2                10       3                        2   \n",
       "148                     1                18       7                        2   \n",
       "149                     2                22       1                        2   \n",
       "150                     2                 2      10                        2   \n",
       "\n",
       "     classSize  resultado  \n",
       "0           19          3  \n",
       "1           17          3  \n",
       "2           49          3  \n",
       "3           33          3  \n",
       "4           55          3  \n",
       "5           20          3  \n",
       "6           19          3  \n",
       "7           27          3  \n",
       "8           58          3  \n",
       "9           20          3  \n",
       "10           9          3  \n",
       "11          30          3  \n",
       "12          29          3  \n",
       "13          39          3  \n",
       "14          42          2  \n",
       "15          43          2  \n",
       "16          10          2  \n",
       "17          46          2  \n",
       "18          10          2  \n",
       "19          42          2  \n",
       "20          27          2  \n",
       "21          23          2  \n",
       "22          31          2  \n",
       "23          22          2  \n",
       "24          37          2  \n",
       "25          13          2  \n",
       "26          24          2  \n",
       "27          38          2  \n",
       "28          42          1  \n",
       "29          28          1  \n",
       "..         ...        ...  \n",
       "121         17          3  \n",
       "122          7          3  \n",
       "123         21          3  \n",
       "124         36          3  \n",
       "125         54          3  \n",
       "126         29          3  \n",
       "127         45          3  \n",
       "128         11          2  \n",
       "129         16          2  \n",
       "130         18          2  \n",
       "131         44          2  \n",
       "132         17          2  \n",
       "133         21          2  \n",
       "134         20          2  \n",
       "135         24          2  \n",
       "136          5          2  \n",
       "137         42          2  \n",
       "138         30          1  \n",
       "139         19          1  \n",
       "140         11          1  \n",
       "141         29          1  \n",
       "142         15          1  \n",
       "143         37          1  \n",
       "144         10          1  \n",
       "145         24          1  \n",
       "146         26          1  \n",
       "147         12          1  \n",
       "148         48          1  \n",
       "149         51          1  \n",
       "150         27          1  \n",
       "\n",
       "[151 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree, preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "dataset_teaching = pd.read_csv('teachingAssistantEvaluation.data', sep=',')\n",
    "print(\"\\nDataset teaching Assistant Evaluation:\")\n",
    "dataset_teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nativeEnglishSpeaker', 'courseInstructor', 'course',\n",
       "       'summerOrRegularSemester', 'classSize', 'resultado'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_teaching.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Teaching Assistant Evaluation normalizada:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nativeEnglishSpeaker</th>\n",
       "      <th>courseInstructor</th>\n",
       "      <th>course</th>\n",
       "      <th>summerOrRegularSemester</th>\n",
       "      <th>classSize</th>\n",
       "      <th>resultado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.375485</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.690028</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.199557</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.845658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.375485</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.644422</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.270352</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.399382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.976370</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2.111312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1.375485</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.612213</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.682388</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.690028</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.067508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1.228494</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.344757</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0.199557</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.612213</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.468178</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.094425</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.165937</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.640530</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.088122</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.123361</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866272</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.123361</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1.099717</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.123361</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1.177532</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.976370</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.390363</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1.228494</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.410977</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.094425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.390363</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.976370</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1.099717</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1.669467</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.067508</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1.669467</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.378768</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.711325</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.243752</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.858316</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.456583</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>0.199557</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710642</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.976370</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.156918</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.829379</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.300953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>0.052566</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.788457</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>1.081503</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.099717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>1.228494</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.094425</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.845658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.682388</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.623808</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.534398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>0.052566</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.632827</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.094425</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.033497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.829379</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.088122</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>0.934512</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333162</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2</td>\n",
       "      <td>1.228494</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.312548</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>0.640530</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.923473</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>0.934512</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.767843</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>0.493539</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1.255347</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>0.052566</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.845658</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2</td>\n",
       "      <td>1.522476</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.534398</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.682388</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.612213</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.241416</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.300953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.682388</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.779438</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2</td>\n",
       "      <td>1.228494</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.099717</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.976370</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.165937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.690028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2</td>\n",
       "      <td>1.375485</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.312548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2</td>\n",
       "      <td>0.493539</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.088122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2</td>\n",
       "      <td>0.346548</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.001288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.564334</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710642</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2</td>\n",
       "      <td>0.787521</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.390363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "      <td>1.375485</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.300953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.564334</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.145323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.234733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>0.640530</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.566607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "      <td>1.228494</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.800052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.711325</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.067508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nativeEnglishSpeaker  courseInstructor  course  summerOrRegularSemester  \\\n",
       "0                       1          1.375485       3                        1   \n",
       "1                       2          0.199557       3                        1   \n",
       "2                       1          1.375485       3                        2   \n",
       "3                       1         -1.270352       2                        2   \n",
       "4                       2         -0.976370      11                        2   \n",
       "5                       2          1.375485       3                        1   \n",
       "6                       2         -0.682388       5                        2   \n",
       "7                       2         -0.535398       3                        2   \n",
       "8                       1          1.228494       3                        1   \n",
       "9                       2          0.199557       3                        1   \n",
       "10                      2         -0.535398      22                        2   \n",
       "11                      2         -0.094425       1                        2   \n",
       "12                      2          0.640530      21                        2   \n",
       "13                      2         -1.123361      17                        2   \n",
       "14                      2         -1.123361      17                        2   \n",
       "15                      2         -1.123361      17                        2   \n",
       "16                      2         -0.976370      11                        2   \n",
       "17                      2          1.228494       3                        2   \n",
       "18                      2         -0.094425       3                        1   \n",
       "19                      2         -0.976370      25                        2   \n",
       "20                      2          1.669467       7                        2   \n",
       "21                      2          1.669467       7                        2   \n",
       "22                      2         -1.711325       9                        2   \n",
       "23                      2         -1.858316      15                        1   \n",
       "24                      2          0.199557      13                        2   \n",
       "25                      2         -0.976370      11                        2   \n",
       "26                      2         -0.829379       3                        2   \n",
       "27                      2          0.052566      15                        2   \n",
       "28                      2          1.081503       2                        2   \n",
       "29                      2          1.228494       3                        2   \n",
       "..                    ...               ...     ...                      ...   \n",
       "121                     2         -0.094425      14                        2   \n",
       "122                     2         -0.682388       6                        2   \n",
       "123                     1         -0.535398       3                        2   \n",
       "124                     2          0.052566      15                        2   \n",
       "125                     1         -0.094425       1                        2   \n",
       "126                     1         -0.829379       3                        2   \n",
       "127                     2          0.934512       2                        2   \n",
       "128                     2          1.228494       1                        2   \n",
       "129                     2          0.640530      12                        2   \n",
       "130                     2          0.934512      15                        2   \n",
       "131                     1          0.493539      18                        2   \n",
       "132                     2          0.052566      23                        2   \n",
       "133                     2          1.522476      26                        2   \n",
       "134                     2         -0.682388      24                        2   \n",
       "135                     2         -0.241416       8                        2   \n",
       "136                     2         -0.682388       6                        2   \n",
       "137                     2          1.228494       1                        2   \n",
       "138                     2         -0.976370      11                        2   \n",
       "139                     2         -0.535398       3                        2   \n",
       "140                     2          1.375485       3                        2   \n",
       "141                     2          0.493539      18                        2   \n",
       "142                     2          0.346548      20                        2   \n",
       "143                     2         -1.564334       2                        2   \n",
       "144                     2          0.787521       4                        2   \n",
       "145                     2          1.375485       3                        2   \n",
       "146                     2         -1.564334       2                        2   \n",
       "147                     2         -0.535398       3                        2   \n",
       "148                     1          0.640530       7                        2   \n",
       "149                     2          1.228494       1                        2   \n",
       "150                     2         -1.711325      10                        2   \n",
       "\n",
       "     classSize  resultado  \n",
       "0    -0.690028          3  \n",
       "1    -0.845658          3  \n",
       "2     1.644422          3  \n",
       "3     0.399382          3  \n",
       "4     2.111312          3  \n",
       "5    -0.612213          3  \n",
       "6    -0.690028          3  \n",
       "7    -0.067508          3  \n",
       "8     2.344757          3  \n",
       "9    -0.612213          3  \n",
       "10   -1.468178          3  \n",
       "11    0.165937          3  \n",
       "12    0.088122          3  \n",
       "13    0.866272          3  \n",
       "14    1.099717          2  \n",
       "15    1.177532          2  \n",
       "16   -1.390363          2  \n",
       "17    1.410977          2  \n",
       "18   -1.390363          2  \n",
       "19    1.099717          2  \n",
       "20   -0.067508          2  \n",
       "21   -0.378768          2  \n",
       "22    0.243752          2  \n",
       "23   -0.456583          2  \n",
       "24    0.710642          2  \n",
       "25   -1.156918          2  \n",
       "26   -0.300953          2  \n",
       "27    0.788457          2  \n",
       "28    1.099717          1  \n",
       "29    0.010307          1  \n",
       "..         ...        ...  \n",
       "121  -0.845658          3  \n",
       "122  -1.623808          3  \n",
       "123  -0.534398          3  \n",
       "124   0.632827          3  \n",
       "125   2.033497          3  \n",
       "126   0.088122          3  \n",
       "127   1.333162          3  \n",
       "128  -1.312548          2  \n",
       "129  -0.923473          2  \n",
       "130  -0.767843          2  \n",
       "131   1.255347          2  \n",
       "132  -0.845658          2  \n",
       "133  -0.534398          2  \n",
       "134  -0.612213          2  \n",
       "135  -0.300953          2  \n",
       "136  -1.779438          2  \n",
       "137   1.099717          2  \n",
       "138   0.165937          1  \n",
       "139  -0.690028          1  \n",
       "140  -1.312548          1  \n",
       "141   0.088122          1  \n",
       "142  -1.001288          1  \n",
       "143   0.710642          1  \n",
       "144  -1.390363          1  \n",
       "145  -0.300953          1  \n",
       "146  -0.145323          1  \n",
       "147  -1.234733          1  \n",
       "148   1.566607          1  \n",
       "149   1.800052          1  \n",
       "150  -0.067508          1  \n",
       "\n",
       "[151 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teaching_normalizada = dataset_teaching.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "teaching_normalizada = normalizador.fit_transform(dataset_teaching)\n",
    "dataset_teaching['courseInstructor'] = teaching_normalizada[:,1]\n",
    "dataset_teaching['classSize'] = teaching_normalizada[:,4]\n",
    "\n",
    "print(\"\\nDataset Teaching Assistant Evaluation normalizada:\")\n",
    "dataset_teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teaching Assistant Evaluation features:\n",
      "\n",
      "[[ 1.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.90028366e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  3.00000000e+00  1.00000000e+00\n",
      "  -8.45658364e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.64442160e+00  3.00000000e+00]\n",
      " [ 1.00000000e+00 -1.27035233e+00  2.00000000e+00  2.00000000e+00\n",
      "   3.99381616e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "   2.11131159e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  5.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  3.00000000e+00  2.00000000e+00\n",
      "  -6.75083764e-02  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.22849398e+00  3.00000000e+00  1.00000000e+00\n",
      "   2.34475658e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  2.20000000e+01  2.00000000e+00\n",
      "  -1.46817835e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "   1.65936620e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  2.10000000e+01  2.00000000e+00\n",
      "   8.81216211e-02  3.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   8.66271608e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   1.09971660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   1.17753160e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "  -1.39036335e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.41097660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  3.00000000e+00  1.00000000e+00\n",
      "  -1.39036335e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  2.50000000e+01  2.00000000e+00\n",
      "   1.09971660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.66946686e+00  7.00000000e+00  2.00000000e+00\n",
      "  -6.75083764e-02  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.66946686e+00  7.00000000e+00  2.00000000e+00\n",
      "  -3.78768371e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.71132521e+00  9.00000000e+00  2.00000000e+00\n",
      "   2.43751619e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.85831617e+00  1.50000000e+01  1.00000000e+00\n",
      "  -4.56583370e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  1.30000000e+01  2.00000000e+00\n",
      "   7.10641611e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "  -1.15691836e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -8.29379453e-01  3.00000000e+00  2.00000000e+00\n",
      "  -3.00953373e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  1.50000000e+01  2.00000000e+00\n",
      "   7.88456610e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.08150302e+00  2.00000000e+00  2.00000000e+00\n",
      "   1.09971660e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.03066223e-02  1.00000000e+00]\n",
      " [ 2.00000000e+00 -3.88406575e-01  1.00000000e+00  2.00000000e+00\n",
      "   1.80005159e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  5.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "   2.43751619e-01  1.00000000e+00]\n",
      " [ 1.00000000e+00 -9.44246560e-02  3.00000000e+00  1.00000000e+00\n",
      "  -1.15691836e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.27035233e+00  2.00000000e+00  2.00000000e+00\n",
      "   7.10641611e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00  3.46548222e-01  8.00000000e+00  2.00000000e+00\n",
      "   6.32826612e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.41734329e+00  1.60000000e+01  2.00000000e+00\n",
      "  -5.34398369e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.27035233e+00  2.00000000e+00  2.00000000e+00\n",
      "   1.56660660e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  1.50000000e+01  2.00000000e+00\n",
      "   7.88456610e-01  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.90028366e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  3.00000000e+00  1.00000000e+00\n",
      "  -8.45658364e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.64442160e+00  3.00000000e+00]\n",
      " [ 1.00000000e+00 -1.27035233e+00  2.00000000e+00  2.00000000e+00\n",
      "   3.99381616e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "   2.11131159e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  5.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  3.00000000e+00  2.00000000e+00\n",
      "  -6.75083764e-02  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "   2.34475658e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  2.20000000e+01  2.00000000e+00\n",
      "  -1.46817835e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "   1.65936620e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  2.10000000e+01  2.00000000e+00\n",
      "   8.81216211e-02  3.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   8.66271608e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   1.09971660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   1.17753160e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "  -1.39036335e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.41097660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  3.00000000e+00  1.00000000e+00\n",
      "  -1.39036335e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  2.50000000e+01  2.00000000e+00\n",
      "   1.09971660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.66946686e+00  7.00000000e+00  2.00000000e+00\n",
      "  -6.75083764e-02  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.66946686e+00  7.00000000e+00  2.00000000e+00\n",
      "  -3.78768371e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.71132521e+00  9.00000000e+00  2.00000000e+00\n",
      "   2.43751619e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.85831617e+00  1.50000000e+01  1.00000000e+00\n",
      "  -4.56583370e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  1.30000000e+01  2.00000000e+00\n",
      "   7.10641611e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "  -1.15691836e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -8.29379453e-01  3.00000000e+00  2.00000000e+00\n",
      "  -3.00953373e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  1.50000000e+01  2.00000000e+00\n",
      "   7.88456610e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.08150302e+00  2.00000000e+00  2.00000000e+00\n",
      "   1.09971660e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.03066223e-02  1.00000000e+00]\n",
      " [ 2.00000000e+00 -3.88406575e-01  1.00000000e+00  2.00000000e+00\n",
      "   1.80005159e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  5.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "   2.43751619e-01  1.00000000e+00]\n",
      " [ 1.00000000e+00 -9.44246560e-02  3.00000000e+00  1.00000000e+00\n",
      "  -1.15691836e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.27035233e+00  2.00000000e+00  2.00000000e+00\n",
      "   7.10641611e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00  3.46548222e-01  8.00000000e+00  2.00000000e+00\n",
      "   6.32826612e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.41734329e+00  1.60000000e+01  2.00000000e+00\n",
      "  -5.34398369e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.27035233e+00  2.00000000e+00  2.00000000e+00\n",
      "   1.56660660e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  1.50000000e+01  2.00000000e+00\n",
      "   7.88456610e-01  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -2.23138374e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00 -9.44246560e-02  3.00000000e+00  1.00000000e+00\n",
      "  -8.45658364e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  3.46548222e-01  1.90000000e+01  2.00000000e+00\n",
      "  -1.31254836e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  2.00000000e+00  2.00000000e+00\n",
      "   8.66271608e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  3.00000000e+00  1.00000000e+00\n",
      "  -1.31254836e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  2.10000000e+01  2.00000000e+00\n",
      "  -6.90028366e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "   1.33316160e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "   7.88456610e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  2.20000000e+01  2.00000000e+00\n",
      "  -8.45658364e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  4.93539181e-01  1.70000000e+01  2.00000000e+00\n",
      "  -6.90028366e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  5.00000000e+00  2.00000000e+00\n",
      "  -3.00953373e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  2.50000000e+01  2.00000000e+00\n",
      "  -2.23138374e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00  4.93539181e-01  1.70000000e+01  2.00000000e+00\n",
      "   2.43751619e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -1.85831617e+00  1.50000000e+01  2.00000000e+00\n",
      "   2.43751619e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -1.85831617e+00  8.00000000e+00  2.00000000e+00\n",
      "  -7.67843365e-01  2.00000000e+00]\n",
      " [ 1.00000000e+00 -3.88406575e-01  1.60000000e+01  2.00000000e+00\n",
      "  -4.56583370e-01  2.00000000e+00]\n",
      " [ 1.00000000e+00  1.22849398e+00  1.30000000e+01  2.00000000e+00\n",
      "  -6.75083764e-02  2.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  2.00000000e+00  2.00000000e+00\n",
      "  -1.07910336e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "  -6.12213367e-01  2.00000000e+00]\n",
      " [ 1.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   5.55011613e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  2.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  1.00000000e+00\n",
      "  -6.12213367e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -1.12336137e+00  1.70000000e+01  2.00000000e+00\n",
      "   7.10641611e-01  2.00000000e+00]\n",
      " [ 1.00000000e+00  1.22849398e+00  3.00000000e+00  2.00000000e+00\n",
      "  -1.00128836e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  9.34512059e-01  2.00000000e+00  2.00000000e+00\n",
      "  -2.23138374e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "  -1.39036335e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  9.34512059e-01  2.00000000e+00  2.00000000e+00\n",
      "  -1.07910336e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "   7.88456610e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "   8.81216211e-02  1.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  3.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "   1.65936620e-01  1.00000000e+00]\n",
      " [ 1.00000000e+00  5.25663033e-02  1.50000000e+01  2.00000000e+00\n",
      "   3.21566617e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -8.29379453e-01  3.00000000e+00  2.00000000e+00\n",
      "  -6.75083764e-02  1.00000000e+00]\n",
      " [ 2.00000000e+00 -2.41415615e-01  7.00000000e+00  2.00000000e+00\n",
      "   4.77196615e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -8.29379453e-01  7.00000000e+00  2.00000000e+00\n",
      "  -3.78768371e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  1.00000000e+00  2.00000000e+00\n",
      "   2.96727657e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "  -1.23473336e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.71132521e+00  9.00000000e+00  2.00000000e+00\n",
      "   8.81216211e-02  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.99557263e-01  1.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00  9.34512059e-01  2.00000000e+00  2.00000000e+00\n",
      "  -1.93506835e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00 -9.44246560e-02  1.40000000e+01  2.00000000e+00\n",
      "  -8.45658364e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  6.00000000e+00  2.00000000e+00\n",
      "  -1.62380835e+00  3.00000000e+00]\n",
      " [ 1.00000000e+00 -5.35397534e-01  3.00000000e+00  2.00000000e+00\n",
      "  -5.34398369e-01  3.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  1.50000000e+01  2.00000000e+00\n",
      "   6.32826612e-01  3.00000000e+00]\n",
      " [ 1.00000000e+00 -9.44246560e-02  1.00000000e+00  2.00000000e+00\n",
      "   2.03349659e+00  3.00000000e+00]\n",
      " [ 1.00000000e+00 -8.29379453e-01  3.00000000e+00  2.00000000e+00\n",
      "   8.81216211e-02  3.00000000e+00]\n",
      " [ 2.00000000e+00  9.34512059e-01  2.00000000e+00  2.00000000e+00\n",
      "   1.33316160e+00  3.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  1.00000000e+00  2.00000000e+00\n",
      "  -1.31254836e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  6.40530141e-01  1.20000000e+01  2.00000000e+00\n",
      "  -9.23473362e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  9.34512059e-01  1.50000000e+01  2.00000000e+00\n",
      "  -7.67843365e-01  2.00000000e+00]\n",
      " [ 1.00000000e+00  4.93539181e-01  1.80000000e+01  2.00000000e+00\n",
      "   1.25534660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  5.25663033e-02  2.30000000e+01  2.00000000e+00\n",
      "  -8.45658364e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.52247590e+00  2.60000000e+01  2.00000000e+00\n",
      "  -5.34398369e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  2.40000000e+01  2.00000000e+00\n",
      "  -6.12213367e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -2.41415615e-01  8.00000000e+00  2.00000000e+00\n",
      "  -3.00953373e-01  2.00000000e+00]\n",
      " [ 2.00000000e+00 -6.82388493e-01  6.00000000e+00  2.00000000e+00\n",
      "  -1.77943835e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  1.00000000e+00  2.00000000e+00\n",
      "   1.09971660e+00  2.00000000e+00]\n",
      " [ 2.00000000e+00 -9.76370412e-01  1.10000000e+01  2.00000000e+00\n",
      "   1.65936620e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  3.00000000e+00  2.00000000e+00\n",
      "  -6.90028366e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "  -1.31254836e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  4.93539181e-01  1.80000000e+01  2.00000000e+00\n",
      "   8.81216211e-02  1.00000000e+00]\n",
      " [ 2.00000000e+00  3.46548222e-01  2.00000000e+01  2.00000000e+00\n",
      "  -1.00128836e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.56433425e+00  2.00000000e+00  2.00000000e+00\n",
      "   7.10641611e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00  7.87521100e-01  4.00000000e+00  2.00000000e+00\n",
      "  -1.39036335e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.37548494e+00  3.00000000e+00  2.00000000e+00\n",
      "  -3.00953373e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.56433425e+00  2.00000000e+00  2.00000000e+00\n",
      "  -1.45323375e-01  1.00000000e+00]\n",
      " [ 2.00000000e+00 -5.35397534e-01  3.00000000e+00  2.00000000e+00\n",
      "  -1.23473336e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  6.40530141e-01  7.00000000e+00  2.00000000e+00\n",
      "   1.56660660e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00  1.22849398e+00  1.00000000e+00  2.00000000e+00\n",
      "   1.80005159e+00  1.00000000e+00]\n",
      " [ 2.00000000e+00 -1.71132521e+00  1.00000000e+01  2.00000000e+00\n",
      "  -6.75083764e-02  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "teaching_values = dataset_teaching.iloc[:,0:6].values\n",
    "print(\"\\nTeaching Assistant Evaluation features:\\n\")\n",
    "print(teaching_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teaching Assistant Evaluation features classes:\n",
      "\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1\n",
      " 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "\n",
      "Teaching Assistant Evaluation features classes shape:\n",
      "(151,)\n"
     ]
    }
   ],
   "source": [
    "teaching_classes = dataset_teaching.iloc[:,5].values\n",
    "print(\"\\nTeaching Assistant Evaluation features classes:\\n\")\n",
    "print(teaching_classes)\n",
    "print(\"\\nTeaching Assistant Evaluation features classes shape:\")\n",
    "print(teaching_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(teaching_classes.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(teaching_classes.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(teaching_classes.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(teaching_classes.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(teaching_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11508265\n",
      "Iteration 2, loss = 1.11457218\n",
      "Iteration 3, loss = 1.11385808\n",
      "Iteration 4, loss = 1.11297440\n",
      "Iteration 5, loss = 1.11195363\n",
      "Iteration 6, loss = 1.11082606\n",
      "Iteration 7, loss = 1.10961920\n",
      "Iteration 8, loss = 1.10835764\n",
      "Iteration 9, loss = 1.10706279\n",
      "Iteration 10, loss = 1.10575301\n",
      "Iteration 11, loss = 1.10444365\n",
      "Iteration 12, loss = 1.10314727\n",
      "Iteration 13, loss = 1.10187384\n",
      "Iteration 14, loss = 1.10063103\n",
      "Iteration 15, loss = 1.09942446\n",
      "Iteration 16, loss = 1.09825800\n",
      "Iteration 17, loss = 1.09713401\n",
      "Iteration 18, loss = 1.09605358\n",
      "Iteration 19, loss = 1.09501680\n",
      "Iteration 20, loss = 1.09402294\n",
      "Iteration 21, loss = 1.09307063\n",
      "Iteration 22, loss = 1.09215803\n",
      "Iteration 23, loss = 1.09128299\n",
      "Iteration 24, loss = 1.09044311\n",
      "Iteration 25, loss = 1.08963588\n",
      "Iteration 26, loss = 1.08885878\n",
      "Iteration 27, loss = 1.08810929\n",
      "Iteration 28, loss = 1.08738494\n",
      "Iteration 29, loss = 1.08668343\n",
      "Iteration 30, loss = 1.08600253\n",
      "Iteration 31, loss = 1.08534022\n",
      "Iteration 32, loss = 1.08469460\n",
      "Iteration 33, loss = 1.08406397\n",
      "Iteration 34, loss = 1.08344679\n",
      "Iteration 35, loss = 1.08284166\n",
      "Iteration 36, loss = 1.08224738\n",
      "Iteration 37, loss = 1.08166285\n",
      "Iteration 38, loss = 1.08108715\n",
      "Iteration 39, loss = 1.08051945\n",
      "Iteration 40, loss = 1.07995906\n",
      "Iteration 41, loss = 1.07940537\n",
      "Iteration 42, loss = 1.07885787\n",
      "Iteration 43, loss = 1.07831613\n",
      "Iteration 44, loss = 1.07777979\n",
      "Iteration 45, loss = 1.07724853\n",
      "Iteration 46, loss = 1.07672210\n",
      "Iteration 47, loss = 1.07620029\n",
      "Iteration 48, loss = 1.07568290\n",
      "Iteration 49, loss = 1.07516979\n",
      "Iteration 50, loss = 1.07466081\n",
      "Iteration 51, loss = 1.07415586\n",
      "Iteration 52, loss = 1.07365482\n",
      "Iteration 53, loss = 1.07315760\n",
      "Iteration 54, loss = 1.07266411\n",
      "Iteration 55, loss = 1.07217426\n",
      "Iteration 56, loss = 1.07168796\n",
      "Iteration 57, loss = 1.07120514\n",
      "Iteration 58, loss = 1.07072570\n",
      "Iteration 59, loss = 1.07024957\n",
      "Iteration 60, loss = 1.06977665\n",
      "Iteration 61, loss = 1.06930686\n",
      "Iteration 62, loss = 1.06884009\n",
      "Iteration 63, loss = 1.06837626\n",
      "Iteration 64, loss = 1.06791528\n",
      "Iteration 65, loss = 1.06745704\n",
      "Iteration 66, loss = 1.06700144\n",
      "Iteration 67, loss = 1.06654839\n",
      "Iteration 68, loss = 1.06609777\n",
      "Iteration 69, loss = 1.06564950\n",
      "Iteration 70, loss = 1.06520346\n",
      "Iteration 71, loss = 1.06475956\n",
      "Iteration 72, loss = 1.06431768\n",
      "Iteration 73, loss = 1.06387774\n",
      "Iteration 74, loss = 1.06343963\n",
      "Iteration 75, loss = 1.06300324\n",
      "Iteration 76, loss = 1.06256848\n",
      "Iteration 77, loss = 1.06213525\n",
      "Iteration 78, loss = 1.06170346\n",
      "Iteration 79, loss = 1.06127300\n",
      "Iteration 80, loss = 1.06084378\n",
      "Iteration 81, loss = 1.06041572\n",
      "Iteration 82, loss = 1.05998871\n",
      "Iteration 83, loss = 1.05956267\n",
      "Iteration 84, loss = 1.05913751\n",
      "Iteration 85, loss = 1.05871315\n",
      "Iteration 86, loss = 1.05828948\n",
      "Iteration 87, loss = 1.05786644\n",
      "Iteration 88, loss = 1.05744393\n",
      "Iteration 89, loss = 1.05702187\n",
      "Iteration 90, loss = 1.05660018\n",
      "Iteration 91, loss = 1.05617878\n",
      "Iteration 92, loss = 1.05575759\n",
      "Iteration 93, loss = 1.05533652\n",
      "Iteration 94, loss = 1.05491549\n",
      "Iteration 95, loss = 1.05449443\n",
      "Iteration 96, loss = 1.05407326\n",
      "Iteration 97, loss = 1.05365190\n",
      "Iteration 98, loss = 1.05323026\n",
      "Iteration 99, loss = 1.05280828\n",
      "Iteration 100, loss = 1.05238586\n",
      "Iteration 101, loss = 1.05196295\n",
      "Iteration 102, loss = 1.05153945\n",
      "Iteration 103, loss = 1.05111528\n",
      "Iteration 104, loss = 1.05069037\n",
      "Iteration 105, loss = 1.05026465\n",
      "Iteration 106, loss = 1.04983802\n",
      "Iteration 107, loss = 1.04941042\n",
      "Iteration 108, loss = 1.04898176\n",
      "Iteration 109, loss = 1.04855196\n",
      "Iteration 110, loss = 1.04812094\n",
      "Iteration 111, loss = 1.04768862\n",
      "Iteration 112, loss = 1.04725492\n",
      "Iteration 113, loss = 1.04681975\n",
      "Iteration 114, loss = 1.04638304\n",
      "Iteration 115, loss = 1.04594470\n",
      "Iteration 116, loss = 1.04550465\n",
      "Iteration 117, loss = 1.04506280\n",
      "Iteration 118, loss = 1.04461907\n",
      "Iteration 119, loss = 1.04417337\n",
      "Iteration 120, loss = 1.04372561\n",
      "Iteration 121, loss = 1.04327572\n",
      "Iteration 122, loss = 1.04282359\n",
      "Iteration 123, loss = 1.04236914\n",
      "Iteration 124, loss = 1.04191229\n",
      "Iteration 125, loss = 1.04145294\n",
      "Iteration 126, loss = 1.04099099\n",
      "Iteration 127, loss = 1.04052636\n",
      "Iteration 128, loss = 1.04005896\n",
      "Iteration 129, loss = 1.03958868\n",
      "Iteration 130, loss = 1.03911544\n",
      "Iteration 131, loss = 1.03863913\n",
      "Iteration 132, loss = 1.03815966\n",
      "Iteration 133, loss = 1.03767693\n",
      "Iteration 134, loss = 1.03719083\n",
      "Iteration 135, loss = 1.03670128\n",
      "Iteration 136, loss = 1.03620816\n",
      "Iteration 137, loss = 1.03571137\n",
      "Iteration 138, loss = 1.03521081\n",
      "Iteration 139, loss = 1.03470637\n",
      "Iteration 140, loss = 1.03419794\n",
      "Iteration 141, loss = 1.03368541\n",
      "Iteration 142, loss = 1.03316867\n",
      "Iteration 143, loss = 1.03264762\n",
      "Iteration 144, loss = 1.03212213\n",
      "Iteration 145, loss = 1.03159209\n",
      "Iteration 146, loss = 1.03105738\n",
      "Iteration 147, loss = 1.03051789\n",
      "Iteration 148, loss = 1.02997350\n",
      "Iteration 149, loss = 1.02942407\n",
      "Iteration 150, loss = 1.02886950\n",
      "Iteration 151, loss = 1.02830965\n",
      "Iteration 152, loss = 1.02774439\n",
      "Iteration 153, loss = 1.02717360\n",
      "Iteration 154, loss = 1.02659714\n",
      "Iteration 155, loss = 1.02601489\n",
      "Iteration 156, loss = 1.02542670\n",
      "Iteration 157, loss = 1.02483245\n",
      "Iteration 158, loss = 1.02423198\n",
      "Iteration 159, loss = 1.02362515\n",
      "Iteration 160, loss = 1.02301183\n",
      "Iteration 161, loss = 1.02239187\n",
      "Iteration 162, loss = 1.02176512\n",
      "Iteration 163, loss = 1.02113142\n",
      "Iteration 164, loss = 1.02049062\n",
      "Iteration 165, loss = 1.01984258\n",
      "Iteration 166, loss = 1.01918712\n",
      "Iteration 167, loss = 1.01852409\n",
      "Iteration 168, loss = 1.01785332\n",
      "Iteration 169, loss = 1.01717466\n",
      "Iteration 170, loss = 1.01648793\n",
      "Iteration 171, loss = 1.01579295\n",
      "Iteration 172, loss = 1.01508956\n",
      "Iteration 173, loss = 1.01437759\n",
      "Iteration 174, loss = 1.01365684\n",
      "Iteration 175, loss = 1.01292715\n",
      "Iteration 176, loss = 1.01218832\n",
      "Iteration 177, loss = 1.01144018\n",
      "Iteration 178, loss = 1.01068253\n",
      "Iteration 179, loss = 1.00991519\n",
      "Iteration 180, loss = 1.00913796\n",
      "Iteration 181, loss = 1.00835066\n",
      "Iteration 182, loss = 1.00755308\n",
      "Iteration 183, loss = 1.00674504\n",
      "Iteration 184, loss = 1.00592635\n",
      "Iteration 185, loss = 1.00509680\n",
      "Iteration 186, loss = 1.00425621\n",
      "Iteration 187, loss = 1.00340438\n",
      "Iteration 188, loss = 1.00254113\n",
      "Iteration 189, loss = 1.00166627\n",
      "Iteration 190, loss = 1.00077962\n",
      "Iteration 191, loss = 0.99988100\n",
      "Iteration 192, loss = 0.99897025\n",
      "Iteration 193, loss = 0.99804721\n",
      "Iteration 194, loss = 0.99711173\n",
      "Iteration 195, loss = 0.99616367\n",
      "Iteration 196, loss = 0.99520290\n",
      "Iteration 197, loss = 0.99422932\n",
      "Iteration 198, loss = 0.99324284\n",
      "Iteration 199, loss = 0.99224338\n",
      "Iteration 200, loss = 0.99123089\n",
      "Iteration 201, loss = 0.99020536\n",
      "Iteration 202, loss = 0.98916677\n",
      "Iteration 203, loss = 0.98811515\n",
      "Iteration 204, loss = 0.98705055\n",
      "Iteration 205, loss = 0.98597307\n",
      "Iteration 206, loss = 0.98488279\n",
      "Iteration 207, loss = 0.98377987\n",
      "Iteration 208, loss = 0.98266446\n",
      "Iteration 209, loss = 0.98153673\n",
      "Iteration 210, loss = 0.98039690\n",
      "Iteration 211, loss = 0.97924516\n",
      "Iteration 212, loss = 0.97808175\n",
      "Iteration 213, loss = 0.97690686\n",
      "Iteration 214, loss = 0.97572071\n",
      "Iteration 215, loss = 0.97452348\n",
      "Iteration 216, loss = 0.97331533\n",
      "Iteration 217, loss = 0.97209640\n",
      "Iteration 218, loss = 0.97086677\n",
      "Iteration 219, loss = 0.96962649\n",
      "Iteration 220, loss = 0.96837555\n",
      "Iteration 221, loss = 0.96711391\n",
      "Iteration 222, loss = 0.96584148\n",
      "Iteration 223, loss = 0.96455813\n",
      "Iteration 224, loss = 0.96326370\n",
      "Iteration 225, loss = 0.96195800\n",
      "Iteration 226, loss = 0.96064083\n",
      "Iteration 227, loss = 0.95931199\n",
      "Iteration 228, loss = 0.95797127\n",
      "Iteration 229, loss = 0.95661848\n",
      "Iteration 230, loss = 0.95525346\n",
      "Iteration 231, loss = 0.95387604\n",
      "Iteration 232, loss = 0.95248610\n",
      "Iteration 233, loss = 0.95108353\n",
      "Iteration 234, loss = 0.94966825\n",
      "Iteration 235, loss = 0.94824019\n",
      "Iteration 236, loss = 0.94679929\n",
      "Iteration 237, loss = 0.94534551\n",
      "Iteration 238, loss = 0.94387881\n",
      "Iteration 239, loss = 0.94239916\n",
      "Iteration 240, loss = 0.94090652\n",
      "Iteration 241, loss = 0.93940086\n",
      "Iteration 242, loss = 0.93788213\n",
      "Iteration 243, loss = 0.93635029\n",
      "Iteration 244, loss = 0.93480530\n",
      "Iteration 245, loss = 0.93324708\n",
      "Iteration 246, loss = 0.93167560\n",
      "Iteration 247, loss = 0.93009077\n",
      "Iteration 248, loss = 0.92849252\n",
      "Iteration 249, loss = 0.92688078\n",
      "Iteration 250, loss = 0.92525545\n",
      "Iteration 251, loss = 0.92361642\n",
      "Iteration 252, loss = 0.92196357\n",
      "Iteration 253, loss = 0.92029676\n",
      "Iteration 254, loss = 0.91861580\n",
      "Iteration 255, loss = 0.91692050\n",
      "Iteration 256, loss = 0.91521060\n",
      "Iteration 257, loss = 0.91348581\n",
      "Iteration 258, loss = 0.91174580\n",
      "Iteration 259, loss = 0.90999015\n",
      "Iteration 260, loss = 0.90821839\n",
      "Iteration 261, loss = 0.90642998\n",
      "Iteration 262, loss = 0.90462428\n",
      "Iteration 263, loss = 0.90280054\n",
      "Iteration 264, loss = 0.90095793\n",
      "Iteration 265, loss = 0.89909545\n",
      "Iteration 266, loss = 0.89721198\n",
      "Iteration 267, loss = 0.89530620\n",
      "Iteration 268, loss = 0.89337663\n",
      "Iteration 269, loss = 0.89142152\n",
      "Iteration 270, loss = 0.88943888\n",
      "Iteration 271, loss = 0.88742642\n",
      "Iteration 272, loss = 0.88538150\n",
      "Iteration 273, loss = 0.88330109\n",
      "Iteration 274, loss = 0.88118174\n",
      "Iteration 275, loss = 0.87901954\n",
      "Iteration 276, loss = 0.87681010\n",
      "Iteration 277, loss = 0.87454860\n",
      "Iteration 278, loss = 0.87222982\n",
      "Iteration 279, loss = 0.86984837\n",
      "Iteration 280, loss = 0.86739897\n",
      "Iteration 281, loss = 0.86487705\n",
      "Iteration 282, loss = 0.86227954\n",
      "Iteration 283, loss = 0.85960612\n",
      "Iteration 284, loss = 0.85686073\n",
      "Iteration 285, loss = 0.85405333\n",
      "Iteration 286, loss = 0.85120145\n",
      "Iteration 287, loss = 0.84833081\n",
      "Iteration 288, loss = 0.84547403\n",
      "Iteration 289, loss = 0.84266680\n",
      "Iteration 290, loss = 0.83994171\n",
      "Iteration 291, loss = 0.83732141\n",
      "Iteration 292, loss = 0.83481364\n",
      "Iteration 293, loss = 0.83241042\n",
      "Iteration 294, loss = 0.83009155\n",
      "Iteration 295, loss = 0.82783074\n",
      "Iteration 296, loss = 0.82560172\n",
      "Iteration 297, loss = 0.82338250\n",
      "Iteration 298, loss = 0.82115717\n",
      "Iteration 299, loss = 0.81891582\n",
      "Iteration 300, loss = 0.81665348\n",
      "Iteration 1, loss = 1.22545105\n",
      "Iteration 2, loss = 1.22018894\n",
      "Iteration 3, loss = 1.21295777\n",
      "Iteration 4, loss = 1.20422052\n",
      "Iteration 5, loss = 1.19442768\n",
      "Iteration 6, loss = 1.18400061\n",
      "Iteration 7, loss = 1.17331946\n",
      "Iteration 8, loss = 1.16271525\n",
      "Iteration 9, loss = 1.15246538\n",
      "Iteration 10, loss = 1.14279201\n",
      "Iteration 11, loss = 1.13386290\n",
      "Iteration 12, loss = 1.12579405\n",
      "Iteration 13, loss = 1.11865369\n",
      "Iteration 14, loss = 1.11246743\n",
      "Iteration 15, loss = 1.10722395\n",
      "Iteration 16, loss = 1.10288124\n",
      "Iteration 17, loss = 1.09937296\n",
      "Iteration 18, loss = 1.09661475\n",
      "Iteration 19, loss = 1.09451033\n",
      "Iteration 20, loss = 1.09295718\n",
      "Iteration 21, loss = 1.09185159\n",
      "Iteration 22, loss = 1.09109305\n",
      "Iteration 23, loss = 1.09058781\n",
      "Iteration 24, loss = 1.09025156\n",
      "Iteration 25, loss = 1.09001133\n",
      "Iteration 26, loss = 1.08980649\n",
      "Iteration 27, loss = 1.08958907\n",
      "Iteration 28, loss = 1.08932344\n",
      "Iteration 29, loss = 1.08898543\n",
      "Iteration 30, loss = 1.08856116\n",
      "Iteration 31, loss = 1.08804550\n",
      "Iteration 32, loss = 1.08744049\n",
      "Iteration 33, loss = 1.08675365\n",
      "Iteration 34, loss = 1.08599641\n",
      "Iteration 35, loss = 1.08518264\n",
      "Iteration 36, loss = 1.08432732\n",
      "Iteration 37, loss = 1.08344552\n",
      "Iteration 38, loss = 1.08255148\n",
      "Iteration 39, loss = 1.08165801\n",
      "Iteration 40, loss = 1.08077603\n",
      "Iteration 41, loss = 1.07991437\n",
      "Iteration 42, loss = 1.07907968\n",
      "Iteration 43, loss = 1.07827650\n",
      "Iteration 44, loss = 1.07750739\n",
      "Iteration 45, loss = 1.07677319\n",
      "Iteration 46, loss = 1.07607333\n",
      "Iteration 47, loss = 1.07540605\n",
      "Iteration 48, loss = 1.07476877\n",
      "Iteration 49, loss = 1.07415833\n",
      "Iteration 50, loss = 1.07357125\n",
      "Iteration 51, loss = 1.07300397\n",
      "Iteration 52, loss = 1.07245299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 1.07191504\n",
      "Iteration 54, loss = 1.07138715\n",
      "Iteration 55, loss = 1.07086673\n",
      "Iteration 56, loss = 1.07035160\n",
      "Iteration 57, loss = 1.06983998\n",
      "Iteration 58, loss = 1.06933048\n",
      "Iteration 59, loss = 1.06882207\n",
      "Iteration 60, loss = 1.06831404\n",
      "Iteration 61, loss = 1.06780591\n",
      "Iteration 62, loss = 1.06729744\n",
      "Iteration 63, loss = 1.06678853\n",
      "Iteration 64, loss = 1.06627919\n",
      "Iteration 65, loss = 1.06576951\n",
      "Iteration 66, loss = 1.06525958\n",
      "Iteration 67, loss = 1.06474952\n",
      "Iteration 68, loss = 1.06423941\n",
      "Iteration 69, loss = 1.06372929\n",
      "Iteration 70, loss = 1.06321917\n",
      "Iteration 71, loss = 1.06270901\n",
      "Iteration 72, loss = 1.06219869\n",
      "Iteration 73, loss = 1.06168807\n",
      "Iteration 74, loss = 1.06117697\n",
      "Iteration 75, loss = 1.06066517\n",
      "Iteration 76, loss = 1.06015242\n",
      "Iteration 77, loss = 1.05963846\n",
      "Iteration 78, loss = 1.05912302\n",
      "Iteration 79, loss = 1.05860584\n",
      "Iteration 80, loss = 1.05808667\n",
      "Iteration 81, loss = 1.05756524\n",
      "Iteration 82, loss = 1.05704133\n",
      "Iteration 83, loss = 1.05651472\n",
      "Iteration 84, loss = 1.05598521\n",
      "Iteration 85, loss = 1.05545262\n",
      "Iteration 86, loss = 1.05491679\n",
      "Iteration 87, loss = 1.05437757\n",
      "Iteration 88, loss = 1.05383484\n",
      "Iteration 89, loss = 1.05328846\n",
      "Iteration 90, loss = 1.05273834\n",
      "Iteration 91, loss = 1.05218437\n",
      "Iteration 92, loss = 1.05162646\n",
      "Iteration 93, loss = 1.05106451\n",
      "Iteration 94, loss = 1.05049844\n",
      "Iteration 95, loss = 1.04992816\n",
      "Iteration 96, loss = 1.04935358\n",
      "Iteration 97, loss = 1.04877460\n",
      "Iteration 98, loss = 1.04819114\n",
      "Iteration 99, loss = 1.04760311\n",
      "Iteration 100, loss = 1.04701041\n",
      "Iteration 101, loss = 1.04641293\n",
      "Iteration 102, loss = 1.04581059\n",
      "Iteration 103, loss = 1.04520329\n",
      "Iteration 104, loss = 1.04459092\n",
      "Iteration 105, loss = 1.04397338\n",
      "Iteration 106, loss = 1.04335058\n",
      "Iteration 107, loss = 1.04272240\n",
      "Iteration 108, loss = 1.04208876\n",
      "Iteration 109, loss = 1.04144954\n",
      "Iteration 110, loss = 1.04080466\n",
      "Iteration 111, loss = 1.04015400\n",
      "Iteration 112, loss = 1.03949749\n",
      "Iteration 113, loss = 1.03883501\n",
      "Iteration 114, loss = 1.03816648\n",
      "Iteration 115, loss = 1.03749180\n",
      "Iteration 116, loss = 1.03681089\n",
      "Iteration 117, loss = 1.03612363\n",
      "Iteration 118, loss = 1.03542996\n",
      "Iteration 119, loss = 1.03472977\n",
      "Iteration 120, loss = 1.03402297\n",
      "Iteration 121, loss = 1.03330949\n",
      "Iteration 122, loss = 1.03258922\n",
      "Iteration 123, loss = 1.03186209\n",
      "Iteration 124, loss = 1.03112800\n",
      "Iteration 125, loss = 1.03038687\n",
      "Iteration 126, loss = 1.02963862\n",
      "Iteration 127, loss = 1.02888315\n",
      "Iteration 128, loss = 1.02812039\n",
      "Iteration 129, loss = 1.02735024\n",
      "Iteration 130, loss = 1.02657264\n",
      "Iteration 131, loss = 1.02578748\n",
      "Iteration 132, loss = 1.02499470\n",
      "Iteration 133, loss = 1.02419421\n",
      "Iteration 134, loss = 1.02338593\n",
      "Iteration 135, loss = 1.02256977\n",
      "Iteration 136, loss = 1.02174567\n",
      "Iteration 137, loss = 1.02091353\n",
      "Iteration 138, loss = 1.02007328\n",
      "Iteration 139, loss = 1.01922485\n",
      "Iteration 140, loss = 1.01836816\n",
      "Iteration 141, loss = 1.01750313\n",
      "Iteration 142, loss = 1.01662968\n",
      "Iteration 143, loss = 1.01574775\n",
      "Iteration 144, loss = 1.01485725\n",
      "Iteration 145, loss = 1.01395812\n",
      "Iteration 146, loss = 1.01305028\n",
      "Iteration 147, loss = 1.01213365\n",
      "Iteration 148, loss = 1.01120818\n",
      "Iteration 149, loss = 1.01027379\n",
      "Iteration 150, loss = 1.00933042\n",
      "Iteration 151, loss = 1.00837798\n",
      "Iteration 152, loss = 1.00741642\n",
      "Iteration 153, loss = 1.00644567\n",
      "Iteration 154, loss = 1.00546567\n",
      "Iteration 155, loss = 1.00447635\n",
      "Iteration 156, loss = 1.00347765\n",
      "Iteration 157, loss = 1.00246950\n",
      "Iteration 158, loss = 1.00145185\n",
      "Iteration 159, loss = 1.00042464\n",
      "Iteration 160, loss = 0.99938781\n",
      "Iteration 161, loss = 0.99834130\n",
      "Iteration 162, loss = 0.99728505\n",
      "Iteration 163, loss = 0.99621903\n",
      "Iteration 164, loss = 0.99514316\n",
      "Iteration 165, loss = 0.99405740\n",
      "Iteration 166, loss = 0.99296170\n",
      "Iteration 167, loss = 0.99185602\n",
      "Iteration 168, loss = 0.99074030\n",
      "Iteration 169, loss = 0.98961451\n",
      "Iteration 170, loss = 0.98847860\n",
      "Iteration 171, loss = 0.98733253\n",
      "Iteration 172, loss = 0.98617627\n",
      "Iteration 173, loss = 0.98500977\n",
      "Iteration 174, loss = 0.98383300\n",
      "Iteration 175, loss = 0.98264593\n",
      "Iteration 176, loss = 0.98144854\n",
      "Iteration 177, loss = 0.98024079\n",
      "Iteration 178, loss = 0.97902265\n",
      "Iteration 179, loss = 0.97779411\n",
      "Iteration 180, loss = 0.97655515\n",
      "Iteration 181, loss = 0.97530574\n",
      "Iteration 182, loss = 0.97404587\n",
      "Iteration 183, loss = 0.97277553\n",
      "Iteration 184, loss = 0.97149470\n",
      "Iteration 185, loss = 0.97020339\n",
      "Iteration 186, loss = 0.96890158\n",
      "Iteration 187, loss = 0.96758927\n",
      "Iteration 188, loss = 0.96626647\n",
      "Iteration 189, loss = 0.96493317\n",
      "Iteration 190, loss = 0.96358939\n",
      "Iteration 191, loss = 0.96223512\n",
      "Iteration 192, loss = 0.96087039\n",
      "Iteration 193, loss = 0.95949520\n",
      "Iteration 194, loss = 0.95810958\n",
      "Iteration 195, loss = 0.95671354\n",
      "Iteration 196, loss = 0.95530711\n",
      "Iteration 197, loss = 0.95389032\n",
      "Iteration 198, loss = 0.95246319\n",
      "Iteration 199, loss = 0.95102576\n",
      "Iteration 200, loss = 0.94957806\n",
      "Iteration 201, loss = 0.94812013\n",
      "Iteration 202, loss = 0.94665201\n",
      "Iteration 203, loss = 0.94517374\n",
      "Iteration 204, loss = 0.94368538\n",
      "Iteration 205, loss = 0.94218698\n",
      "Iteration 206, loss = 0.94067858\n",
      "Iteration 207, loss = 0.93916024\n",
      "Iteration 208, loss = 0.93763202\n",
      "Iteration 209, loss = 0.93609399\n",
      "Iteration 210, loss = 0.93454621\n",
      "Iteration 211, loss = 0.93298875\n",
      "Iteration 212, loss = 0.93142167\n",
      "Iteration 213, loss = 0.92984506\n",
      "Iteration 214, loss = 0.92825899\n",
      "Iteration 215, loss = 0.92666354\n",
      "Iteration 216, loss = 0.92505879\n",
      "Iteration 217, loss = 0.92344483\n",
      "Iteration 218, loss = 0.92182174\n",
      "Iteration 219, loss = 0.92018962\n",
      "Iteration 220, loss = 0.91854856\n",
      "Iteration 221, loss = 0.91689865\n",
      "Iteration 222, loss = 0.91524000\n",
      "Iteration 223, loss = 0.91357271\n",
      "Iteration 224, loss = 0.91189687\n",
      "Iteration 225, loss = 0.91021260\n",
      "Iteration 226, loss = 0.90852001\n",
      "Iteration 227, loss = 0.90681920\n",
      "Iteration 228, loss = 0.90511030\n",
      "Iteration 229, loss = 0.90339341\n",
      "Iteration 230, loss = 0.90166865\n",
      "Iteration 231, loss = 0.89993615\n",
      "Iteration 232, loss = 0.89819602\n",
      "Iteration 233, loss = 0.89644840\n",
      "Iteration 234, loss = 0.89469341\n",
      "Iteration 235, loss = 0.89293117\n",
      "Iteration 236, loss = 0.89116182\n",
      "Iteration 237, loss = 0.88938549\n",
      "Iteration 238, loss = 0.88760231\n",
      "Iteration 239, loss = 0.88581242\n",
      "Iteration 240, loss = 0.88401596\n",
      "Iteration 241, loss = 0.88221307\n",
      "Iteration 242, loss = 0.88040388\n",
      "Iteration 243, loss = 0.87858854\n",
      "Iteration 244, loss = 0.87676719\n",
      "Iteration 245, loss = 0.87493998\n",
      "Iteration 246, loss = 0.87310705\n",
      "Iteration 247, loss = 0.87126855\n",
      "Iteration 248, loss = 0.86942463\n",
      "Iteration 249, loss = 0.86757543\n",
      "Iteration 250, loss = 0.86572112\n",
      "Iteration 251, loss = 0.86386183\n",
      "Iteration 252, loss = 0.86199772\n",
      "Iteration 253, loss = 0.86012895\n",
      "Iteration 254, loss = 0.85825566\n",
      "Iteration 255, loss = 0.85637801\n",
      "Iteration 256, loss = 0.85449616\n",
      "Iteration 257, loss = 0.85261026\n",
      "Iteration 258, loss = 0.85072047\n",
      "Iteration 259, loss = 0.84882694\n",
      "Iteration 260, loss = 0.84692983\n",
      "Iteration 261, loss = 0.84502930\n",
      "Iteration 262, loss = 0.84312550\n",
      "Iteration 263, loss = 0.84121858\n",
      "Iteration 264, loss = 0.83930871\n",
      "Iteration 265, loss = 0.83739605\n",
      "Iteration 266, loss = 0.83548074\n",
      "Iteration 267, loss = 0.83356295\n",
      "Iteration 268, loss = 0.83164283\n",
      "Iteration 269, loss = 0.82972053\n",
      "Iteration 270, loss = 0.82779622\n",
      "Iteration 271, loss = 0.82587005\n",
      "Iteration 272, loss = 0.82394217\n",
      "Iteration 273, loss = 0.82201273\n",
      "Iteration 274, loss = 0.82008189\n",
      "Iteration 275, loss = 0.81814981\n",
      "Iteration 276, loss = 0.81621663\n",
      "Iteration 277, loss = 0.81428250\n",
      "Iteration 278, loss = 0.81234758\n",
      "Iteration 279, loss = 0.81041201\n",
      "Iteration 280, loss = 0.80847595\n",
      "Iteration 281, loss = 0.80653953\n",
      "Iteration 282, loss = 0.80460292\n",
      "Iteration 283, loss = 0.80266624\n",
      "Iteration 284, loss = 0.80072965\n",
      "Iteration 285, loss = 0.79879329\n",
      "Iteration 286, loss = 0.79685729\n",
      "Iteration 287, loss = 0.79492181\n",
      "Iteration 288, loss = 0.79298698\n",
      "Iteration 289, loss = 0.79105293\n",
      "Iteration 290, loss = 0.78911980\n",
      "Iteration 291, loss = 0.78718773\n",
      "Iteration 292, loss = 0.78525685\n",
      "Iteration 293, loss = 0.78332729\n",
      "Iteration 294, loss = 0.78139918\n",
      "Iteration 295, loss = 0.77947266\n",
      "Iteration 296, loss = 0.77754783\n",
      "Iteration 297, loss = 0.77562484\n",
      "Iteration 298, loss = 0.77370380\n",
      "Iteration 299, loss = 0.77178484\n",
      "Iteration 300, loss = 0.76986807\n",
      "Iteration 1, loss = 1.26338539\n",
      "Iteration 2, loss = 1.25641942\n",
      "Iteration 3, loss = 1.24693658\n",
      "Iteration 4, loss = 1.23561804\n",
      "Iteration 5, loss = 1.22311848\n",
      "Iteration 6, loss = 1.21003497\n",
      "Iteration 7, loss = 1.19688581\n",
      "Iteration 8, loss = 1.18409827\n",
      "Iteration 9, loss = 1.17200442\n",
      "Iteration 10, loss = 1.16084328\n",
      "Iteration 11, loss = 1.15076815\n",
      "Iteration 12, loss = 1.14185725\n",
      "Iteration 13, loss = 1.13412634\n",
      "Iteration 14, loss = 1.12754201\n",
      "Iteration 15, loss = 1.12203451\n",
      "Iteration 16, loss = 1.11750947\n",
      "Iteration 17, loss = 1.11385797\n",
      "Iteration 18, loss = 1.11096481\n",
      "Iteration 19, loss = 1.10871492\n",
      "Iteration 20, loss = 1.10699807\n",
      "Iteration 21, loss = 1.10571214\n",
      "Iteration 22, loss = 1.10476507\n",
      "Iteration 23, loss = 1.10407588\n",
      "Iteration 24, loss = 1.10357496\n",
      "Iteration 25, loss = 1.10320378\n",
      "Iteration 26, loss = 1.10291424\n",
      "Iteration 27, loss = 1.10266784\n",
      "Iteration 28, loss = 1.10243463\n",
      "Iteration 29, loss = 1.10219220\n",
      "Iteration 30, loss = 1.10192461\n",
      "Iteration 31, loss = 1.10162139\n",
      "Iteration 32, loss = 1.10127656\n",
      "Iteration 33, loss = 1.10088780\n",
      "Iteration 34, loss = 1.10045562\n",
      "Iteration 35, loss = 1.09998268\n",
      "Iteration 36, loss = 1.09947318\n",
      "Iteration 37, loss = 1.09893231\n",
      "Iteration 38, loss = 1.09836582\n",
      "Iteration 39, loss = 1.09777964\n",
      "Iteration 40, loss = 1.09717960\n",
      "Iteration 41, loss = 1.09657116\n",
      "Iteration 42, loss = 1.09595930\n",
      "Iteration 43, loss = 1.09534836\n",
      "Iteration 44, loss = 1.09474198\n",
      "Iteration 45, loss = 1.09414306\n",
      "Iteration 46, loss = 1.09355378\n",
      "Iteration 47, loss = 1.09297564\n",
      "Iteration 48, loss = 1.09240947\n",
      "Iteration 49, loss = 1.09185553\n",
      "Iteration 50, loss = 1.09131354\n",
      "Iteration 51, loss = 1.09078283\n",
      "Iteration 52, loss = 1.09026234\n",
      "Iteration 53, loss = 1.08975075\n",
      "Iteration 54, loss = 1.08924650\n",
      "Iteration 55, loss = 1.08874790\n",
      "Iteration 56, loss = 1.08825318\n",
      "Iteration 57, loss = 1.08776048\n",
      "Iteration 58, loss = 1.08726795\n",
      "Iteration 59, loss = 1.08677377\n",
      "Iteration 60, loss = 1.08627610\n",
      "Iteration 61, loss = 1.08577319\n",
      "Iteration 62, loss = 1.08526332\n",
      "Iteration 63, loss = 1.08474483\n",
      "Iteration 64, loss = 1.08421611\n",
      "Iteration 65, loss = 1.08367562\n",
      "Iteration 66, loss = 1.08312188\n",
      "Iteration 67, loss = 1.08255350\n",
      "Iteration 68, loss = 1.08196917\n",
      "Iteration 69, loss = 1.08136775\n",
      "Iteration 70, loss = 1.08074827\n",
      "Iteration 71, loss = 1.08011003\n",
      "Iteration 72, loss = 1.07945266\n",
      "Iteration 73, loss = 1.07877625\n",
      "Iteration 74, loss = 1.07808144\n",
      "Iteration 75, loss = 1.07736951\n",
      "Iteration 76, loss = 1.07664252\n",
      "Iteration 77, loss = 1.07590329\n",
      "Iteration 78, loss = 1.07515540\n",
      "Iteration 79, loss = 1.07440308\n",
      "Iteration 80, loss = 1.07365100\n",
      "Iteration 81, loss = 1.07290400\n",
      "Iteration 82, loss = 1.07216672\n",
      "Iteration 83, loss = 1.07144329\n",
      "Iteration 84, loss = 1.07073703\n",
      "Iteration 85, loss = 1.07005028\n",
      "Iteration 86, loss = 1.06938429\n",
      "Iteration 87, loss = 1.06873931\n",
      "Iteration 88, loss = 1.06811473\n",
      "Iteration 89, loss = 1.06750922\n",
      "Iteration 90, loss = 1.06692104\n",
      "Iteration 91, loss = 1.06634816\n",
      "Iteration 92, loss = 1.06578845\n",
      "Iteration 93, loss = 1.06523985\n",
      "Iteration 94, loss = 1.06470040\n",
      "Iteration 95, loss = 1.06416830\n",
      "Iteration 96, loss = 1.06364198\n",
      "Iteration 97, loss = 1.06312003\n",
      "Iteration 98, loss = 1.06260124\n",
      "Iteration 99, loss = 1.06208457\n",
      "Iteration 100, loss = 1.06156912\n",
      "Iteration 101, loss = 1.06105409\n",
      "Iteration 102, loss = 1.06053882\n",
      "Iteration 103, loss = 1.06002271\n",
      "Iteration 104, loss = 1.05950522\n",
      "Iteration 105, loss = 1.05898590\n",
      "Iteration 106, loss = 1.05846430\n",
      "Iteration 107, loss = 1.05794005\n",
      "Iteration 108, loss = 1.05741280\n",
      "Iteration 109, loss = 1.05688223\n",
      "Iteration 110, loss = 1.05634803\n",
      "Iteration 111, loss = 1.05580995\n",
      "Iteration 112, loss = 1.05526775\n",
      "Iteration 113, loss = 1.05472119\n",
      "Iteration 114, loss = 1.05417007\n",
      "Iteration 115, loss = 1.05361422\n",
      "Iteration 116, loss = 1.05305346\n",
      "Iteration 117, loss = 1.05248766\n",
      "Iteration 118, loss = 1.05191667\n",
      "Iteration 119, loss = 1.05134037\n",
      "Iteration 120, loss = 1.05075866\n",
      "Iteration 121, loss = 1.05017143\n",
      "Iteration 122, loss = 1.04957860\n",
      "Iteration 123, loss = 1.04898008\n",
      "Iteration 124, loss = 1.04837580\n",
      "Iteration 125, loss = 1.04776567\n",
      "Iteration 126, loss = 1.04714963\n",
      "Iteration 127, loss = 1.04652761\n",
      "Iteration 128, loss = 1.04589953\n",
      "Iteration 129, loss = 1.04526534\n",
      "Iteration 130, loss = 1.04462495\n",
      "Iteration 131, loss = 1.04397831\n",
      "Iteration 132, loss = 1.04332533\n",
      "Iteration 133, loss = 1.04266595\n",
      "Iteration 134, loss = 1.04200009\n",
      "Iteration 135, loss = 1.04132767\n",
      "Iteration 136, loss = 1.04064861\n",
      "Iteration 137, loss = 1.03996283\n",
      "Iteration 138, loss = 1.03927024\n",
      "Iteration 139, loss = 1.03857077\n",
      "Iteration 140, loss = 1.03786431\n",
      "Iteration 141, loss = 1.03715079\n",
      "Iteration 142, loss = 1.03643011\n",
      "Iteration 143, loss = 1.03570218\n",
      "Iteration 144, loss = 1.03496690\n",
      "Iteration 145, loss = 1.03422418\n",
      "Iteration 146, loss = 1.03347392\n",
      "Iteration 147, loss = 1.03271602\n",
      "Iteration 148, loss = 1.03195038\n",
      "Iteration 149, loss = 1.03117691\n",
      "Iteration 150, loss = 1.03039549\n",
      "Iteration 151, loss = 1.02960603\n",
      "Iteration 152, loss = 1.02880843\n",
      "Iteration 153, loss = 1.02800257\n",
      "Iteration 154, loss = 1.02718836\n",
      "Iteration 155, loss = 1.02636569\n",
      "Iteration 156, loss = 1.02553445\n",
      "Iteration 157, loss = 1.02469454\n",
      "Iteration 158, loss = 1.02384585\n",
      "Iteration 159, loss = 1.02298828\n",
      "Iteration 160, loss = 1.02212170\n",
      "Iteration 161, loss = 1.02124603\n",
      "Iteration 162, loss = 1.02036114\n",
      "Iteration 163, loss = 1.01946694\n",
      "Iteration 164, loss = 1.01856331\n",
      "Iteration 165, loss = 1.01765014\n",
      "Iteration 166, loss = 1.01672732\n",
      "Iteration 167, loss = 1.01579474\n",
      "Iteration 168, loss = 1.01485230\n",
      "Iteration 169, loss = 1.01389988\n",
      "Iteration 170, loss = 1.01293737\n",
      "Iteration 171, loss = 1.01196466\n",
      "Iteration 172, loss = 1.01098164\n",
      "Iteration 173, loss = 1.00998819\n",
      "Iteration 174, loss = 1.00898420\n",
      "Iteration 175, loss = 1.00796955\n",
      "Iteration 176, loss = 1.00694414\n",
      "Iteration 177, loss = 1.00590783\n",
      "Iteration 178, loss = 1.00486052\n",
      "Iteration 179, loss = 1.00380208\n",
      "Iteration 180, loss = 1.00273240\n",
      "Iteration 181, loss = 1.00165135\n",
      "Iteration 182, loss = 1.00055880\n",
      "Iteration 183, loss = 0.99945463\n",
      "Iteration 184, loss = 0.99833871\n",
      "Iteration 185, loss = 0.99721090\n",
      "Iteration 186, loss = 0.99607108\n",
      "Iteration 187, loss = 0.99491910\n",
      "Iteration 188, loss = 0.99375482\n",
      "Iteration 189, loss = 0.99257809\n",
      "Iteration 190, loss = 0.99138876\n",
      "Iteration 191, loss = 0.99018668\n",
      "Iteration 192, loss = 0.98897167\n",
      "Iteration 193, loss = 0.98774358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 194, loss = 0.98650221\n",
      "Iteration 195, loss = 0.98524738\n",
      "Iteration 196, loss = 0.98397890\n",
      "Iteration 197, loss = 0.98269655\n",
      "Iteration 198, loss = 0.98140011\n",
      "Iteration 199, loss = 0.98008934\n",
      "Iteration 200, loss = 0.97876399\n",
      "Iteration 201, loss = 0.97742377\n",
      "Iteration 202, loss = 0.97606840\n",
      "Iteration 203, loss = 0.97469753\n",
      "Iteration 204, loss = 0.97331083\n",
      "Iteration 205, loss = 0.97190789\n",
      "Iteration 206, loss = 0.97048829\n",
      "Iteration 207, loss = 0.96905156\n",
      "Iteration 208, loss = 0.96759715\n",
      "Iteration 209, loss = 0.96612449\n",
      "Iteration 210, loss = 0.96463291\n",
      "Iteration 211, loss = 0.96312167\n",
      "Iteration 212, loss = 0.96158993\n",
      "Iteration 213, loss = 0.96003673\n",
      "Iteration 214, loss = 0.95846101\n",
      "Iteration 215, loss = 0.95686155\n",
      "Iteration 216, loss = 0.95523694\n",
      "Iteration 217, loss = 0.95358563\n",
      "Iteration 218, loss = 0.95190580\n",
      "Iteration 219, loss = 0.95019543\n",
      "Iteration 220, loss = 0.94845220\n",
      "Iteration 221, loss = 0.94667354\n",
      "Iteration 222, loss = 0.94485655\n",
      "Iteration 223, loss = 0.94299805\n",
      "Iteration 224, loss = 0.94109461\n",
      "Iteration 225, loss = 0.93914262\n",
      "Iteration 226, loss = 0.93713844\n",
      "Iteration 227, loss = 0.93507866\n",
      "Iteration 228, loss = 0.93296042\n",
      "Iteration 229, loss = 0.93078199\n",
      "Iteration 230, loss = 0.92854342\n",
      "Iteration 231, loss = 0.92624734\n",
      "Iteration 232, loss = 0.92389978\n",
      "Iteration 233, loss = 0.92151075\n",
      "Iteration 234, loss = 0.91909439\n",
      "Iteration 235, loss = 0.91666818\n",
      "Iteration 236, loss = 0.91425130\n",
      "Iteration 237, loss = 0.91186209\n",
      "Iteration 238, loss = 0.90951536\n",
      "Iteration 239, loss = 0.90722033\n",
      "Iteration 240, loss = 0.90497972\n",
      "Iteration 241, loss = 0.90279040\n",
      "Iteration 242, loss = 0.90064496\n",
      "Iteration 243, loss = 0.89853375\n",
      "Iteration 244, loss = 0.89644665\n",
      "Iteration 245, loss = 0.89437433\n",
      "Iteration 246, loss = 0.89230886\n",
      "Iteration 247, loss = 0.89024395\n",
      "Iteration 248, loss = 0.88817482\n",
      "Iteration 249, loss = 0.88609802\n",
      "Iteration 250, loss = 0.88401127\n",
      "Iteration 251, loss = 0.88191322\n",
      "Iteration 252, loss = 0.87980338\n",
      "Iteration 253, loss = 0.87768191\n",
      "Iteration 254, loss = 0.87554951\n",
      "Iteration 255, loss = 0.87340726\n",
      "Iteration 256, loss = 0.87125643\n",
      "Iteration 257, loss = 0.86909832\n",
      "Iteration 258, loss = 0.86693416\n",
      "Iteration 259, loss = 0.86476495\n",
      "Iteration 260, loss = 0.86259146\n",
      "Iteration 261, loss = 0.86041420\n",
      "Iteration 262, loss = 0.85823342\n",
      "Iteration 263, loss = 0.85604917\n",
      "Iteration 264, loss = 0.85386139\n",
      "Iteration 265, loss = 0.85166991\n",
      "Iteration 266, loss = 0.84947457\n",
      "Iteration 267, loss = 0.84727523\n",
      "Iteration 268, loss = 0.84507183\n",
      "Iteration 269, loss = 0.84286437\n",
      "Iteration 270, loss = 0.84065295\n",
      "Iteration 271, loss = 0.83843775\n",
      "Iteration 272, loss = 0.83621899\n",
      "Iteration 273, loss = 0.83399697\n",
      "Iteration 274, loss = 0.83177196\n",
      "Iteration 275, loss = 0.82954426\n",
      "Iteration 276, loss = 0.82731415\n",
      "Iteration 277, loss = 0.82508188\n",
      "Iteration 278, loss = 0.82284767\n",
      "Iteration 279, loss = 0.82061170\n",
      "Iteration 280, loss = 0.81837415\n",
      "Iteration 281, loss = 0.81613514\n",
      "Iteration 282, loss = 0.81389483\n",
      "Iteration 283, loss = 0.81165335\n",
      "Iteration 284, loss = 0.80941085\n",
      "Iteration 285, loss = 0.80716749\n",
      "Iteration 286, loss = 0.80492346\n",
      "Iteration 287, loss = 0.80267896\n",
      "Iteration 288, loss = 0.80043423\n",
      "Iteration 289, loss = 0.79818949\n",
      "Iteration 290, loss = 0.79594500\n",
      "Iteration 291, loss = 0.79370103\n",
      "Iteration 292, loss = 0.79145783\n",
      "Iteration 293, loss = 0.78921568\n",
      "Iteration 294, loss = 0.78697481\n",
      "Iteration 295, loss = 0.78473548\n",
      "Iteration 296, loss = 0.78249792\n",
      "Iteration 297, loss = 0.78026234\n",
      "Iteration 298, loss = 0.77802897\n",
      "Iteration 299, loss = 0.77579800\n",
      "Iteration 300, loss = 0.77356962\n",
      "Iteration 1, loss = 1.16647912\n",
      "Iteration 2, loss = 1.16436968\n",
      "Iteration 3, loss = 1.16146312\n",
      "Iteration 4, loss = 1.15793333\n",
      "Iteration 5, loss = 1.15394309\n",
      "Iteration 6, loss = 1.14963779\n",
      "Iteration 7, loss = 1.14514226\n",
      "Iteration 8, loss = 1.14056005\n",
      "Iteration 9, loss = 1.13597445\n",
      "Iteration 10, loss = 1.13145051\n",
      "Iteration 11, loss = 1.12703761\n",
      "Iteration 12, loss = 1.12277203\n",
      "Iteration 13, loss = 1.11867944\n",
      "Iteration 14, loss = 1.11477701\n",
      "Iteration 15, loss = 1.11107523\n",
      "Iteration 16, loss = 1.10757934\n",
      "Iteration 17, loss = 1.10429052\n",
      "Iteration 18, loss = 1.10120684\n",
      "Iteration 19, loss = 1.09832393\n",
      "Iteration 20, loss = 1.09563562\n",
      "Iteration 21, loss = 1.09313434\n",
      "Iteration 22, loss = 1.09081149\n",
      "Iteration 23, loss = 1.08865776\n",
      "Iteration 24, loss = 1.08666334\n",
      "Iteration 25, loss = 1.08481814\n",
      "Iteration 26, loss = 1.08311193\n",
      "Iteration 27, loss = 1.08153450\n",
      "Iteration 28, loss = 1.08007578\n",
      "Iteration 29, loss = 1.07872589\n",
      "Iteration 30, loss = 1.07747528\n",
      "Iteration 31, loss = 1.07631478\n",
      "Iteration 32, loss = 1.07523561\n",
      "Iteration 33, loss = 1.07422948\n",
      "Iteration 34, loss = 1.07328858\n",
      "Iteration 35, loss = 1.07240564\n",
      "Iteration 36, loss = 1.07157389\n",
      "Iteration 37, loss = 1.07078710\n",
      "Iteration 38, loss = 1.07003957\n",
      "Iteration 39, loss = 1.06932612\n",
      "Iteration 40, loss = 1.06864206\n",
      "Iteration 41, loss = 1.06798320\n",
      "Iteration 42, loss = 1.06734581\n",
      "Iteration 43, loss = 1.06672659\n",
      "Iteration 44, loss = 1.06612265\n",
      "Iteration 45, loss = 1.06553147\n",
      "Iteration 46, loss = 1.06495090\n",
      "Iteration 47, loss = 1.06437909\n",
      "Iteration 48, loss = 1.06381447\n",
      "Iteration 49, loss = 1.06325575\n",
      "Iteration 50, loss = 1.06270186\n",
      "Iteration 51, loss = 1.06215191\n",
      "Iteration 52, loss = 1.06160522\n",
      "Iteration 53, loss = 1.06106123\n",
      "Iteration 54, loss = 1.06051953\n",
      "Iteration 55, loss = 1.05997980\n",
      "Iteration 56, loss = 1.05944182\n",
      "Iteration 57, loss = 1.05890545\n",
      "Iteration 58, loss = 1.05837058\n",
      "Iteration 59, loss = 1.05783718\n",
      "Iteration 60, loss = 1.05730523\n",
      "Iteration 61, loss = 1.05677475\n",
      "Iteration 62, loss = 1.05624577\n",
      "Iteration 63, loss = 1.05571832\n",
      "Iteration 64, loss = 1.05519246\n",
      "Iteration 65, loss = 1.05466823\n",
      "Iteration 66, loss = 1.05414567\n",
      "Iteration 67, loss = 1.05362483\n",
      "Iteration 68, loss = 1.05310573\n",
      "Iteration 69, loss = 1.05258840\n",
      "Iteration 70, loss = 1.05207287\n",
      "Iteration 71, loss = 1.05155913\n",
      "Iteration 72, loss = 1.05104719\n",
      "Iteration 73, loss = 1.05053704\n",
      "Iteration 74, loss = 1.05002866\n",
      "Iteration 75, loss = 1.04952204\n",
      "Iteration 76, loss = 1.04901714\n",
      "Iteration 77, loss = 1.04851393\n",
      "Iteration 78, loss = 1.04801237\n",
      "Iteration 79, loss = 1.04751242\n",
      "Iteration 80, loss = 1.04701403\n",
      "Iteration 81, loss = 1.04651717\n",
      "Iteration 82, loss = 1.04602177\n",
      "Iteration 83, loss = 1.04552779\n",
      "Iteration 84, loss = 1.04503519\n",
      "Iteration 85, loss = 1.04454390\n",
      "Iteration 86, loss = 1.04405389\n",
      "Iteration 87, loss = 1.04356509\n",
      "Iteration 88, loss = 1.04307747\n",
      "Iteration 89, loss = 1.04259098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90, loss = 1.04210557\n",
      "Iteration 91, loss = 1.04162120\n",
      "Iteration 92, loss = 1.04113782\n",
      "Iteration 93, loss = 1.04065539\n",
      "Iteration 94, loss = 1.04017388\n",
      "Iteration 95, loss = 1.03969324\n",
      "Iteration 96, loss = 1.03921343\n",
      "Iteration 97, loss = 1.03873443\n",
      "Iteration 98, loss = 1.03825620\n",
      "Iteration 99, loss = 1.03777869\n",
      "Iteration 100, loss = 1.03730188\n",
      "Iteration 101, loss = 1.03682574\n",
      "Iteration 102, loss = 1.03635024\n",
      "Iteration 103, loss = 1.03587533\n",
      "Iteration 104, loss = 1.03540100\n",
      "Iteration 105, loss = 1.03492721\n",
      "Iteration 106, loss = 1.03445393\n",
      "Iteration 107, loss = 1.03398113\n",
      "Iteration 108, loss = 1.03350878\n",
      "Iteration 109, loss = 1.03303686\n",
      "Iteration 110, loss = 1.03256532\n",
      "Iteration 111, loss = 1.03209414\n",
      "Iteration 112, loss = 1.03162329\n",
      "Iteration 113, loss = 1.03115273\n",
      "Iteration 114, loss = 1.03068245\n",
      "Iteration 115, loss = 1.03021239\n",
      "Iteration 116, loss = 1.02974253\n",
      "Iteration 117, loss = 1.02927284\n",
      "Iteration 118, loss = 1.02880328\n",
      "Iteration 119, loss = 1.02833382\n",
      "Iteration 120, loss = 1.02786443\n",
      "Iteration 121, loss = 1.02739505\n",
      "Iteration 122, loss = 1.02692567\n",
      "Iteration 123, loss = 1.02645624\n",
      "Iteration 124, loss = 1.02598672\n",
      "Iteration 125, loss = 1.02551708\n",
      "Iteration 126, loss = 1.02504728\n",
      "Iteration 127, loss = 1.02457727\n",
      "Iteration 128, loss = 1.02410701\n",
      "Iteration 129, loss = 1.02363647\n",
      "Iteration 130, loss = 1.02316559\n",
      "Iteration 131, loss = 1.02269434\n",
      "Iteration 132, loss = 1.02222268\n",
      "Iteration 133, loss = 1.02175055\n",
      "Iteration 134, loss = 1.02127791\n",
      "Iteration 135, loss = 1.02080471\n",
      "Iteration 136, loss = 1.02033092\n",
      "Iteration 137, loss = 1.01985647\n",
      "Iteration 138, loss = 1.01938132\n",
      "Iteration 139, loss = 1.01890543\n",
      "Iteration 140, loss = 1.01842874\n",
      "Iteration 141, loss = 1.01795120\n",
      "Iteration 142, loss = 1.01747276\n",
      "Iteration 143, loss = 1.01699337\n",
      "Iteration 144, loss = 1.01651297\n",
      "Iteration 145, loss = 1.01603152\n",
      "Iteration 146, loss = 1.01554896\n",
      "Iteration 147, loss = 1.01506524\n",
      "Iteration 148, loss = 1.01458029\n",
      "Iteration 149, loss = 1.01409407\n",
      "Iteration 150, loss = 1.01360653\n",
      "Iteration 151, loss = 1.01311760\n",
      "Iteration 152, loss = 1.01262723\n",
      "Iteration 153, loss = 1.01213536\n",
      "Iteration 154, loss = 1.01164195\n",
      "Iteration 155, loss = 1.01114692\n",
      "Iteration 156, loss = 1.01065023\n",
      "Iteration 157, loss = 1.01015181\n",
      "Iteration 158, loss = 1.00965162\n",
      "Iteration 159, loss = 1.00914959\n",
      "Iteration 160, loss = 1.00864567\n",
      "Iteration 161, loss = 1.00813981\n",
      "Iteration 162, loss = 1.00763194\n",
      "Iteration 163, loss = 1.00712200\n",
      "Iteration 164, loss = 1.00660996\n",
      "Iteration 165, loss = 1.00609574\n",
      "Iteration 166, loss = 1.00557930\n",
      "Iteration 167, loss = 1.00506057\n",
      "Iteration 168, loss = 1.00453951\n",
      "Iteration 169, loss = 1.00401607\n",
      "Iteration 170, loss = 1.00349018\n",
      "Iteration 171, loss = 1.00296180\n",
      "Iteration 172, loss = 1.00243088\n",
      "Iteration 173, loss = 1.00189736\n",
      "Iteration 174, loss = 1.00136119\n",
      "Iteration 175, loss = 1.00082232\n",
      "Iteration 176, loss = 1.00028070\n",
      "Iteration 177, loss = 0.99973629\n",
      "Iteration 178, loss = 0.99918902\n",
      "Iteration 179, loss = 0.99863886\n",
      "Iteration 180, loss = 0.99808575\n",
      "Iteration 181, loss = 0.99752965\n",
      "Iteration 182, loss = 0.99697049\n",
      "Iteration 183, loss = 0.99640824\n",
      "Iteration 184, loss = 0.99584285\n",
      "Iteration 185, loss = 0.99527425\n",
      "Iteration 186, loss = 0.99470241\n",
      "Iteration 187, loss = 0.99412726\n",
      "Iteration 188, loss = 0.99354876\n",
      "Iteration 189, loss = 0.99296684\n",
      "Iteration 190, loss = 0.99238147\n",
      "Iteration 191, loss = 0.99179257\n",
      "Iteration 192, loss = 0.99120009\n",
      "Iteration 193, loss = 0.99060396\n",
      "Iteration 194, loss = 0.99000414\n",
      "Iteration 195, loss = 0.98940055\n",
      "Iteration 196, loss = 0.98879313\n",
      "Iteration 197, loss = 0.98818181\n",
      "Iteration 198, loss = 0.98756653\n",
      "Iteration 199, loss = 0.98694721\n",
      "Iteration 200, loss = 0.98632379\n",
      "Iteration 201, loss = 0.98569619\n",
      "Iteration 202, loss = 0.98506435\n",
      "Iteration 203, loss = 0.98442818\n",
      "Iteration 204, loss = 0.98378761\n",
      "Iteration 205, loss = 0.98314257\n",
      "Iteration 206, loss = 0.98249298\n",
      "Iteration 207, loss = 0.98183876\n",
      "Iteration 208, loss = 0.98117984\n",
      "Iteration 209, loss = 0.98051614\n",
      "Iteration 210, loss = 0.97984759\n",
      "Iteration 211, loss = 0.97917410\n",
      "Iteration 212, loss = 0.97849561\n",
      "Iteration 213, loss = 0.97781203\n",
      "Iteration 214, loss = 0.97712330\n",
      "Iteration 215, loss = 0.97642934\n",
      "Iteration 216, loss = 0.97573007\n",
      "Iteration 217, loss = 0.97502543\n",
      "Iteration 218, loss = 0.97431533\n",
      "Iteration 219, loss = 0.97359972\n",
      "Iteration 220, loss = 0.97287852\n",
      "Iteration 221, loss = 0.97215165\n",
      "Iteration 222, loss = 0.97141905\n",
      "Iteration 223, loss = 0.97068066\n",
      "Iteration 224, loss = 0.96993640\n",
      "Iteration 225, loss = 0.96918620\n",
      "Iteration 226, loss = 0.96843000\n",
      "Iteration 227, loss = 0.96766772\n",
      "Iteration 228, loss = 0.96689931\n",
      "Iteration 229, loss = 0.96612470\n",
      "Iteration 230, loss = 0.96534381\n",
      "Iteration 231, loss = 0.96455660\n",
      "Iteration 232, loss = 0.96376298\n",
      "Iteration 233, loss = 0.96296289\n",
      "Iteration 234, loss = 0.96215628\n",
      "Iteration 235, loss = 0.96134308\n",
      "Iteration 236, loss = 0.96052322\n",
      "Iteration 237, loss = 0.95969664\n",
      "Iteration 238, loss = 0.95886328\n",
      "Iteration 239, loss = 0.95802309\n",
      "Iteration 240, loss = 0.95717599\n",
      "Iteration 241, loss = 0.95632194\n",
      "Iteration 242, loss = 0.95546087\n",
      "Iteration 243, loss = 0.95459272\n",
      "Iteration 244, loss = 0.95371745\n",
      "Iteration 245, loss = 0.95283499\n",
      "Iteration 246, loss = 0.95194529\n",
      "Iteration 247, loss = 0.95104829\n",
      "Iteration 248, loss = 0.95014395\n",
      "Iteration 249, loss = 0.94923222\n",
      "Iteration 250, loss = 0.94831304\n",
      "Iteration 251, loss = 0.94738637\n",
      "Iteration 252, loss = 0.94645216\n",
      "Iteration 253, loss = 0.94551037\n",
      "Iteration 254, loss = 0.94456094\n",
      "Iteration 255, loss = 0.94360385\n",
      "Iteration 256, loss = 0.94263904\n",
      "Iteration 257, loss = 0.94166648\n",
      "Iteration 258, loss = 0.94068613\n",
      "Iteration 259, loss = 0.93969795\n",
      "Iteration 260, loss = 0.93870191\n",
      "Iteration 261, loss = 0.93769798\n",
      "Iteration 262, loss = 0.93668612\n",
      "Iteration 263, loss = 0.93566630\n",
      "Iteration 264, loss = 0.93463851\n",
      "Iteration 265, loss = 0.93360270\n",
      "Iteration 266, loss = 0.93255886\n",
      "Iteration 267, loss = 0.93150697\n",
      "Iteration 268, loss = 0.93044700\n",
      "Iteration 269, loss = 0.92937894\n",
      "Iteration 270, loss = 0.92830278\n",
      "Iteration 271, loss = 0.92721849\n",
      "Iteration 272, loss = 0.92612607\n",
      "Iteration 273, loss = 0.92502551\n",
      "Iteration 274, loss = 0.92391681\n",
      "Iteration 275, loss = 0.92279995\n",
      "Iteration 276, loss = 0.92167494\n",
      "Iteration 277, loss = 0.92054177\n",
      "Iteration 278, loss = 0.91940046\n",
      "Iteration 279, loss = 0.91825099\n",
      "Iteration 280, loss = 0.91709339\n",
      "Iteration 281, loss = 0.91592766\n",
      "Iteration 282, loss = 0.91475381\n",
      "Iteration 283, loss = 0.91357186\n",
      "Iteration 284, loss = 0.91238183\n",
      "Iteration 285, loss = 0.91118373\n",
      "Iteration 286, loss = 0.90997759\n",
      "Iteration 287, loss = 0.90876343\n",
      "Iteration 288, loss = 0.90754128\n",
      "Iteration 289, loss = 0.90631118\n",
      "Iteration 290, loss = 0.90507314\n",
      "Iteration 291, loss = 0.90382722\n",
      "Iteration 292, loss = 0.90257344\n",
      "Iteration 293, loss = 0.90131185\n",
      "Iteration 294, loss = 0.90004249\n",
      "Iteration 295, loss = 0.89876540\n",
      "Iteration 296, loss = 0.89748065\n",
      "Iteration 297, loss = 0.89618827\n",
      "Iteration 298, loss = 0.89488832\n",
      "Iteration 299, loss = 0.89358086\n",
      "Iteration 300, loss = 0.89226594\n",
      "Iteration 1, loss = 1.11401426\n",
      "Iteration 2, loss = 1.11344065\n",
      "Iteration 3, loss = 1.11264921\n",
      "Iteration 4, loss = 1.11168738\n",
      "Iteration 5, loss = 1.11060072\n",
      "Iteration 6, loss = 1.10943127\n",
      "Iteration 7, loss = 1.10821637\n",
      "Iteration 8, loss = 1.10698803\n",
      "Iteration 9, loss = 1.10577264\n",
      "Iteration 10, loss = 1.10459098\n",
      "Iteration 11, loss = 1.10345855\n",
      "Iteration 12, loss = 1.10238599\n",
      "Iteration 13, loss = 1.10137971\n",
      "Iteration 14, loss = 1.10044247\n",
      "Iteration 15, loss = 1.09957406\n",
      "Iteration 16, loss = 1.09877191\n",
      "Iteration 17, loss = 1.09803170\n",
      "Iteration 18, loss = 1.09734788\n",
      "Iteration 19, loss = 1.09671415\n",
      "Iteration 20, loss = 1.09612387\n",
      "Iteration 21, loss = 1.09557034\n",
      "Iteration 22, loss = 1.09504711\n",
      "Iteration 23, loss = 1.09454815\n",
      "Iteration 24, loss = 1.09406800\n",
      "Iteration 25, loss = 1.09360182\n",
      "Iteration 26, loss = 1.09314549\n",
      "Iteration 27, loss = 1.09269556\n",
      "Iteration 28, loss = 1.09224925\n",
      "Iteration 29, loss = 1.09180443\n",
      "Iteration 30, loss = 1.09135951\n",
      "Iteration 31, loss = 1.09091342\n",
      "Iteration 32, loss = 1.09046552\n",
      "Iteration 33, loss = 1.09001549\n",
      "Iteration 34, loss = 1.08956332\n",
      "Iteration 35, loss = 1.08910921\n",
      "Iteration 36, loss = 1.08865348\n",
      "Iteration 37, loss = 1.08819656\n",
      "Iteration 38, loss = 1.08773895\n",
      "Iteration 39, loss = 1.08728113\n",
      "Iteration 40, loss = 1.08682357\n",
      "Iteration 41, loss = 1.08636670\n",
      "Iteration 42, loss = 1.08591089\n",
      "Iteration 43, loss = 1.08545644\n",
      "Iteration 44, loss = 1.08500359\n",
      "Iteration 45, loss = 1.08455249\n",
      "Iteration 46, loss = 1.08410324\n",
      "Iteration 47, loss = 1.08365586\n",
      "Iteration 48, loss = 1.08321033\n",
      "Iteration 49, loss = 1.08276656\n",
      "Iteration 50, loss = 1.08232444\n",
      "Iteration 51, loss = 1.08188382\n",
      "Iteration 52, loss = 1.08144452\n",
      "Iteration 53, loss = 1.08100638\n",
      "Iteration 54, loss = 1.08056919\n",
      "Iteration 55, loss = 1.08013277\n",
      "Iteration 56, loss = 1.07969692\n",
      "Iteration 57, loss = 1.07926147\n",
      "Iteration 58, loss = 1.07882623\n",
      "Iteration 59, loss = 1.07839106\n",
      "Iteration 60, loss = 1.07795581\n",
      "Iteration 61, loss = 1.07752035\n",
      "Iteration 62, loss = 1.07708456\n",
      "Iteration 63, loss = 1.07664834\n",
      "Iteration 64, loss = 1.07621161\n",
      "Iteration 65, loss = 1.07577427\n",
      "Iteration 66, loss = 1.07533627\n",
      "Iteration 67, loss = 1.07489756\n",
      "Iteration 68, loss = 1.07445806\n",
      "Iteration 69, loss = 1.07401776\n",
      "Iteration 70, loss = 1.07357660\n",
      "Iteration 71, loss = 1.07313454\n",
      "Iteration 72, loss = 1.07269157\n",
      "Iteration 73, loss = 1.07224764\n",
      "Iteration 74, loss = 1.07180273\n",
      "Iteration 75, loss = 1.07135681\n",
      "Iteration 76, loss = 1.07090985\n",
      "Iteration 77, loss = 1.07046182\n",
      "Iteration 78, loss = 1.07001269\n",
      "Iteration 79, loss = 1.06956244\n",
      "Iteration 80, loss = 1.06911102\n",
      "Iteration 81, loss = 1.06865842\n",
      "Iteration 82, loss = 1.06820459\n",
      "Iteration 83, loss = 1.06774950\n",
      "Iteration 84, loss = 1.06729313\n",
      "Iteration 85, loss = 1.06683543\n",
      "Iteration 86, loss = 1.06637638\n",
      "Iteration 87, loss = 1.06591593\n",
      "Iteration 88, loss = 1.06545406\n",
      "Iteration 89, loss = 1.06499073\n",
      "Iteration 90, loss = 1.06452590\n",
      "Iteration 91, loss = 1.06405955\n",
      "Iteration 92, loss = 1.06359163\n",
      "Iteration 93, loss = 1.06312212\n",
      "Iteration 94, loss = 1.06265098\n",
      "Iteration 95, loss = 1.06217817\n",
      "Iteration 96, loss = 1.06170368\n",
      "Iteration 97, loss = 1.06122745\n",
      "Iteration 98, loss = 1.06074946\n",
      "Iteration 99, loss = 1.06026968\n",
      "Iteration 100, loss = 1.05978807\n",
      "Iteration 101, loss = 1.05930460\n",
      "Iteration 102, loss = 1.05881925\n",
      "Iteration 103, loss = 1.05833197\n",
      "Iteration 104, loss = 1.05784274\n",
      "Iteration 105, loss = 1.05735152\n",
      "Iteration 106, loss = 1.05685828\n",
      "Iteration 107, loss = 1.05636299\n",
      "Iteration 108, loss = 1.05586561\n",
      "Iteration 109, loss = 1.05536612\n",
      "Iteration 110, loss = 1.05486447\n",
      "Iteration 111, loss = 1.05436065\n",
      "Iteration 112, loss = 1.05385460\n",
      "Iteration 113, loss = 1.05334631\n",
      "Iteration 114, loss = 1.05283573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 115, loss = 1.05232283\n",
      "Iteration 116, loss = 1.05180759\n",
      "Iteration 117, loss = 1.05128995\n",
      "Iteration 118, loss = 1.05076990\n",
      "Iteration 119, loss = 1.05024738\n",
      "Iteration 120, loss = 1.04972238\n",
      "Iteration 121, loss = 1.04919485\n",
      "Iteration 122, loss = 1.04866476\n",
      "Iteration 123, loss = 1.04813207\n",
      "Iteration 124, loss = 1.04759675\n",
      "Iteration 125, loss = 1.04705875\n",
      "Iteration 126, loss = 1.04651805\n",
      "Iteration 127, loss = 1.04597460\n",
      "Iteration 128, loss = 1.04542837\n",
      "Iteration 129, loss = 1.04487932\n",
      "Iteration 130, loss = 1.04432741\n",
      "Iteration 131, loss = 1.04377260\n",
      "Iteration 132, loss = 1.04321486\n",
      "Iteration 133, loss = 1.04265414\n",
      "Iteration 134, loss = 1.04209040\n",
      "Iteration 135, loss = 1.04152361\n",
      "Iteration 136, loss = 1.04095373\n",
      "Iteration 137, loss = 1.04038070\n",
      "Iteration 138, loss = 1.03980450\n",
      "Iteration 139, loss = 1.03922507\n",
      "Iteration 140, loss = 1.03864238\n",
      "Iteration 141, loss = 1.03805639\n",
      "Iteration 142, loss = 1.03746704\n",
      "Iteration 143, loss = 1.03687430\n",
      "Iteration 144, loss = 1.03627812\n",
      "Iteration 145, loss = 1.03567845\n",
      "Iteration 146, loss = 1.03507525\n",
      "Iteration 147, loss = 1.03446848\n",
      "Iteration 148, loss = 1.03385808\n",
      "Iteration 149, loss = 1.03324401\n",
      "Iteration 150, loss = 1.03262622\n",
      "Iteration 151, loss = 1.03200466\n",
      "Iteration 152, loss = 1.03137928\n",
      "Iteration 153, loss = 1.03075004\n",
      "Iteration 154, loss = 1.03011687\n",
      "Iteration 155, loss = 1.02947974\n",
      "Iteration 156, loss = 1.02883858\n",
      "Iteration 157, loss = 1.02819335\n",
      "Iteration 158, loss = 1.02754400\n",
      "Iteration 159, loss = 1.02689046\n",
      "Iteration 160, loss = 1.02623269\n",
      "Iteration 161, loss = 1.02557063\n",
      "Iteration 162, loss = 1.02490422\n",
      "Iteration 163, loss = 1.02423341\n",
      "Iteration 164, loss = 1.02355815\n",
      "Iteration 165, loss = 1.02287837\n",
      "Iteration 166, loss = 1.02219401\n",
      "Iteration 167, loss = 1.02150503\n",
      "Iteration 168, loss = 1.02081136\n",
      "Iteration 169, loss = 1.02011293\n",
      "Iteration 170, loss = 1.01940970\n",
      "Iteration 171, loss = 1.01870160\n",
      "Iteration 172, loss = 1.01798857\n",
      "Iteration 173, loss = 1.01727055\n",
      "Iteration 174, loss = 1.01654748\n",
      "Iteration 175, loss = 1.01581930\n",
      "Iteration 176, loss = 1.01508593\n",
      "Iteration 177, loss = 1.01434734\n",
      "Iteration 178, loss = 1.01360344\n",
      "Iteration 179, loss = 1.01285417\n",
      "Iteration 180, loss = 1.01209948\n",
      "Iteration 181, loss = 1.01133930\n",
      "Iteration 182, loss = 1.01057357\n",
      "Iteration 183, loss = 1.00980223\n",
      "Iteration 184, loss = 1.00902521\n",
      "Iteration 185, loss = 1.00824244\n",
      "Iteration 186, loss = 1.00745388\n",
      "Iteration 187, loss = 1.00665945\n",
      "Iteration 188, loss = 1.00585909\n",
      "Iteration 189, loss = 1.00505275\n",
      "Iteration 190, loss = 1.00424035\n",
      "Iteration 191, loss = 1.00342185\n",
      "Iteration 192, loss = 1.00259719\n",
      "Iteration 193, loss = 1.00176629\n",
      "Iteration 194, loss = 1.00092912\n",
      "Iteration 195, loss = 1.00008560\n",
      "Iteration 196, loss = 0.99923568\n",
      "Iteration 197, loss = 0.99837932\n",
      "Iteration 198, loss = 0.99751644\n",
      "Iteration 199, loss = 0.99664702\n",
      "Iteration 200, loss = 0.99577098\n",
      "Iteration 201, loss = 0.99488828\n",
      "Iteration 202, loss = 0.99399888\n",
      "Iteration 203, loss = 0.99310272\n",
      "Iteration 204, loss = 0.99219976\n",
      "Iteration 205, loss = 0.99128995\n",
      "Iteration 206, loss = 0.99037326\n",
      "Iteration 207, loss = 0.98944963\n",
      "Iteration 208, loss = 0.98851904\n",
      "Iteration 209, loss = 0.98758143\n",
      "Iteration 210, loss = 0.98663678\n",
      "Iteration 211, loss = 0.98568504\n",
      "Iteration 212, loss = 0.98472618\n",
      "Iteration 213, loss = 0.98376016\n",
      "Iteration 214, loss = 0.98278696\n",
      "Iteration 215, loss = 0.98180652\n",
      "Iteration 216, loss = 0.98081883\n",
      "Iteration 217, loss = 0.97982385\n",
      "Iteration 218, loss = 0.97882155\n",
      "Iteration 219, loss = 0.97781189\n",
      "Iteration 220, loss = 0.97679484\n",
      "Iteration 221, loss = 0.97577037\n",
      "Iteration 222, loss = 0.97473844\n",
      "Iteration 223, loss = 0.97369903\n",
      "Iteration 224, loss = 0.97265209\n",
      "Iteration 225, loss = 0.97159760\n",
      "Iteration 226, loss = 0.97053551\n",
      "Iteration 227, loss = 0.96946579\n",
      "Iteration 228, loss = 0.96838840\n",
      "Iteration 229, loss = 0.96730331\n",
      "Iteration 230, loss = 0.96621047\n",
      "Iteration 231, loss = 0.96510986\n",
      "Iteration 232, loss = 0.96400142\n",
      "Iteration 233, loss = 0.96288512\n",
      "Iteration 234, loss = 0.96176093\n",
      "Iteration 235, loss = 0.96062879\n",
      "Iteration 236, loss = 0.95948868\n",
      "Iteration 237, loss = 0.95834054\n",
      "Iteration 238, loss = 0.95718436\n",
      "Iteration 239, loss = 0.95602008\n",
      "Iteration 240, loss = 0.95484767\n",
      "Iteration 241, loss = 0.95366710\n",
      "Iteration 242, loss = 0.95247834\n",
      "Iteration 243, loss = 0.95128134\n",
      "Iteration 244, loss = 0.95007609\n",
      "Iteration 245, loss = 0.94886256\n",
      "Iteration 246, loss = 0.94764071\n",
      "Iteration 247, loss = 0.94641052\n",
      "Iteration 248, loss = 0.94517198\n",
      "Iteration 249, loss = 0.94392505\n",
      "Iteration 250, loss = 0.94266973\n",
      "Iteration 251, loss = 0.94140599\n",
      "Iteration 252, loss = 0.94013383\n",
      "Iteration 253, loss = 0.93885322\n",
      "Iteration 254, loss = 0.93756416\n",
      "Iteration 255, loss = 0.93626665\n",
      "Iteration 256, loss = 0.93496067\n",
      "Iteration 257, loss = 0.93364621\n",
      "Iteration 258, loss = 0.93232329\n",
      "Iteration 259, loss = 0.93099188\n",
      "Iteration 260, loss = 0.92965201\n",
      "Iteration 261, loss = 0.92830366\n",
      "Iteration 262, loss = 0.92694683\n",
      "Iteration 263, loss = 0.92558155\n",
      "Iteration 264, loss = 0.92420780\n",
      "Iteration 265, loss = 0.92282560\n",
      "Iteration 266, loss = 0.92143495\n",
      "Iteration 267, loss = 0.92003588\n",
      "Iteration 268, loss = 0.91862838\n",
      "Iteration 269, loss = 0.91721247\n",
      "Iteration 270, loss = 0.91578816\n",
      "Iteration 271, loss = 0.91435548\n",
      "Iteration 272, loss = 0.91291444\n",
      "Iteration 273, loss = 0.91146505\n",
      "Iteration 274, loss = 0.91000733\n",
      "Iteration 275, loss = 0.90854131\n",
      "Iteration 276, loss = 0.90706700\n",
      "Iteration 277, loss = 0.90558443\n",
      "Iteration 278, loss = 0.90409363\n",
      "Iteration 279, loss = 0.90259461\n",
      "Iteration 280, loss = 0.90108741\n",
      "Iteration 281, loss = 0.89957205\n",
      "Iteration 282, loss = 0.89804855\n",
      "Iteration 283, loss = 0.89651696\n",
      "Iteration 284, loss = 0.89497729\n",
      "Iteration 285, loss = 0.89342958\n",
      "Iteration 286, loss = 0.89187387\n",
      "Iteration 287, loss = 0.89031018\n",
      "Iteration 288, loss = 0.88873855\n",
      "Iteration 289, loss = 0.88715901\n",
      "Iteration 290, loss = 0.88557160\n",
      "Iteration 291, loss = 0.88397635\n",
      "Iteration 292, loss = 0.88237330\n",
      "Iteration 293, loss = 0.88076250\n",
      "Iteration 294, loss = 0.87914397\n",
      "Iteration 295, loss = 0.87751776\n",
      "Iteration 296, loss = 0.87588390\n",
      "Iteration 297, loss = 0.87424243\n",
      "Iteration 298, loss = 0.87259339\n",
      "Iteration 299, loss = 0.87093683\n",
      "Iteration 300, loss = 0.86927278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/.local/lib/python3.5/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for train, test in kf.split(teaching_values, teaching_classes):\n",
    "    data_train, target_train = teaching_values[train], teaching_classes[train]\n",
    "    data_test, target_test = teaching_values[test], teaching_classes[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        49\n",
      "           2       1.00      1.00      1.00        50\n",
      "           3       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           1.00       151\n",
      "   macro avg       1.00      1.00      1.00       151\n",
      "weighted avg       1.00      1.00      1.00       151\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[49  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 52]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.94      0.97        49\n",
      "           2       0.94      0.94      0.94        50\n",
      "           3       0.95      1.00      0.97        52\n",
      "\n",
      "    accuracy                           0.96       151\n",
      "   macro avg       0.96      0.96      0.96       151\n",
      "weighted avg       0.96      0.96      0.96       151\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[46  3  0]\n",
      " [ 0 47  3]\n",
      " [ 0  0 52]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91        49\n",
      "           2       0.78      0.80      0.79        50\n",
      "           3       0.87      0.87      0.87        52\n",
      "\n",
      "    accuracy                           0.85       151\n",
      "   macro avg       0.86      0.85      0.85       151\n",
      "weighted avg       0.86      0.85      0.85       151\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[44  4  1]\n",
      " [ 4 40  6]\n",
      " [ 0  7 45]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.84      0.85        49\n",
      "           2       0.59      0.58      0.59        50\n",
      "           3       0.71      0.75      0.73        52\n",
      "\n",
      "    accuracy                           0.72       151\n",
      "   macro avg       0.72      0.72      0.72       151\n",
      "weighted avg       0.72      0.72      0.72       151\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[41  7  1]\n",
      " [ 6 29 15]\n",
      " [ 0 13 39]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        49\n",
      "           2       1.00      1.00      1.00        50\n",
      "           3       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           1.00       151\n",
      "   macro avg       1.00      1.00      1.00       151\n",
      "weighted avg       1.00      1.00      1.00       151\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[49  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 52]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(teaching_classes, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusão: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(teaching_classes, predicted_classes[classificador]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}