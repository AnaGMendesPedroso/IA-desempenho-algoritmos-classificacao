{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Transfusion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "0                   2                 50                  12500   \n",
       "1                   0                 13                   3250   \n",
       "2                   1                 16                   4000   \n",
       "3                   2                 20                   5000   \n",
       "4                   1                 24                   6000   \n",
       "..                ...                ...                    ...   \n",
       "743                23                  2                    500   \n",
       "744                21                  2                    500   \n",
       "745                23                  3                    750   \n",
       "746                39                  1                    250   \n",
       "747                72                  1                    250   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "0               98                                           1  \n",
       "1               28                                           1  \n",
       "2               35                                           1  \n",
       "3               45                                           1  \n",
       "4               77                                           0  \n",
       "..             ...                                         ...  \n",
       "743             38                                           0  \n",
       "744             52                                           0  \n",
       "745             62                                           0  \n",
       "746             39                                           0  \n",
       "747             72                                           0  \n",
       "\n",
       "[748 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_transfusion = pd.read_table('/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/datasets/transfusion.data', sep=',')\n",
    "print(\"Dataset Transfusion\")\n",
    "dataset_transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Transfusion Normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.927899</td>\n",
       "      <td>7.623346</td>\n",
       "      <td>7.623346</td>\n",
       "      <td>2.615633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.175118</td>\n",
       "      <td>1.282738</td>\n",
       "      <td>1.282738</td>\n",
       "      <td>-0.257881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.051508</td>\n",
       "      <td>1.796842</td>\n",
       "      <td>1.796842</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.927899</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>0.439973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.051508</td>\n",
       "      <td>3.167784</td>\n",
       "      <td>3.167784</td>\n",
       "      <td>1.753579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>1.667904</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>0.152621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>1.420685</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>0.727324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>1.667904</td>\n",
       "      <td>-0.430940</td>\n",
       "      <td>-0.430940</td>\n",
       "      <td>1.137826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>3.645659</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>7.724778</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>1.548328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "0           -0.927899           7.623346               7.623346   \n",
       "1           -1.175118           1.282738               1.282738   \n",
       "2           -1.051508           1.796842               1.796842   \n",
       "3           -0.927899           2.482313               2.482313   \n",
       "4           -1.051508           3.167784               3.167784   \n",
       "..                ...                ...                    ...   \n",
       "743          1.667904          -0.602307              -0.602307   \n",
       "744          1.420685          -0.602307              -0.602307   \n",
       "745          1.667904          -0.430940              -0.430940   \n",
       "746          3.645659          -0.773675              -0.773675   \n",
       "747          7.724778          -0.773675              -0.773675   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "0         2.615633                                           1  \n",
       "1        -0.257881                                           1  \n",
       "2         0.029471                                           1  \n",
       "3         0.439973                                           1  \n",
       "4         1.753579                                           0  \n",
       "..             ...                                         ...  \n",
       "743       0.152621                                           0  \n",
       "744       0.727324                                           0  \n",
       "745       1.137826                                           0  \n",
       "746       0.193671                                           0  \n",
       "747       1.548328                                           0  \n",
       "\n",
       "[748 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfusion_normalised = dataset_transfusion.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "transfusion_normalised = normalizador.fit_transform(dataset_transfusion)\n",
    "dataset_transfusion['Recency (months)'] = transfusion_normalised[:,0]\n",
    "dataset_transfusion['Frequency (times)'] = transfusion_normalised[:,1]\n",
    "dataset_transfusion['Monetary (c.c. blood)'] = transfusion_normalised[:,2]\n",
    "dataset_transfusion['Time (months)'] = transfusion_normalised[:,3]\n",
    "\n",
    "print(\"Dataset Transfusion Normalized\")\n",
    "dataset_transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfusion features\n",
      "[[-0.92789873  7.62334626  7.62334626  2.61563344]\n",
      " [-1.17511806  1.28273826  1.28273826 -0.2578809 ]\n",
      " [-1.0515084   1.79684161  1.79684161  0.02947053]\n",
      " ...\n",
      " [ 1.66790417 -0.43093957 -0.43093957  1.13782607]\n",
      " [ 3.64565877 -0.77367514 -0.77367514  0.19367135]\n",
      " [ 7.72477762 -0.77367514 -0.77367514  1.54832812]]\n"
     ]
    }
   ],
   "source": [
    "transfusion_values = dataset_transfusion.iloc[:,0:4].values\n",
    "print(\"Transfusion features\")\n",
    "print(transfusion_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfusion classes\n",
      "[1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0\n",
      " 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "Transfusion classes shape\n",
      "(748,)\n"
     ]
    }
   ],
   "source": [
    "transfusion_classes = dataset_transfusion.iloc[:,4].values\n",
    "print(\"Transfusion classes\")\n",
    "print(transfusion_classes)\n",
    "print(\"Transfusion classes shape\")\n",
    "print(transfusion_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(transfusion_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11494724\n",
      "Iteration 2, loss = 1.05182682\n",
      "Iteration 3, loss = 0.96505088\n",
      "Iteration 4, loss = 0.87233082\n",
      "Iteration 5, loss = 0.78541509\n",
      "Iteration 6, loss = 0.71449737\n",
      "Iteration 7, loss = 0.65689075\n",
      "Iteration 8, loss = 0.61587165\n",
      "Iteration 9, loss = 0.58907013\n",
      "Iteration 10, loss = 0.57107551\n",
      "Iteration 11, loss = 0.55929043\n",
      "Iteration 12, loss = 0.55252568\n",
      "Iteration 13, loss = 0.54851275\n",
      "Iteration 14, loss = 0.54614897\n",
      "Iteration 15, loss = 0.54500526\n",
      "Iteration 16, loss = 0.54451002\n",
      "Iteration 17, loss = 0.54418833\n",
      "Iteration 18, loss = 0.54411706\n",
      "Iteration 19, loss = 0.54398938\n",
      "Iteration 20, loss = 0.54382429\n",
      "Iteration 21, loss = 0.54368665\n",
      "Iteration 22, loss = 0.54351037\n",
      "Iteration 23, loss = 0.54324712\n",
      "Iteration 24, loss = 0.54297412\n",
      "Iteration 25, loss = 0.54273251\n",
      "Iteration 26, loss = 0.54245930\n",
      "Iteration 27, loss = 0.54217570\n",
      "Iteration 28, loss = 0.54194407\n",
      "Iteration 29, loss = 0.54170121\n",
      "Iteration 30, loss = 0.54139048\n",
      "Iteration 31, loss = 0.54110481\n",
      "Iteration 32, loss = 0.54085688\n",
      "Iteration 33, loss = 0.54062069\n",
      "Iteration 34, loss = 0.54034411\n",
      "Iteration 35, loss = 0.54008216\n",
      "Iteration 36, loss = 0.53982141\n",
      "Iteration 37, loss = 0.53957650\n",
      "Iteration 38, loss = 0.53932552\n",
      "Iteration 39, loss = 0.53903687\n",
      "Iteration 40, loss = 0.53880029\n",
      "Iteration 41, loss = 0.53850891\n",
      "Iteration 42, loss = 0.53824766\n",
      "Iteration 43, loss = 0.53797473\n",
      "Iteration 44, loss = 0.53771394\n",
      "Iteration 45, loss = 0.53742838\n",
      "Iteration 46, loss = 0.53714330\n",
      "Iteration 47, loss = 0.53685900\n",
      "Iteration 48, loss = 0.53658305\n",
      "Iteration 49, loss = 0.53632712\n",
      "Iteration 50, loss = 0.53599395\n",
      "Iteration 51, loss = 0.53575546\n",
      "Iteration 52, loss = 0.53539417\n",
      "Iteration 53, loss = 0.53512514\n",
      "Iteration 54, loss = 0.53480355\n",
      "Iteration 55, loss = 0.53449214\n",
      "Iteration 56, loss = 0.53421620\n",
      "Iteration 57, loss = 0.53386356\n",
      "Iteration 58, loss = 0.53355077\n",
      "Iteration 59, loss = 0.53322151\n",
      "Iteration 60, loss = 0.53289356\n",
      "Iteration 61, loss = 0.53257445\n",
      "Iteration 62, loss = 0.53225832\n",
      "Iteration 63, loss = 0.53189734\n",
      "Iteration 64, loss = 0.53160488\n",
      "Iteration 65, loss = 0.53121303\n",
      "Iteration 66, loss = 0.53084977\n",
      "Iteration 67, loss = 0.53050139\n",
      "Iteration 68, loss = 0.53015574\n",
      "Iteration 69, loss = 0.52979882\n",
      "Iteration 70, loss = 0.52944655\n",
      "Iteration 71, loss = 0.52906200\n",
      "Iteration 72, loss = 0.52870566\n",
      "Iteration 73, loss = 0.52830265\n",
      "Iteration 74, loss = 0.52792303\n",
      "Iteration 75, loss = 0.52754792\n",
      "Iteration 76, loss = 0.52716204\n",
      "Iteration 77, loss = 0.52680758\n",
      "Iteration 78, loss = 0.52637601\n",
      "Iteration 79, loss = 0.52599065\n",
      "Iteration 80, loss = 0.52556492\n",
      "Iteration 81, loss = 0.52517024\n",
      "Iteration 82, loss = 0.52475203\n",
      "Iteration 83, loss = 0.52433266\n",
      "Iteration 84, loss = 0.52393736\n",
      "Iteration 85, loss = 0.52348690\n",
      "Iteration 86, loss = 0.52305249\n",
      "Iteration 87, loss = 0.52264202\n",
      "Iteration 88, loss = 0.52221174\n",
      "Iteration 89, loss = 0.52175616\n",
      "Iteration 90, loss = 0.52131625\n",
      "Iteration 91, loss = 0.52086965\n",
      "Iteration 92, loss = 0.52042326\n",
      "Iteration 93, loss = 0.51999531\n",
      "Iteration 94, loss = 0.51951698\n",
      "Iteration 95, loss = 0.51907027\n",
      "Iteration 96, loss = 0.51860767\n",
      "Iteration 97, loss = 0.51817624\n",
      "Iteration 98, loss = 0.51765916\n",
      "Iteration 99, loss = 0.51720889\n",
      "Iteration 100, loss = 0.51670416\n",
      "Iteration 101, loss = 0.51625300\n",
      "Iteration 102, loss = 0.51574549\n",
      "Iteration 103, loss = 0.51526298\n",
      "Iteration 104, loss = 0.51477269\n",
      "Iteration 105, loss = 0.51429585\n",
      "Iteration 106, loss = 0.51379167\n",
      "Iteration 107, loss = 0.51327815\n",
      "Iteration 108, loss = 0.51278563\n",
      "Iteration 109, loss = 0.51230614\n",
      "Iteration 110, loss = 0.51181084\n",
      "Iteration 111, loss = 0.51128622\n",
      "Iteration 112, loss = 0.51079117\n",
      "Iteration 113, loss = 0.51026376\n",
      "Iteration 114, loss = 0.50974092\n",
      "Iteration 115, loss = 0.50925149\n",
      "Iteration 116, loss = 0.50870345\n",
      "Iteration 117, loss = 0.50821803\n",
      "Iteration 118, loss = 0.50767697\n",
      "Iteration 119, loss = 0.50714808\n",
      "Iteration 120, loss = 0.50662904\n",
      "Iteration 121, loss = 0.50614028\n",
      "Iteration 122, loss = 0.50560334\n",
      "Iteration 123, loss = 0.50505697\n",
      "Iteration 124, loss = 0.50452743\n",
      "Iteration 125, loss = 0.50402471\n",
      "Iteration 126, loss = 0.50347164\n",
      "Iteration 127, loss = 0.50295466\n",
      "Iteration 128, loss = 0.50249210\n",
      "Iteration 129, loss = 0.50189830\n",
      "Iteration 130, loss = 0.50132361\n",
      "Iteration 131, loss = 0.50082834\n",
      "Iteration 132, loss = 0.50030231\n",
      "Iteration 133, loss = 0.49974492\n",
      "Iteration 134, loss = 0.49922402\n",
      "Iteration 135, loss = 0.49869800\n",
      "Iteration 136, loss = 0.49816611\n",
      "Iteration 137, loss = 0.49761734\n",
      "Iteration 138, loss = 0.49712592\n",
      "Iteration 139, loss = 0.49658978\n",
      "Iteration 140, loss = 0.49605012\n",
      "Iteration 141, loss = 0.49550706\n",
      "Iteration 142, loss = 0.49499818\n",
      "Iteration 143, loss = 0.49445753\n",
      "Iteration 144, loss = 0.49398483\n",
      "Iteration 145, loss = 0.49346985\n",
      "Iteration 146, loss = 0.49293289\n",
      "Iteration 147, loss = 0.49235679\n",
      "Iteration 148, loss = 0.49190295\n",
      "Iteration 149, loss = 0.49135921\n",
      "Iteration 150, loss = 0.49082476\n",
      "Iteration 151, loss = 0.49033151\n",
      "Iteration 152, loss = 0.48985970\n",
      "Iteration 153, loss = 0.48929872\n",
      "Iteration 154, loss = 0.48877375\n",
      "Iteration 155, loss = 0.48831856\n",
      "Iteration 156, loss = 0.48779206\n",
      "Iteration 157, loss = 0.48726715\n",
      "Iteration 158, loss = 0.48684538\n",
      "Iteration 159, loss = 0.48630600\n",
      "Iteration 160, loss = 0.48585069\n",
      "Iteration 161, loss = 0.48532732\n",
      "Iteration 162, loss = 0.48483031\n",
      "Iteration 163, loss = 0.48435015\n",
      "Iteration 164, loss = 0.48389156\n",
      "Iteration 165, loss = 0.48340334\n",
      "Iteration 166, loss = 0.48292816\n",
      "Iteration 167, loss = 0.48246660\n",
      "Iteration 168, loss = 0.48201956\n",
      "Iteration 169, loss = 0.48153037\n",
      "Iteration 170, loss = 0.48107699\n",
      "Iteration 171, loss = 0.48062037\n",
      "Iteration 172, loss = 0.48018161\n",
      "Iteration 173, loss = 0.47974049\n",
      "Iteration 174, loss = 0.47927606\n",
      "Iteration 175, loss = 0.47884533\n",
      "Iteration 176, loss = 0.47843851\n",
      "Iteration 177, loss = 0.47797019\n",
      "Iteration 178, loss = 0.47754854\n",
      "Iteration 179, loss = 0.47710787\n",
      "Iteration 180, loss = 0.47671612\n",
      "Iteration 181, loss = 0.47627242\n",
      "Iteration 182, loss = 0.47589299\n",
      "Iteration 183, loss = 0.47543086\n",
      "Iteration 184, loss = 0.47503374\n",
      "Iteration 185, loss = 0.47466751\n",
      "Iteration 186, loss = 0.47428374\n",
      "Iteration 187, loss = 0.47384468\n",
      "Iteration 188, loss = 0.47347614\n",
      "Iteration 189, loss = 0.47317744\n",
      "Iteration 190, loss = 0.47270983\n",
      "Iteration 191, loss = 0.47232902\n",
      "Iteration 192, loss = 0.47194913\n",
      "Iteration 193, loss = 0.47159428\n",
      "Iteration 194, loss = 0.47122159\n",
      "Iteration 195, loss = 0.47084052\n",
      "Iteration 196, loss = 0.47050902\n",
      "Iteration 197, loss = 0.47015130\n",
      "Iteration 198, loss = 0.46977580\n",
      "Iteration 199, loss = 0.46946945\n",
      "Iteration 200, loss = 0.46910027\n",
      "Iteration 201, loss = 0.46884068\n",
      "Iteration 202, loss = 0.46845215\n",
      "Iteration 203, loss = 0.46816105\n",
      "Iteration 204, loss = 0.46782935\n",
      "Iteration 205, loss = 0.46751072\n",
      "Iteration 206, loss = 0.46719074\n",
      "Iteration 207, loss = 0.46687937\n",
      "Iteration 208, loss = 0.46657030\n",
      "Iteration 209, loss = 0.46628055\n",
      "Iteration 210, loss = 0.46596447\n",
      "Iteration 211, loss = 0.46570491\n",
      "Iteration 212, loss = 0.46539068\n",
      "Iteration 213, loss = 0.46510820\n",
      "Iteration 214, loss = 0.46480505\n",
      "Iteration 215, loss = 0.46455532\n",
      "Iteration 216, loss = 0.46429251\n",
      "Iteration 217, loss = 0.46400930\n",
      "Iteration 218, loss = 0.46373879\n",
      "Iteration 219, loss = 0.46347218\n",
      "Iteration 220, loss = 0.46322160\n",
      "Iteration 221, loss = 0.46293455\n",
      "Iteration 222, loss = 0.46272026\n",
      "Iteration 223, loss = 0.46245053\n",
      "Iteration 224, loss = 0.46222765\n",
      "Iteration 225, loss = 0.46198661\n",
      "Iteration 226, loss = 0.46176302\n",
      "Iteration 227, loss = 0.46150465\n",
      "Iteration 228, loss = 0.46130725\n",
      "Iteration 229, loss = 0.46107103\n",
      "Iteration 230, loss = 0.46083949\n",
      "Iteration 231, loss = 0.46061597\n",
      "Iteration 232, loss = 0.46042101\n",
      "Iteration 233, loss = 0.46020420\n",
      "Iteration 234, loss = 0.46000256\n",
      "Iteration 235, loss = 0.45979976\n",
      "Iteration 236, loss = 0.45957387\n",
      "Iteration 237, loss = 0.45938671\n",
      "Iteration 238, loss = 0.45919766\n",
      "Iteration 239, loss = 0.45898542\n",
      "Iteration 240, loss = 0.45882311\n",
      "Iteration 241, loss = 0.45865358\n",
      "Iteration 242, loss = 0.45843427\n",
      "Iteration 243, loss = 0.45825103\n",
      "Iteration 244, loss = 0.45809957\n",
      "Iteration 245, loss = 0.45793063\n",
      "Iteration 246, loss = 0.45778279\n",
      "Iteration 247, loss = 0.45757183\n",
      "Iteration 248, loss = 0.45743137\n",
      "Iteration 249, loss = 0.45723837\n",
      "Iteration 250, loss = 0.45708641\n",
      "Iteration 251, loss = 0.45694048\n",
      "Iteration 252, loss = 0.45676522\n",
      "Iteration 253, loss = 0.45662919\n",
      "Iteration 254, loss = 0.45649054\n",
      "Iteration 255, loss = 0.45632885\n",
      "Iteration 256, loss = 0.45619186\n",
      "Iteration 257, loss = 0.45603198\n",
      "Iteration 258, loss = 0.45591812\n",
      "Iteration 259, loss = 0.45576857\n",
      "Iteration 260, loss = 0.45563355\n",
      "Iteration 261, loss = 0.45551693\n",
      "Iteration 262, loss = 0.45538084\n",
      "Iteration 263, loss = 0.45527293\n",
      "Iteration 264, loss = 0.45512887\n",
      "Iteration 265, loss = 0.45498163\n",
      "Iteration 266, loss = 0.45486611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 267, loss = 0.45476066\n",
      "Iteration 268, loss = 0.45462744\n",
      "Iteration 269, loss = 0.45450331\n",
      "Iteration 270, loss = 0.45440492\n",
      "Iteration 271, loss = 0.45428424\n",
      "Iteration 272, loss = 0.45416522\n",
      "Iteration 273, loss = 0.45408934\n",
      "Iteration 274, loss = 0.45397796\n",
      "Iteration 275, loss = 0.45387168\n",
      "Iteration 276, loss = 0.45375868\n",
      "Iteration 277, loss = 0.45364397\n",
      "Iteration 278, loss = 0.45361535\n",
      "Iteration 279, loss = 0.45347975\n",
      "Iteration 280, loss = 0.45341127\n",
      "Iteration 281, loss = 0.45326696\n",
      "Iteration 282, loss = 0.45319700\n",
      "Iteration 283, loss = 0.45309696\n",
      "Iteration 284, loss = 0.45302160\n",
      "Iteration 285, loss = 0.45291924\n",
      "Iteration 286, loss = 0.45284476\n",
      "Iteration 287, loss = 0.45274869\n",
      "Iteration 288, loss = 0.45267120\n",
      "Iteration 289, loss = 0.45260135\n",
      "Iteration 290, loss = 0.45250857\n",
      "Iteration 291, loss = 0.45246714\n",
      "Iteration 292, loss = 0.45236306\n",
      "Iteration 293, loss = 0.45232728\n",
      "Iteration 294, loss = 0.45229833\n",
      "Iteration 295, loss = 0.45213171\n",
      "Iteration 296, loss = 0.45208381\n",
      "Iteration 297, loss = 0.45201604\n",
      "Iteration 298, loss = 0.45194749\n",
      "Iteration 299, loss = 0.45188393\n",
      "Iteration 300, loss = 0.45181374\n",
      "Iteration 1, loss = 0.63844216\n",
      "Iteration 2, loss = 0.62861297\n",
      "Iteration 3, loss = 0.61539878\n",
      "Iteration 4, loss = 0.60164316\n",
      "Iteration 5, loss = 0.58778369\n",
      "Iteration 6, loss = 0.57568578\n",
      "Iteration 7, loss = 0.56576766\n",
      "Iteration 8, loss = 0.55838639\n",
      "Iteration 9, loss = 0.55218865\n",
      "Iteration 10, loss = 0.54804738\n",
      "Iteration 11, loss = 0.54532291\n",
      "Iteration 12, loss = 0.54271552\n",
      "Iteration 13, loss = 0.54081089\n",
      "Iteration 14, loss = 0.53979579\n",
      "Iteration 15, loss = 0.53885536\n",
      "Iteration 16, loss = 0.53824879\n",
      "Iteration 17, loss = 0.53786959\n",
      "Iteration 18, loss = 0.53731681\n",
      "Iteration 19, loss = 0.53688057\n",
      "Iteration 20, loss = 0.53648878\n",
      "Iteration 21, loss = 0.53612786\n",
      "Iteration 22, loss = 0.53578340\n",
      "Iteration 23, loss = 0.53544631\n",
      "Iteration 24, loss = 0.53513340\n",
      "Iteration 25, loss = 0.53477571\n",
      "Iteration 26, loss = 0.53445148\n",
      "Iteration 27, loss = 0.53411345\n",
      "Iteration 28, loss = 0.53377685\n",
      "Iteration 29, loss = 0.53344207\n",
      "Iteration 30, loss = 0.53308478\n",
      "Iteration 31, loss = 0.53276761\n",
      "Iteration 32, loss = 0.53242620\n",
      "Iteration 33, loss = 0.53209094\n",
      "Iteration 34, loss = 0.53177736\n",
      "Iteration 35, loss = 0.53145580\n",
      "Iteration 36, loss = 0.53113850\n",
      "Iteration 37, loss = 0.53082525\n",
      "Iteration 38, loss = 0.53047605\n",
      "Iteration 39, loss = 0.53018492\n",
      "Iteration 40, loss = 0.52985678\n",
      "Iteration 41, loss = 0.52952043\n",
      "Iteration 42, loss = 0.52919968\n",
      "Iteration 43, loss = 0.52887985\n",
      "Iteration 44, loss = 0.52857236\n",
      "Iteration 45, loss = 0.52825251\n",
      "Iteration 46, loss = 0.52793125\n",
      "Iteration 47, loss = 0.52761736\n",
      "Iteration 48, loss = 0.52730297\n",
      "Iteration 49, loss = 0.52699198\n",
      "Iteration 50, loss = 0.52665316\n",
      "Iteration 51, loss = 0.52636710\n",
      "Iteration 52, loss = 0.52607345\n",
      "Iteration 53, loss = 0.52573031\n",
      "Iteration 54, loss = 0.52540639\n",
      "Iteration 55, loss = 0.52513044\n",
      "Iteration 56, loss = 0.52480371\n",
      "Iteration 57, loss = 0.52456970\n",
      "Iteration 58, loss = 0.52416834\n",
      "Iteration 59, loss = 0.52387851\n",
      "Iteration 60, loss = 0.52356296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 0.52325948\n",
      "Iteration 62, loss = 0.52293021\n",
      "Iteration 63, loss = 0.52262399\n",
      "Iteration 64, loss = 0.52234948\n",
      "Iteration 65, loss = 0.52205274\n",
      "Iteration 66, loss = 0.52172223\n",
      "Iteration 67, loss = 0.52141646\n",
      "Iteration 68, loss = 0.52109529\n",
      "Iteration 69, loss = 0.52081921\n",
      "Iteration 70, loss = 0.52048607\n",
      "Iteration 71, loss = 0.52016440\n",
      "Iteration 72, loss = 0.51989542\n",
      "Iteration 73, loss = 0.51957454\n",
      "Iteration 74, loss = 0.51926827\n",
      "Iteration 75, loss = 0.51897874\n",
      "Iteration 76, loss = 0.51866452\n",
      "Iteration 77, loss = 0.51836607\n",
      "Iteration 78, loss = 0.51805396\n",
      "Iteration 79, loss = 0.51781989\n",
      "Iteration 80, loss = 0.51748424\n",
      "Iteration 81, loss = 0.51717321\n",
      "Iteration 82, loss = 0.51686193\n",
      "Iteration 83, loss = 0.51656603\n",
      "Iteration 84, loss = 0.51628458\n",
      "Iteration 85, loss = 0.51595385\n",
      "Iteration 86, loss = 0.51565793\n",
      "Iteration 87, loss = 0.51538681\n",
      "Iteration 88, loss = 0.51509225\n",
      "Iteration 89, loss = 0.51480697\n",
      "Iteration 90, loss = 0.51449185\n",
      "Iteration 91, loss = 0.51420176\n",
      "Iteration 92, loss = 0.51391502\n",
      "Iteration 93, loss = 0.51361674\n",
      "Iteration 94, loss = 0.51331227\n",
      "Iteration 95, loss = 0.51303122\n",
      "Iteration 96, loss = 0.51273785\n",
      "Iteration 97, loss = 0.51246183\n",
      "Iteration 98, loss = 0.51216002\n",
      "Iteration 99, loss = 0.51187163\n",
      "Iteration 100, loss = 0.51159945\n",
      "Iteration 101, loss = 0.51133125\n",
      "Iteration 102, loss = 0.51104642\n",
      "Iteration 103, loss = 0.51073348\n",
      "Iteration 104, loss = 0.51047152\n",
      "Iteration 105, loss = 0.51019098\n",
      "Iteration 106, loss = 0.50990443\n",
      "Iteration 107, loss = 0.50963779\n",
      "Iteration 108, loss = 0.50935048\n",
      "Iteration 109, loss = 0.50907465\n",
      "Iteration 110, loss = 0.50884165\n",
      "Iteration 111, loss = 0.50852623\n",
      "Iteration 112, loss = 0.50826605\n",
      "Iteration 113, loss = 0.50798578\n",
      "Iteration 114, loss = 0.50770550\n",
      "Iteration 115, loss = 0.50744273\n",
      "Iteration 116, loss = 0.50717270\n",
      "Iteration 117, loss = 0.50690977\n",
      "Iteration 118, loss = 0.50664533\n",
      "Iteration 119, loss = 0.50637184\n",
      "Iteration 120, loss = 0.50612207\n",
      "Iteration 121, loss = 0.50585192\n",
      "Iteration 122, loss = 0.50560235\n",
      "Iteration 123, loss = 0.50535964\n",
      "Iteration 124, loss = 0.50508564\n",
      "Iteration 125, loss = 0.50482875\n",
      "Iteration 126, loss = 0.50461341\n",
      "Iteration 127, loss = 0.50431124\n",
      "Iteration 128, loss = 0.50408512\n",
      "Iteration 129, loss = 0.50382319\n",
      "Iteration 130, loss = 0.50358682\n",
      "Iteration 131, loss = 0.50334813\n",
      "Iteration 132, loss = 0.50310004\n",
      "Iteration 133, loss = 0.50285740\n",
      "Iteration 134, loss = 0.50262704\n",
      "Iteration 135, loss = 0.50237634\n",
      "Iteration 136, loss = 0.50212738\n",
      "Iteration 137, loss = 0.50188967\n",
      "Iteration 138, loss = 0.50168671\n",
      "Iteration 139, loss = 0.50143361\n",
      "Iteration 140, loss = 0.50121093\n",
      "Iteration 141, loss = 0.50098633\n",
      "Iteration 142, loss = 0.50074531\n",
      "Iteration 143, loss = 0.50051906\n",
      "Iteration 144, loss = 0.50028636\n",
      "Iteration 145, loss = 0.50008178\n",
      "Iteration 146, loss = 0.49986828\n",
      "Iteration 147, loss = 0.49962553\n",
      "Iteration 148, loss = 0.49941169\n",
      "Iteration 149, loss = 0.49918145\n",
      "Iteration 150, loss = 0.49898921\n",
      "Iteration 151, loss = 0.49877227\n",
      "Iteration 152, loss = 0.49859996\n",
      "Iteration 153, loss = 0.49833159\n",
      "Iteration 154, loss = 0.49813387\n",
      "Iteration 155, loss = 0.49793159\n",
      "Iteration 156, loss = 0.49773006\n",
      "Iteration 157, loss = 0.49754467\n",
      "Iteration 158, loss = 0.49734011\n",
      "Iteration 159, loss = 0.49712091\n",
      "Iteration 160, loss = 0.49693399\n",
      "Iteration 161, loss = 0.49674813\n",
      "Iteration 162, loss = 0.49654106\n",
      "Iteration 163, loss = 0.49636085\n",
      "Iteration 164, loss = 0.49616307\n",
      "Iteration 165, loss = 0.49597151\n",
      "Iteration 166, loss = 0.49578728\n",
      "Iteration 167, loss = 0.49561195\n",
      "Iteration 168, loss = 0.49541544\n",
      "Iteration 169, loss = 0.49525543\n",
      "Iteration 170, loss = 0.49508779\n",
      "Iteration 171, loss = 0.49492074\n",
      "Iteration 172, loss = 0.49471276\n",
      "Iteration 173, loss = 0.49455802\n",
      "Iteration 174, loss = 0.49438031\n",
      "Iteration 175, loss = 0.49418835\n",
      "Iteration 176, loss = 0.49404193\n",
      "Iteration 177, loss = 0.49385722\n",
      "Iteration 178, loss = 0.49369160\n",
      "Iteration 179, loss = 0.49351668\n",
      "Iteration 180, loss = 0.49336406\n",
      "Iteration 181, loss = 0.49320764\n",
      "Iteration 182, loss = 0.49306047\n",
      "Iteration 183, loss = 0.49289909\n",
      "Iteration 184, loss = 0.49273477\n",
      "Iteration 185, loss = 0.49258789\n",
      "Iteration 186, loss = 0.49243475\n",
      "Iteration 187, loss = 0.49228437\n",
      "Iteration 188, loss = 0.49213196\n",
      "Iteration 189, loss = 0.49198723\n",
      "Iteration 190, loss = 0.49183635\n",
      "Iteration 191, loss = 0.49170124\n",
      "Iteration 192, loss = 0.49156535\n",
      "Iteration 193, loss = 0.49142523\n",
      "Iteration 194, loss = 0.49128169\n",
      "Iteration 195, loss = 0.49114346\n",
      "Iteration 196, loss = 0.49102317\n",
      "Iteration 197, loss = 0.49087688\n",
      "Iteration 198, loss = 0.49073973\n",
      "Iteration 199, loss = 0.49061175\n",
      "Iteration 200, loss = 0.49047894\n",
      "Iteration 201, loss = 0.49034339\n",
      "Iteration 202, loss = 0.49022216\n",
      "Iteration 203, loss = 0.49010596\n",
      "Iteration 204, loss = 0.49000583\n",
      "Iteration 205, loss = 0.48985497\n",
      "Iteration 206, loss = 0.48973549\n",
      "Iteration 207, loss = 0.48960386\n",
      "Iteration 208, loss = 0.48948427\n",
      "Iteration 209, loss = 0.48938400\n",
      "Iteration 210, loss = 0.48929157\n",
      "Iteration 211, loss = 0.48916839\n",
      "Iteration 212, loss = 0.48902238\n",
      "Iteration 213, loss = 0.48894104\n",
      "Iteration 214, loss = 0.48882425\n",
      "Iteration 215, loss = 0.48869646\n",
      "Iteration 216, loss = 0.48860816\n",
      "Iteration 217, loss = 0.48851554\n",
      "Iteration 218, loss = 0.48839709\n",
      "Iteration 219, loss = 0.48830203\n",
      "Iteration 220, loss = 0.48817323\n",
      "Iteration 221, loss = 0.48807869\n",
      "Iteration 222, loss = 0.48802605\n",
      "Iteration 223, loss = 0.48789320\n",
      "Iteration 224, loss = 0.48780924\n",
      "Iteration 225, loss = 0.48766874\n",
      "Iteration 226, loss = 0.48758337\n",
      "Iteration 227, loss = 0.48752637\n",
      "Iteration 228, loss = 0.48741070\n",
      "Iteration 229, loss = 0.48731463\n",
      "Iteration 230, loss = 0.48723556\n",
      "Iteration 231, loss = 0.48712712\n",
      "Iteration 232, loss = 0.48705644\n",
      "Iteration 233, loss = 0.48697462\n",
      "Iteration 234, loss = 0.48687141\n",
      "Iteration 235, loss = 0.48679342\n",
      "Iteration 236, loss = 0.48670429\n",
      "Iteration 237, loss = 0.48661327\n",
      "Iteration 238, loss = 0.48655484\n",
      "Iteration 239, loss = 0.48646257\n",
      "Iteration 240, loss = 0.48639047\n",
      "Iteration 241, loss = 0.48630236\n",
      "Iteration 242, loss = 0.48622322\n",
      "Iteration 243, loss = 0.48620083\n",
      "Iteration 244, loss = 0.48610840\n",
      "Iteration 245, loss = 0.48601931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94792532\n",
      "Iteration 2, loss = 0.90661107\n",
      "Iteration 3, loss = 0.85032822\n",
      "Iteration 4, loss = 0.78840833\n",
      "Iteration 5, loss = 0.73098420\n",
      "Iteration 6, loss = 0.68265684\n",
      "Iteration 7, loss = 0.64330433\n",
      "Iteration 8, loss = 0.61276664\n",
      "Iteration 9, loss = 0.59181756\n",
      "Iteration 10, loss = 0.57568206\n",
      "Iteration 11, loss = 0.56622071\n",
      "Iteration 12, loss = 0.55967797\n",
      "Iteration 13, loss = 0.55542208\n",
      "Iteration 14, loss = 0.55238827\n",
      "Iteration 15, loss = 0.55115254\n",
      "Iteration 16, loss = 0.55038979\n",
      "Iteration 17, loss = 0.55036290\n",
      "Iteration 18, loss = 0.54981337\n",
      "Iteration 19, loss = 0.54953655\n",
      "Iteration 20, loss = 0.54930301\n",
      "Iteration 21, loss = 0.54916997\n",
      "Iteration 22, loss = 0.54900521\n",
      "Iteration 23, loss = 0.54884836\n",
      "Iteration 24, loss = 0.54865570\n",
      "Iteration 25, loss = 0.54843693\n",
      "Iteration 26, loss = 0.54824307\n",
      "Iteration 27, loss = 0.54802312\n",
      "Iteration 28, loss = 0.54782362\n",
      "Iteration 29, loss = 0.54770697\n",
      "Iteration 30, loss = 0.54740922\n",
      "Iteration 31, loss = 0.54718483\n",
      "Iteration 32, loss = 0.54696886\n",
      "Iteration 33, loss = 0.54679131\n",
      "Iteration 34, loss = 0.54659546\n",
      "Iteration 35, loss = 0.54644712\n",
      "Iteration 36, loss = 0.54616513\n",
      "Iteration 37, loss = 0.54597543\n",
      "Iteration 38, loss = 0.54575931\n",
      "Iteration 39, loss = 0.54557837\n",
      "Iteration 40, loss = 0.54536140\n",
      "Iteration 41, loss = 0.54513185\n",
      "Iteration 42, loss = 0.54492108\n",
      "Iteration 43, loss = 0.54473363\n",
      "Iteration 44, loss = 0.54453344\n",
      "Iteration 45, loss = 0.54431377\n",
      "Iteration 46, loss = 0.54411260\n",
      "Iteration 47, loss = 0.54387172\n",
      "Iteration 48, loss = 0.54367517\n",
      "Iteration 49, loss = 0.54347470\n",
      "Iteration 50, loss = 0.54324975\n",
      "Iteration 51, loss = 0.54305907\n",
      "Iteration 52, loss = 0.54280583\n",
      "Iteration 53, loss = 0.54259146\n",
      "Iteration 54, loss = 0.54237676\n",
      "Iteration 55, loss = 0.54214284\n",
      "Iteration 56, loss = 0.54191231\n",
      "Iteration 57, loss = 0.54168488\n",
      "Iteration 58, loss = 0.54147931\n",
      "Iteration 59, loss = 0.54127206\n",
      "Iteration 60, loss = 0.54099948\n",
      "Iteration 61, loss = 0.54076716\n",
      "Iteration 62, loss = 0.54053234\n",
      "Iteration 63, loss = 0.54028053\n",
      "Iteration 64, loss = 0.54005705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 0.53981310\n",
      "Iteration 66, loss = 0.53956935\n",
      "Iteration 67, loss = 0.53935543\n",
      "Iteration 68, loss = 0.53908445\n",
      "Iteration 69, loss = 0.53883930\n",
      "Iteration 70, loss = 0.53860338\n",
      "Iteration 71, loss = 0.53833889\n",
      "Iteration 72, loss = 0.53809931\n",
      "Iteration 73, loss = 0.53784563\n",
      "Iteration 74, loss = 0.53757129\n",
      "Iteration 75, loss = 0.53732618\n",
      "Iteration 76, loss = 0.53704310\n",
      "Iteration 77, loss = 0.53677223\n",
      "Iteration 78, loss = 0.53651642\n",
      "Iteration 79, loss = 0.53625914\n",
      "Iteration 80, loss = 0.53597888\n",
      "Iteration 81, loss = 0.53569719\n",
      "Iteration 82, loss = 0.53541507\n",
      "Iteration 83, loss = 0.53515044\n",
      "Iteration 84, loss = 0.53485565\n",
      "Iteration 85, loss = 0.53456828\n",
      "Iteration 86, loss = 0.53429831\n",
      "Iteration 87, loss = 0.53401147\n",
      "Iteration 88, loss = 0.53370353\n",
      "Iteration 89, loss = 0.53343348\n",
      "Iteration 90, loss = 0.53315952\n",
      "Iteration 91, loss = 0.53285240\n",
      "Iteration 92, loss = 0.53255219\n",
      "Iteration 93, loss = 0.53229232\n",
      "Iteration 94, loss = 0.53195800\n",
      "Iteration 95, loss = 0.53163326\n",
      "Iteration 96, loss = 0.53136057\n",
      "Iteration 97, loss = 0.53105795\n",
      "Iteration 98, loss = 0.53074030\n",
      "Iteration 99, loss = 0.53039538\n",
      "Iteration 100, loss = 0.53008679\n",
      "Iteration 101, loss = 0.52983833\n",
      "Iteration 102, loss = 0.52950209\n",
      "Iteration 103, loss = 0.52913793\n",
      "Iteration 104, loss = 0.52882991\n",
      "Iteration 105, loss = 0.52849560\n",
      "Iteration 106, loss = 0.52815736\n",
      "Iteration 107, loss = 0.52784735\n",
      "Iteration 108, loss = 0.52751547\n",
      "Iteration 109, loss = 0.52716854\n",
      "Iteration 110, loss = 0.52686595\n",
      "Iteration 111, loss = 0.52653679\n",
      "Iteration 112, loss = 0.52616968\n",
      "Iteration 113, loss = 0.52585584\n",
      "Iteration 114, loss = 0.52550065\n",
      "Iteration 115, loss = 0.52516945\n",
      "Iteration 116, loss = 0.52481866\n",
      "Iteration 117, loss = 0.52449746\n",
      "Iteration 118, loss = 0.52412335\n",
      "Iteration 119, loss = 0.52379917\n",
      "Iteration 120, loss = 0.52344017\n",
      "Iteration 121, loss = 0.52310229\n",
      "Iteration 122, loss = 0.52273400\n",
      "Iteration 123, loss = 0.52240265\n",
      "Iteration 124, loss = 0.52202360\n",
      "Iteration 125, loss = 0.52168715\n",
      "Iteration 126, loss = 0.52135190\n",
      "Iteration 127, loss = 0.52100372\n",
      "Iteration 128, loss = 0.52065114\n",
      "Iteration 129, loss = 0.52028701\n",
      "Iteration 130, loss = 0.51991638\n",
      "Iteration 131, loss = 0.51954235\n",
      "Iteration 132, loss = 0.51920367\n",
      "Iteration 133, loss = 0.51885686\n",
      "Iteration 134, loss = 0.51850258\n",
      "Iteration 135, loss = 0.51812808\n",
      "Iteration 136, loss = 0.51778351\n",
      "Iteration 137, loss = 0.51741273\n",
      "Iteration 138, loss = 0.51704936\n",
      "Iteration 139, loss = 0.51668567\n",
      "Iteration 140, loss = 0.51631735\n",
      "Iteration 141, loss = 0.51592448\n",
      "Iteration 142, loss = 0.51559115\n",
      "Iteration 143, loss = 0.51523477\n",
      "Iteration 144, loss = 0.51486663\n",
      "Iteration 145, loss = 0.51453885\n",
      "Iteration 146, loss = 0.51415560\n",
      "Iteration 147, loss = 0.51379311\n",
      "Iteration 148, loss = 0.51343289\n",
      "Iteration 149, loss = 0.51306076\n",
      "Iteration 150, loss = 0.51272011\n",
      "Iteration 151, loss = 0.51235630\n",
      "Iteration 152, loss = 0.51199381\n",
      "Iteration 153, loss = 0.51162829\n",
      "Iteration 154, loss = 0.51125771\n",
      "Iteration 155, loss = 0.51097757\n",
      "Iteration 156, loss = 0.51058105\n",
      "Iteration 157, loss = 0.51022027\n",
      "Iteration 158, loss = 0.50984184\n",
      "Iteration 159, loss = 0.50951322\n",
      "Iteration 160, loss = 0.50920277\n",
      "Iteration 161, loss = 0.50878009\n",
      "Iteration 162, loss = 0.50845603\n",
      "Iteration 163, loss = 0.50818396\n",
      "Iteration 164, loss = 0.50775038\n",
      "Iteration 165, loss = 0.50739965\n",
      "Iteration 166, loss = 0.50707225\n",
      "Iteration 167, loss = 0.50671292\n",
      "Iteration 168, loss = 0.50639280\n",
      "Iteration 169, loss = 0.50601781\n",
      "Iteration 170, loss = 0.50573769\n",
      "Iteration 171, loss = 0.50535222\n",
      "Iteration 172, loss = 0.50504658\n",
      "Iteration 173, loss = 0.50470882\n",
      "Iteration 174, loss = 0.50436707\n",
      "Iteration 175, loss = 0.50403403\n",
      "Iteration 176, loss = 0.50372177\n",
      "Iteration 177, loss = 0.50335906\n",
      "Iteration 178, loss = 0.50302816\n",
      "Iteration 179, loss = 0.50271710\n",
      "Iteration 180, loss = 0.50239678\n",
      "Iteration 181, loss = 0.50209360\n",
      "Iteration 182, loss = 0.50173983\n",
      "Iteration 183, loss = 0.50145327\n",
      "Iteration 184, loss = 0.50110244\n",
      "Iteration 185, loss = 0.50079803\n",
      "Iteration 186, loss = 0.50047512\n",
      "Iteration 187, loss = 0.50020448\n",
      "Iteration 188, loss = 0.49988740\n",
      "Iteration 189, loss = 0.49957029\n",
      "Iteration 190, loss = 0.49923209\n",
      "Iteration 191, loss = 0.49898067\n",
      "Iteration 192, loss = 0.49864300\n",
      "Iteration 193, loss = 0.49836161\n",
      "Iteration 194, loss = 0.49802889\n",
      "Iteration 195, loss = 0.49776138\n",
      "Iteration 196, loss = 0.49753839\n",
      "Iteration 197, loss = 0.49715674\n",
      "Iteration 198, loss = 0.49688634\n",
      "Iteration 199, loss = 0.49657450\n",
      "Iteration 200, loss = 0.49631315\n",
      "Iteration 201, loss = 0.49602393\n",
      "Iteration 202, loss = 0.49573045\n",
      "Iteration 203, loss = 0.49545081\n",
      "Iteration 204, loss = 0.49517497\n",
      "Iteration 205, loss = 0.49493770\n",
      "Iteration 206, loss = 0.49463195\n",
      "Iteration 207, loss = 0.49436828\n",
      "Iteration 208, loss = 0.49414945\n",
      "Iteration 209, loss = 0.49383913\n",
      "Iteration 210, loss = 0.49360617\n",
      "Iteration 211, loss = 0.49330240\n",
      "Iteration 212, loss = 0.49303346\n",
      "Iteration 213, loss = 0.49277244\n",
      "Iteration 214, loss = 0.49254133\n",
      "Iteration 215, loss = 0.49226848\n",
      "Iteration 216, loss = 0.49205029\n",
      "Iteration 217, loss = 0.49178479\n",
      "Iteration 218, loss = 0.49151419\n",
      "Iteration 219, loss = 0.49127862\n",
      "Iteration 220, loss = 0.49104949\n",
      "Iteration 221, loss = 0.49081090\n",
      "Iteration 222, loss = 0.49054914\n",
      "Iteration 223, loss = 0.49034482\n",
      "Iteration 224, loss = 0.49006318\n",
      "Iteration 225, loss = 0.48985653\n",
      "Iteration 226, loss = 0.48962393\n",
      "Iteration 227, loss = 0.48938574\n",
      "Iteration 228, loss = 0.48915413\n",
      "Iteration 229, loss = 0.48892496\n",
      "Iteration 230, loss = 0.48872956\n",
      "Iteration 231, loss = 0.48848134\n",
      "Iteration 232, loss = 0.48827403\n",
      "Iteration 233, loss = 0.48805754\n",
      "Iteration 234, loss = 0.48786448\n",
      "Iteration 235, loss = 0.48761585\n",
      "Iteration 236, loss = 0.48740469\n",
      "Iteration 237, loss = 0.48721250\n",
      "Iteration 238, loss = 0.48700685\n",
      "Iteration 239, loss = 0.48679335\n",
      "Iteration 240, loss = 0.48663005\n",
      "Iteration 241, loss = 0.48644149\n",
      "Iteration 242, loss = 0.48630363\n",
      "Iteration 243, loss = 0.48600232\n",
      "Iteration 244, loss = 0.48577640\n",
      "Iteration 245, loss = 0.48561337\n",
      "Iteration 246, loss = 0.48542837\n",
      "Iteration 247, loss = 0.48523489\n",
      "Iteration 248, loss = 0.48502732\n",
      "Iteration 249, loss = 0.48484323\n",
      "Iteration 250, loss = 0.48466811\n",
      "Iteration 251, loss = 0.48448418\n",
      "Iteration 252, loss = 0.48430527\n",
      "Iteration 253, loss = 0.48416262\n",
      "Iteration 254, loss = 0.48397411\n",
      "Iteration 255, loss = 0.48378320\n",
      "Iteration 256, loss = 0.48361205\n",
      "Iteration 257, loss = 0.48343301\n",
      "Iteration 258, loss = 0.48325429\n",
      "Iteration 259, loss = 0.48316335\n",
      "Iteration 260, loss = 0.48292329\n",
      "Iteration 261, loss = 0.48274796\n",
      "Iteration 262, loss = 0.48260812\n",
      "Iteration 263, loss = 0.48243738\n",
      "Iteration 264, loss = 0.48227366\n",
      "Iteration 265, loss = 0.48209977\n",
      "Iteration 266, loss = 0.48194178\n",
      "Iteration 267, loss = 0.48181054\n",
      "Iteration 268, loss = 0.48163622\n",
      "Iteration 269, loss = 0.48148485\n",
      "Iteration 270, loss = 0.48132877\n",
      "Iteration 271, loss = 0.48118297\n",
      "Iteration 272, loss = 0.48102885\n",
      "Iteration 273, loss = 0.48091711\n",
      "Iteration 274, loss = 0.48071624\n",
      "Iteration 275, loss = 0.48060246\n",
      "Iteration 276, loss = 0.48046549\n",
      "Iteration 277, loss = 0.48031901\n",
      "Iteration 278, loss = 0.48016806\n",
      "Iteration 279, loss = 0.48003976\n",
      "Iteration 280, loss = 0.47990899\n",
      "Iteration 281, loss = 0.47976717\n",
      "Iteration 282, loss = 0.47966141\n",
      "Iteration 283, loss = 0.47950166\n",
      "Iteration 284, loss = 0.47939492\n",
      "Iteration 285, loss = 0.47924254\n",
      "Iteration 286, loss = 0.47916370\n",
      "Iteration 287, loss = 0.47897674\n",
      "Iteration 288, loss = 0.47886065\n",
      "Iteration 289, loss = 0.47873575\n",
      "Iteration 290, loss = 0.47860925\n",
      "Iteration 291, loss = 0.47848316\n",
      "Iteration 292, loss = 0.47839874\n",
      "Iteration 293, loss = 0.47826303\n",
      "Iteration 294, loss = 0.47812010\n",
      "Iteration 295, loss = 0.47801772\n",
      "Iteration 296, loss = 0.47791848\n",
      "Iteration 297, loss = 0.47778467\n",
      "Iteration 298, loss = 0.47765793\n",
      "Iteration 299, loss = 0.47759744\n",
      "Iteration 300, loss = 0.47743976\n",
      "Iteration 1, loss = 0.86415934\n",
      "Iteration 2, loss = 0.83387255\n",
      "Iteration 3, loss = 0.79215896\n",
      "Iteration 4, loss = 0.74612813\n",
      "Iteration 5, loss = 0.70359132\n",
      "Iteration 6, loss = 0.66525015\n",
      "Iteration 7, loss = 0.63535431\n",
      "Iteration 8, loss = 0.61143596\n",
      "Iteration 9, loss = 0.59285544\n",
      "Iteration 10, loss = 0.58099604\n",
      "Iteration 11, loss = 0.57187804\n",
      "Iteration 12, loss = 0.56595131\n",
      "Iteration 13, loss = 0.56167410\n",
      "Iteration 14, loss = 0.55894193\n",
      "Iteration 15, loss = 0.55725095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.55614870\n",
      "Iteration 17, loss = 0.55547196\n",
      "Iteration 18, loss = 0.55475951\n",
      "Iteration 19, loss = 0.55423815\n",
      "Iteration 20, loss = 0.55388458\n",
      "Iteration 21, loss = 0.55340248\n",
      "Iteration 22, loss = 0.55296294\n",
      "Iteration 23, loss = 0.55264189\n",
      "Iteration 24, loss = 0.55222565\n",
      "Iteration 25, loss = 0.55182338\n",
      "Iteration 26, loss = 0.55141391\n",
      "Iteration 27, loss = 0.55103108\n",
      "Iteration 28, loss = 0.55062961\n",
      "Iteration 29, loss = 0.55023810\n",
      "Iteration 30, loss = 0.54996087\n",
      "Iteration 31, loss = 0.54948581\n",
      "Iteration 32, loss = 0.54913074\n",
      "Iteration 33, loss = 0.54879439\n",
      "Iteration 34, loss = 0.54837526\n",
      "Iteration 35, loss = 0.54801946\n",
      "Iteration 36, loss = 0.54763116\n",
      "Iteration 37, loss = 0.54725383\n",
      "Iteration 38, loss = 0.54690364\n",
      "Iteration 39, loss = 0.54661729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.54617954\n",
      "Iteration 41, loss = 0.54586006\n",
      "Iteration 42, loss = 0.54551498\n",
      "Iteration 43, loss = 0.54513368\n",
      "Iteration 44, loss = 0.54479882\n",
      "Iteration 45, loss = 0.54444856\n",
      "Iteration 46, loss = 0.54410913\n",
      "Iteration 47, loss = 0.54376398\n",
      "Iteration 48, loss = 0.54343851\n",
      "Iteration 49, loss = 0.54307936\n",
      "Iteration 50, loss = 0.54276614\n",
      "Iteration 51, loss = 0.54241684\n",
      "Iteration 52, loss = 0.54212385\n",
      "Iteration 53, loss = 0.54173158\n",
      "Iteration 54, loss = 0.54138617\n",
      "Iteration 55, loss = 0.54111220\n",
      "Iteration 56, loss = 0.54073377\n",
      "Iteration 57, loss = 0.54041836\n",
      "Iteration 58, loss = 0.54008281\n",
      "Iteration 59, loss = 0.53973681\n",
      "Iteration 60, loss = 0.53940652\n",
      "Iteration 61, loss = 0.53907502\n",
      "Iteration 62, loss = 0.53876033\n",
      "Iteration 63, loss = 0.53841614\n",
      "Iteration 64, loss = 0.53812168\n",
      "Iteration 65, loss = 0.53775212\n",
      "Iteration 66, loss = 0.53744226\n",
      "Iteration 67, loss = 0.53710552\n",
      "Iteration 68, loss = 0.53678934\n",
      "Iteration 69, loss = 0.53644063\n",
      "Iteration 70, loss = 0.53615073\n",
      "Iteration 71, loss = 0.53579488\n",
      "Iteration 72, loss = 0.53544979\n",
      "Iteration 73, loss = 0.53513011\n",
      "Iteration 74, loss = 0.53483672\n",
      "Iteration 75, loss = 0.53448258\n",
      "Iteration 76, loss = 0.53418266\n",
      "Iteration 77, loss = 0.53382359\n",
      "Iteration 78, loss = 0.53348723\n",
      "Iteration 79, loss = 0.53317775\n",
      "Iteration 80, loss = 0.53283449\n",
      "Iteration 81, loss = 0.53250736\n",
      "Iteration 82, loss = 0.53217458\n",
      "Iteration 83, loss = 0.53184083\n",
      "Iteration 84, loss = 0.53153384\n",
      "Iteration 85, loss = 0.53118647\n",
      "Iteration 86, loss = 0.53082966\n",
      "Iteration 87, loss = 0.53050731\n",
      "Iteration 88, loss = 0.53017787\n",
      "Iteration 89, loss = 0.52983493\n",
      "Iteration 90, loss = 0.52951512\n",
      "Iteration 91, loss = 0.52916275\n",
      "Iteration 92, loss = 0.52881828\n",
      "Iteration 93, loss = 0.52849216\n",
      "Iteration 94, loss = 0.52814161\n",
      "Iteration 95, loss = 0.52784155\n",
      "Iteration 96, loss = 0.52747047\n",
      "Iteration 97, loss = 0.52712689\n",
      "Iteration 98, loss = 0.52678633\n",
      "Iteration 99, loss = 0.52644018\n",
      "Iteration 100, loss = 0.52612846\n",
      "Iteration 101, loss = 0.52581661\n",
      "Iteration 102, loss = 0.52543732\n",
      "Iteration 103, loss = 0.52507931\n",
      "Iteration 104, loss = 0.52473139\n",
      "Iteration 105, loss = 0.52439384\n",
      "Iteration 106, loss = 0.52403696\n",
      "Iteration 107, loss = 0.52368399\n",
      "Iteration 108, loss = 0.52334561\n",
      "Iteration 109, loss = 0.52299249\n",
      "Iteration 110, loss = 0.52264110\n",
      "Iteration 111, loss = 0.52230351\n",
      "Iteration 112, loss = 0.52196700\n",
      "Iteration 113, loss = 0.52158498\n",
      "Iteration 114, loss = 0.52126024\n",
      "Iteration 115, loss = 0.52090334\n",
      "Iteration 116, loss = 0.52055452\n",
      "Iteration 117, loss = 0.52020495\n",
      "Iteration 118, loss = 0.51984783\n",
      "Iteration 119, loss = 0.51947362\n",
      "Iteration 120, loss = 0.51914520\n",
      "Iteration 121, loss = 0.51878611\n",
      "Iteration 122, loss = 0.51842947\n",
      "Iteration 123, loss = 0.51807025\n",
      "Iteration 124, loss = 0.51771098\n",
      "Iteration 125, loss = 0.51734794\n",
      "Iteration 126, loss = 0.51698323\n",
      "Iteration 127, loss = 0.51668185\n",
      "Iteration 128, loss = 0.51626620\n",
      "Iteration 129, loss = 0.51593122\n",
      "Iteration 130, loss = 0.51559038\n",
      "Iteration 131, loss = 0.51522366\n",
      "Iteration 132, loss = 0.51482788\n",
      "Iteration 133, loss = 0.51447619\n",
      "Iteration 134, loss = 0.51414086\n",
      "Iteration 135, loss = 0.51377465\n",
      "Iteration 136, loss = 0.51344469\n",
      "Iteration 137, loss = 0.51307573\n",
      "Iteration 138, loss = 0.51270809\n",
      "Iteration 139, loss = 0.51235145\n",
      "Iteration 140, loss = 0.51197193\n",
      "Iteration 141, loss = 0.51161519\n",
      "Iteration 142, loss = 0.51126229\n",
      "Iteration 143, loss = 0.51090436\n",
      "Iteration 144, loss = 0.51053279\n",
      "Iteration 145, loss = 0.51018215\n",
      "Iteration 146, loss = 0.50983835\n",
      "Iteration 147, loss = 0.50946364\n",
      "Iteration 148, loss = 0.50914131\n",
      "Iteration 149, loss = 0.50876322\n",
      "Iteration 150, loss = 0.50837286\n",
      "Iteration 151, loss = 0.50803409\n",
      "Iteration 152, loss = 0.50766938\n",
      "Iteration 153, loss = 0.50732666\n",
      "Iteration 154, loss = 0.50694132\n",
      "Iteration 155, loss = 0.50662500\n",
      "Iteration 156, loss = 0.50629627\n",
      "Iteration 157, loss = 0.50591482\n",
      "Iteration 158, loss = 0.50555804\n",
      "Iteration 159, loss = 0.50523808\n",
      "Iteration 160, loss = 0.50483082\n",
      "Iteration 161, loss = 0.50448551\n",
      "Iteration 162, loss = 0.50413950\n",
      "Iteration 163, loss = 0.50379742\n",
      "Iteration 164, loss = 0.50342836\n",
      "Iteration 165, loss = 0.50309468\n",
      "Iteration 166, loss = 0.50273417\n",
      "Iteration 167, loss = 0.50240593\n",
      "Iteration 168, loss = 0.50205302\n",
      "Iteration 169, loss = 0.50169771\n",
      "Iteration 170, loss = 0.50138741\n",
      "Iteration 171, loss = 0.50104374\n",
      "Iteration 172, loss = 0.50076619\n",
      "Iteration 173, loss = 0.50037700\n",
      "Iteration 174, loss = 0.50000768\n",
      "Iteration 175, loss = 0.49972398\n",
      "Iteration 176, loss = 0.49930112\n",
      "Iteration 177, loss = 0.49898509\n",
      "Iteration 178, loss = 0.49867519\n",
      "Iteration 179, loss = 0.49830690\n",
      "Iteration 180, loss = 0.49795876\n",
      "Iteration 181, loss = 0.49762648\n",
      "Iteration 182, loss = 0.49733538\n",
      "Iteration 183, loss = 0.49700586\n",
      "Iteration 184, loss = 0.49666623\n",
      "Iteration 185, loss = 0.49632120\n",
      "Iteration 186, loss = 0.49598698\n",
      "Iteration 187, loss = 0.49567677\n",
      "Iteration 188, loss = 0.49536350\n",
      "Iteration 189, loss = 0.49505062\n",
      "Iteration 190, loss = 0.49469968\n",
      "Iteration 191, loss = 0.49438273\n",
      "Iteration 192, loss = 0.49409252\n",
      "Iteration 193, loss = 0.49378060\n",
      "Iteration 194, loss = 0.49345064\n",
      "Iteration 195, loss = 0.49316226\n",
      "Iteration 196, loss = 0.49283180\n",
      "Iteration 197, loss = 0.49253886\n",
      "Iteration 198, loss = 0.49220539\n",
      "Iteration 199, loss = 0.49191378\n",
      "Iteration 200, loss = 0.49160222\n",
      "Iteration 201, loss = 0.49130411\n",
      "Iteration 202, loss = 0.49101008\n",
      "Iteration 203, loss = 0.49071375\n",
      "Iteration 204, loss = 0.49043484\n",
      "Iteration 205, loss = 0.49011650\n",
      "Iteration 206, loss = 0.48984202\n",
      "Iteration 207, loss = 0.48956284\n",
      "Iteration 208, loss = 0.48923016\n",
      "Iteration 209, loss = 0.48897056\n",
      "Iteration 210, loss = 0.48870013\n",
      "Iteration 211, loss = 0.48842154\n",
      "Iteration 212, loss = 0.48812977\n",
      "Iteration 213, loss = 0.48783570\n",
      "Iteration 214, loss = 0.48753065\n",
      "Iteration 215, loss = 0.48726065\n",
      "Iteration 216, loss = 0.48700419\n",
      "Iteration 217, loss = 0.48671640\n",
      "Iteration 218, loss = 0.48644109\n",
      "Iteration 219, loss = 0.48618523\n",
      "Iteration 220, loss = 0.48591561\n",
      "Iteration 221, loss = 0.48562568\n",
      "Iteration 222, loss = 0.48537038\n",
      "Iteration 223, loss = 0.48512140\n",
      "Iteration 224, loss = 0.48484213\n",
      "Iteration 225, loss = 0.48459274\n",
      "Iteration 226, loss = 0.48436657\n",
      "Iteration 227, loss = 0.48409516\n",
      "Iteration 228, loss = 0.48384673\n",
      "Iteration 229, loss = 0.48357900\n",
      "Iteration 230, loss = 0.48332279\n",
      "Iteration 231, loss = 0.48308705\n",
      "Iteration 232, loss = 0.48285205\n",
      "Iteration 233, loss = 0.48259066\n",
      "Iteration 234, loss = 0.48236337\n",
      "Iteration 235, loss = 0.48213364\n",
      "Iteration 236, loss = 0.48189187\n",
      "Iteration 237, loss = 0.48164704\n",
      "Iteration 238, loss = 0.48142149\n",
      "Iteration 239, loss = 0.48117147\n",
      "Iteration 240, loss = 0.48094474\n",
      "Iteration 241, loss = 0.48073792\n",
      "Iteration 242, loss = 0.48052821\n",
      "Iteration 243, loss = 0.48026534\n",
      "Iteration 244, loss = 0.48004944\n",
      "Iteration 245, loss = 0.47988477\n",
      "Iteration 246, loss = 0.47962375\n",
      "Iteration 247, loss = 0.47940337\n",
      "Iteration 248, loss = 0.47918155\n",
      "Iteration 249, loss = 0.47898856\n",
      "Iteration 250, loss = 0.47879110\n",
      "Iteration 251, loss = 0.47854799\n",
      "Iteration 252, loss = 0.47836769\n",
      "Iteration 253, loss = 0.47814591\n",
      "Iteration 254, loss = 0.47793201\n",
      "Iteration 255, loss = 0.47774320\n",
      "Iteration 256, loss = 0.47753215\n",
      "Iteration 257, loss = 0.47734925\n",
      "Iteration 258, loss = 0.47716220\n",
      "Iteration 259, loss = 0.47696741\n",
      "Iteration 260, loss = 0.47680562\n",
      "Iteration 261, loss = 0.47656967\n",
      "Iteration 262, loss = 0.47640834\n",
      "Iteration 263, loss = 0.47624779\n",
      "Iteration 264, loss = 0.47601738\n",
      "Iteration 265, loss = 0.47582833\n",
      "Iteration 266, loss = 0.47564545\n",
      "Iteration 267, loss = 0.47546505\n",
      "Iteration 268, loss = 0.47528268\n",
      "Iteration 269, loss = 0.47511938\n",
      "Iteration 270, loss = 0.47496429\n",
      "Iteration 271, loss = 0.47477251\n",
      "Iteration 272, loss = 0.47459600\n",
      "Iteration 273, loss = 0.47443940\n",
      "Iteration 274, loss = 0.47425558\n",
      "Iteration 275, loss = 0.47409758\n",
      "Iteration 276, loss = 0.47393154\n",
      "Iteration 277, loss = 0.47377194\n",
      "Iteration 278, loss = 0.47361291\n",
      "Iteration 279, loss = 0.47344788\n",
      "Iteration 280, loss = 0.47329416\n",
      "Iteration 281, loss = 0.47318915\n",
      "Iteration 282, loss = 0.47298199\n",
      "Iteration 283, loss = 0.47283979\n",
      "Iteration 284, loss = 0.47267366\n",
      "Iteration 285, loss = 0.47252910\n",
      "Iteration 286, loss = 0.47236653\n",
      "Iteration 287, loss = 0.47223986\n",
      "Iteration 288, loss = 0.47208574\n",
      "Iteration 289, loss = 0.47193156\n",
      "Iteration 290, loss = 0.47181389\n",
      "Iteration 291, loss = 0.47164835\n",
      "Iteration 292, loss = 0.47151563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 293, loss = 0.47137560\n",
      "Iteration 294, loss = 0.47123504\n",
      "Iteration 295, loss = 0.47114661\n",
      "Iteration 296, loss = 0.47096658\n",
      "Iteration 297, loss = 0.47084297\n",
      "Iteration 298, loss = 0.47070071\n",
      "Iteration 299, loss = 0.47058452\n",
      "Iteration 300, loss = 0.47044403\n",
      "Iteration 1, loss = 0.57739016\n",
      "Iteration 2, loss = 0.57510700\n",
      "Iteration 3, loss = 0.57234506\n",
      "Iteration 4, loss = 0.56834672\n",
      "Iteration 5, loss = 0.56473487\n",
      "Iteration 6, loss = 0.56176123\n",
      "Iteration 7, loss = 0.55919630\n",
      "Iteration 8, loss = 0.55669887\n",
      "Iteration 9, loss = 0.55504546\n",
      "Iteration 10, loss = 0.55333525\n",
      "Iteration 11, loss = 0.55213025\n",
      "Iteration 12, loss = 0.55117760\n",
      "Iteration 13, loss = 0.55032460\n",
      "Iteration 14, loss = 0.54972865\n",
      "Iteration 15, loss = 0.54915420\n",
      "Iteration 16, loss = 0.54875731\n",
      "Iteration 17, loss = 0.54837447\n",
      "Iteration 18, loss = 0.54792934\n",
      "Iteration 19, loss = 0.54777841\n",
      "Iteration 20, loss = 0.54728878\n",
      "Iteration 21, loss = 0.54693015\n",
      "Iteration 22, loss = 0.54659503\n",
      "Iteration 23, loss = 0.54629755\n",
      "Iteration 24, loss = 0.54593862\n",
      "Iteration 25, loss = 0.54562480\n",
      "Iteration 26, loss = 0.54528291\n",
      "Iteration 27, loss = 0.54498729\n",
      "Iteration 28, loss = 0.54465137\n",
      "Iteration 29, loss = 0.54434965\n",
      "Iteration 30, loss = 0.54400926\n",
      "Iteration 31, loss = 0.54370866\n",
      "Iteration 32, loss = 0.54336667\n",
      "Iteration 33, loss = 0.54306424\n",
      "Iteration 34, loss = 0.54276807\n",
      "Iteration 35, loss = 0.54245391\n",
      "Iteration 36, loss = 0.54210820\n",
      "Iteration 37, loss = 0.54178222\n",
      "Iteration 38, loss = 0.54145957\n",
      "Iteration 39, loss = 0.54116088\n",
      "Iteration 40, loss = 0.54083442\n",
      "Iteration 41, loss = 0.54051098\n",
      "Iteration 42, loss = 0.54023890\n",
      "Iteration 43, loss = 0.53989243\n",
      "Iteration 44, loss = 0.53958574\n",
      "Iteration 45, loss = 0.53926239\n",
      "Iteration 46, loss = 0.53895002\n",
      "Iteration 47, loss = 0.53867679\n",
      "Iteration 48, loss = 0.53832662\n",
      "Iteration 49, loss = 0.53799386\n",
      "Iteration 50, loss = 0.53766842\n",
      "Iteration 51, loss = 0.53744879\n",
      "Iteration 52, loss = 0.53706115\n",
      "Iteration 53, loss = 0.53675605\n",
      "Iteration 54, loss = 0.53644192\n",
      "Iteration 55, loss = 0.53611409\n",
      "Iteration 56, loss = 0.53580005\n",
      "Iteration 57, loss = 0.53557008\n",
      "Iteration 58, loss = 0.53518557\n",
      "Iteration 59, loss = 0.53487892\n",
      "Iteration 60, loss = 0.53454735\n",
      "Iteration 61, loss = 0.53426408\n",
      "Iteration 62, loss = 0.53392832\n",
      "Iteration 63, loss = 0.53362172\n",
      "Iteration 64, loss = 0.53329816\n",
      "Iteration 65, loss = 0.53298255\n",
      "Iteration 66, loss = 0.53265288\n",
      "Iteration 67, loss = 0.53234456\n",
      "Iteration 68, loss = 0.53204525\n",
      "Iteration 69, loss = 0.53174331\n",
      "Iteration 70, loss = 0.53138734\n",
      "Iteration 71, loss = 0.53107704\n",
      "Iteration 72, loss = 0.53075253\n",
      "Iteration 73, loss = 0.53046803\n",
      "Iteration 74, loss = 0.53015577\n",
      "Iteration 75, loss = 0.52982412\n",
      "Iteration 76, loss = 0.52951799\n",
      "Iteration 77, loss = 0.52918105\n",
      "Iteration 78, loss = 0.52887661\n",
      "Iteration 79, loss = 0.52857143\n",
      "Iteration 80, loss = 0.52823612\n",
      "Iteration 81, loss = 0.52794083\n",
      "Iteration 82, loss = 0.52762721\n",
      "Iteration 83, loss = 0.52730809\n",
      "Iteration 84, loss = 0.52698473\n",
      "Iteration 85, loss = 0.52671069\n",
      "Iteration 86, loss = 0.52637981\n",
      "Iteration 87, loss = 0.52604731\n",
      "Iteration 88, loss = 0.52577357\n",
      "Iteration 89, loss = 0.52541120\n",
      "Iteration 90, loss = 0.52510980\n",
      "Iteration 91, loss = 0.52479921\n",
      "Iteration 92, loss = 0.52447385\n",
      "Iteration 93, loss = 0.52417096\n",
      "Iteration 94, loss = 0.52387292\n",
      "Iteration 95, loss = 0.52355395\n",
      "Iteration 96, loss = 0.52324980\n",
      "Iteration 97, loss = 0.52293272\n",
      "Iteration 98, loss = 0.52263157\n",
      "Iteration 99, loss = 0.52229954\n",
      "Iteration 100, loss = 0.52208665\n",
      "Iteration 101, loss = 0.52170778\n",
      "Iteration 102, loss = 0.52139109\n",
      "Iteration 103, loss = 0.52114888\n",
      "Iteration 104, loss = 0.52084361\n",
      "Iteration 105, loss = 0.52050093\n",
      "Iteration 106, loss = 0.52016151\n",
      "Iteration 107, loss = 0.51987113\n",
      "Iteration 108, loss = 0.51956718\n",
      "Iteration 109, loss = 0.51928036\n",
      "Iteration 110, loss = 0.51895170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 111, loss = 0.51866721\n",
      "Iteration 112, loss = 0.51835905\n",
      "Iteration 113, loss = 0.51806508\n",
      "Iteration 114, loss = 0.51777753\n",
      "Iteration 115, loss = 0.51752936\n",
      "Iteration 116, loss = 0.51717890\n",
      "Iteration 117, loss = 0.51686522\n",
      "Iteration 118, loss = 0.51659535\n",
      "Iteration 119, loss = 0.51630080\n",
      "Iteration 120, loss = 0.51601365\n",
      "Iteration 121, loss = 0.51572758\n",
      "Iteration 122, loss = 0.51544482\n",
      "Iteration 123, loss = 0.51514525\n",
      "Iteration 124, loss = 0.51486924\n",
      "Iteration 125, loss = 0.51456661\n",
      "Iteration 126, loss = 0.51431658\n",
      "Iteration 127, loss = 0.51402743\n",
      "Iteration 128, loss = 0.51372957\n",
      "Iteration 129, loss = 0.51345692\n",
      "Iteration 130, loss = 0.51319312\n",
      "Iteration 131, loss = 0.51292731\n",
      "Iteration 132, loss = 0.51263107\n",
      "Iteration 133, loss = 0.51235346\n",
      "Iteration 134, loss = 0.51211505\n",
      "Iteration 135, loss = 0.51181481\n",
      "Iteration 136, loss = 0.51155181\n",
      "Iteration 137, loss = 0.51127864\n",
      "Iteration 138, loss = 0.51105711\n",
      "Iteration 139, loss = 0.51077093\n",
      "Iteration 140, loss = 0.51055062\n",
      "Iteration 141, loss = 0.51024077\n",
      "Iteration 142, loss = 0.50997017\n",
      "Iteration 143, loss = 0.50975013\n",
      "Iteration 144, loss = 0.50947766\n",
      "Iteration 145, loss = 0.50925653\n",
      "Iteration 146, loss = 0.50896719\n",
      "Iteration 147, loss = 0.50869242\n",
      "Iteration 148, loss = 0.50847437\n",
      "Iteration 149, loss = 0.50823489\n",
      "Iteration 150, loss = 0.50798927\n",
      "Iteration 151, loss = 0.50770835\n",
      "Iteration 152, loss = 0.50749927\n",
      "Iteration 153, loss = 0.50723934\n",
      "Iteration 154, loss = 0.50700900\n",
      "Iteration 155, loss = 0.50678176\n",
      "Iteration 156, loss = 0.50655002\n",
      "Iteration 157, loss = 0.50633878\n",
      "Iteration 158, loss = 0.50608745\n",
      "Iteration 159, loss = 0.50583116\n",
      "Iteration 160, loss = 0.50560713\n",
      "Iteration 161, loss = 0.50540880\n",
      "Iteration 162, loss = 0.50517376\n",
      "Iteration 163, loss = 0.50494606\n",
      "Iteration 164, loss = 0.50477291\n",
      "Iteration 165, loss = 0.50448992\n",
      "Iteration 166, loss = 0.50431142\n",
      "Iteration 167, loss = 0.50407400\n",
      "Iteration 168, loss = 0.50388639\n",
      "Iteration 169, loss = 0.50365397\n",
      "Iteration 170, loss = 0.50344697\n",
      "Iteration 171, loss = 0.50324415\n",
      "Iteration 172, loss = 0.50301769\n",
      "Iteration 173, loss = 0.50280462\n",
      "Iteration 174, loss = 0.50261360\n",
      "Iteration 175, loss = 0.50242997\n",
      "Iteration 176, loss = 0.50221651\n",
      "Iteration 177, loss = 0.50201007\n",
      "Iteration 178, loss = 0.50182346\n",
      "Iteration 179, loss = 0.50164447\n",
      "Iteration 180, loss = 0.50141722\n",
      "Iteration 181, loss = 0.50126986\n",
      "Iteration 182, loss = 0.50101736\n",
      "Iteration 183, loss = 0.50085138\n",
      "Iteration 184, loss = 0.50064606\n",
      "Iteration 185, loss = 0.50050899\n",
      "Iteration 186, loss = 0.50027348\n",
      "Iteration 187, loss = 0.50009540\n",
      "Iteration 188, loss = 0.49991798\n",
      "Iteration 189, loss = 0.49973931\n",
      "Iteration 190, loss = 0.49957928\n",
      "Iteration 191, loss = 0.49937736\n",
      "Iteration 192, loss = 0.49921789\n",
      "Iteration 193, loss = 0.49903350\n",
      "Iteration 194, loss = 0.49886717\n",
      "Iteration 195, loss = 0.49868101\n",
      "Iteration 196, loss = 0.49852983\n",
      "Iteration 197, loss = 0.49836711\n",
      "Iteration 198, loss = 0.49818630\n",
      "Iteration 199, loss = 0.49802961\n",
      "Iteration 200, loss = 0.49786109\n",
      "Iteration 201, loss = 0.49769081\n",
      "Iteration 202, loss = 0.49752933\n",
      "Iteration 203, loss = 0.49737243\n",
      "Iteration 204, loss = 0.49724461\n",
      "Iteration 205, loss = 0.49709554\n",
      "Iteration 206, loss = 0.49690051\n",
      "Iteration 207, loss = 0.49676830\n",
      "Iteration 208, loss = 0.49660006\n",
      "Iteration 209, loss = 0.49645120\n",
      "Iteration 210, loss = 0.49630699\n",
      "Iteration 211, loss = 0.49615982\n",
      "Iteration 212, loss = 0.49599531\n",
      "Iteration 213, loss = 0.49587947\n",
      "Iteration 214, loss = 0.49571796\n",
      "Iteration 215, loss = 0.49557630\n",
      "Iteration 216, loss = 0.49542909\n",
      "Iteration 217, loss = 0.49532692\n",
      "Iteration 218, loss = 0.49513524\n",
      "Iteration 219, loss = 0.49500977\n",
      "Iteration 220, loss = 0.49487500\n",
      "Iteration 221, loss = 0.49472468\n",
      "Iteration 222, loss = 0.49462747\n",
      "Iteration 223, loss = 0.49448499\n",
      "Iteration 224, loss = 0.49432614\n",
      "Iteration 225, loss = 0.49421095\n",
      "Iteration 226, loss = 0.49408586\n",
      "Iteration 227, loss = 0.49393897\n",
      "Iteration 228, loss = 0.49381141\n",
      "Iteration 229, loss = 0.49368477\n",
      "Iteration 230, loss = 0.49360729\n",
      "Iteration 231, loss = 0.49346916\n",
      "Iteration 232, loss = 0.49331234\n",
      "Iteration 233, loss = 0.49322290\n",
      "Iteration 234, loss = 0.49306895\n",
      "Iteration 235, loss = 0.49296251\n",
      "Iteration 236, loss = 0.49283486\n",
      "Iteration 237, loss = 0.49274823\n",
      "Iteration 238, loss = 0.49264370\n",
      "Iteration 239, loss = 0.49249560\n",
      "Iteration 240, loss = 0.49236776\n",
      "Iteration 241, loss = 0.49225228\n",
      "Iteration 242, loss = 0.49213093\n",
      "Iteration 243, loss = 0.49203152\n",
      "Iteration 244, loss = 0.49192665\n",
      "Iteration 245, loss = 0.49184832\n",
      "Iteration 246, loss = 0.49169634\n",
      "Iteration 247, loss = 0.49160804\n",
      "Iteration 248, loss = 0.49149943\n",
      "Iteration 249, loss = 0.49139035\n",
      "Iteration 250, loss = 0.49126999\n",
      "Iteration 251, loss = 0.49118628\n",
      "Iteration 252, loss = 0.49106182\n",
      "Iteration 253, loss = 0.49095471\n",
      "Iteration 254, loss = 0.49088518\n",
      "Iteration 255, loss = 0.49076526\n",
      "Iteration 256, loss = 0.49067063\n",
      "Iteration 257, loss = 0.49056565\n",
      "Iteration 258, loss = 0.49045995\n",
      "Iteration 259, loss = 0.49040146\n",
      "Iteration 260, loss = 0.49027765\n",
      "Iteration 261, loss = 0.49020705\n",
      "Iteration 262, loss = 0.49008414\n",
      "Iteration 263, loss = 0.49001868\n",
      "Iteration 264, loss = 0.48989610\n",
      "Iteration 265, loss = 0.48983633\n",
      "Iteration 266, loss = 0.48975631\n",
      "Iteration 267, loss = 0.48963160\n",
      "Iteration 268, loss = 0.48954941\n",
      "Iteration 269, loss = 0.48945704\n",
      "Iteration 270, loss = 0.48935904\n",
      "Iteration 271, loss = 0.48929928\n",
      "Iteration 272, loss = 0.48920958\n",
      "Iteration 273, loss = 0.48911023\n",
      "Iteration 274, loss = 0.48901371\n",
      "Iteration 275, loss = 0.48892641\n",
      "Iteration 276, loss = 0.48886178\n",
      "Iteration 277, loss = 0.48878445\n",
      "Iteration 278, loss = 0.48869225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(transfusion_values, transfusion_classes):\n",
    "    data_train, target_train = transfusion_values[train], transfusion_classes[train]\n",
    "    data_test, target_test = transfusion_values[test], transfusion_classes[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       570\n",
      "           1       0.26      0.29      0.27       178\n",
      "\n",
      "    accuracy                           0.64       748\n",
      "   macro avg       0.52      0.52      0.52       748\n",
      "weighted avg       0.65      0.64      0.64       748\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[426 144]\n",
      " [127  51]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       570\n",
      "           1       0.24      0.29      0.26       178\n",
      "\n",
      "    accuracy                           0.62       748\n",
      "   macro avg       0.50      0.50      0.50       748\n",
      "weighted avg       0.64      0.62      0.63       748\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[411 159]\n",
      " [127  51]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       570\n",
      "           1       0.45      0.22      0.30       178\n",
      "\n",
      "    accuracy                           0.75       748\n",
      "   macro avg       0.62      0.57      0.57       748\n",
      "weighted avg       0.71      0.75      0.72       748\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[522  48]\n",
      " [138  40]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.87       570\n",
      "           1       0.59      0.27      0.37       178\n",
      "\n",
      "    accuracy                           0.78       748\n",
      "   macro avg       0.70      0.61      0.62       748\n",
      "weighted avg       0.75      0.78      0.75       748\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[536  34]\n",
      " [130  48]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.96      0.87       570\n",
      "           1       0.63      0.20      0.31       178\n",
      "\n",
      "    accuracy                           0.78       748\n",
      "   macro avg       0.71      0.58      0.59       748\n",
      "weighted avg       0.76      0.78      0.74       748\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[549  21]\n",
      " [142  36]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(transfusion_classes, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusão: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(transfusion_classes, predicted_classes[classificador]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}