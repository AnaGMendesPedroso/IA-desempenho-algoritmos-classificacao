{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Contraceptive:\n"
     ]
    },
    {
     "data": {
      "text/plain": "      Wifes age  Wifes education  Husbands education  \\\n0            24                2                   3   \n1            45                1                   3   \n2            43                2                   3   \n3            42                3                   2   \n4            36                3                   3   \n...         ...              ...                 ...   \n1468         33                4                   4   \n1469         33                4                   4   \n1470         39                3                   3   \n1471         33                3                   3   \n1472         17                3                   3   \n\n      Number of children ever born  Wifes religion   Wifes now working  \\\n0                                3               1                   1   \n1                               10               1                   1   \n2                                7               1                   1   \n3                                9               1                   1   \n4                                8               1                   1   \n...                            ...             ...                 ...   \n1468                             2               1                   0   \n1469                             3               1                   1   \n1470                             8               1                   0   \n1471                             4               1                   0   \n1472                             1               1                   1   \n\n      Husbands occupation  Standard-of-living index  Media exposure  \\\n0                       2                         3               0   \n1                       3                         4               0   \n2                       3                         4               0   \n3                       3                         3               0   \n4                       3                         2               0   \n...                   ...                       ...             ...   \n1468                    2                         4               0   \n1469                    1                         4               0   \n1470                    1                         4               0   \n1471                    2                         2               0   \n1472                    2                         4               0   \n\n      Contraceptive method used  \n0                             1  \n1                             1  \n2                             1  \n3                             1  \n4                             1  \n...                         ...  \n1468                          3  \n1469                          3  \n1470                          3  \n1471                          3  \n1472                          3  \n\n[1473 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wifes age</th>\n      <th>Wifes education</th>\n      <th>Husbands education</th>\n      <th>Number of children ever born</th>\n      <th>Wifes religion</th>\n      <th>Wifes now working</th>\n      <th>Husbands occupation</th>\n      <th>Standard-of-living index</th>\n      <th>Media exposure</th>\n      <th>Contraceptive method used</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>45</td>\n      <td>1</td>\n      <td>3</td>\n      <td>10</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43</td>\n      <td>2</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>42</td>\n      <td>3</td>\n      <td>2</td>\n      <td>9</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36</td>\n      <td>3</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1468</th>\n      <td>33</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1469</th>\n      <td>33</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1470</th>\n      <td>39</td>\n      <td>3</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1471</th>\n      <td>33</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1472</th>\n      <td>17</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>1473 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_contraceptive = pd.read_table('contraceptive.data', sep=',')\n",
    "print(\"\\nDataset Contraceptive:\")\n",
    "dataset_contraceptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Wifes age', 'Wifes education', 'Husbands education',\n       'Number of children ever born', 'Wifes religion', ' Wifes now working',\n       'Husbands occupation', 'Standard-of-living index', 'Media exposure',\n       'Contraceptive method used'],\n      dtype='object')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_contraceptive.columns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contraceptive features:\n",
      "\n",
      "[[24  2  3 ...  2  3  0]\n",
      " [45  1  3 ...  3  4  0]\n",
      " [43  2  3 ...  3  4  0]\n",
      " ...\n",
      " [39  3  3 ...  1  4  0]\n",
      " [33  3  3 ...  2  2  0]\n",
      " [17  3  3 ...  2  4  0]]\n",
      "\n",
      "Contraceptive features shape:\n",
      "(1473, 9)\n"
     ]
    }
   ],
   "source": [
    "contraceptive_values = dataset_contraceptive.iloc[:,0:9].values\n",
    "print(\"\\nContraceptive features:\\n\")\n",
    "print(contraceptive_values)\n",
    "\n",
    "print(\"\\nContraceptive features shape:\")\n",
    "print(contraceptive_values.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contraceptive classes:\n",
      "\n",
      "[1 1 1 ... 3 3 3]\n",
      "\n",
      "Contraceptive classes shape:\n",
      "(1473,)\n"
     ]
    }
   ],
   "source": [
    "contraceptive_classes = dataset_contraceptive.iloc[:,9].values\n",
    "print(\"\\nContraceptive classes:\\n\")\n",
    "print(contraceptive_classes)\n",
    "print(\"\\nContraceptive classes shape:\")\n",
    "print(contraceptive_classes.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(contraceptive_classes.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(contraceptive_classes.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(contraceptive_classes.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(contraceptive_classes.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(contraceptive_classes.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07593644\n",
      "Iteration 2, loss = 1.07136764\n",
      "Iteration 3, loss = 1.06887229\n",
      "Iteration 4, loss = 1.06785272\n",
      "Iteration 5, loss = 1.06749855\n",
      "Iteration 6, loss = 1.06725973\n",
      "Iteration 7, loss = 1.06712127\n",
      "Iteration 8, loss = 1.06717109\n",
      "Iteration 9, loss = 1.06710908\n",
      "Iteration 10, loss = 1.06704979\n",
      "Iteration 11, loss = 1.06708844\n",
      "Iteration 12, loss = 1.06710580\n",
      "Iteration 13, loss = 1.06709748\n",
      "Iteration 14, loss = 1.06701612\n",
      "Iteration 15, loss = 1.06695209\n",
      "Iteration 16, loss = 1.06695255\n",
      "Iteration 17, loss = 1.06699154\n",
      "Iteration 18, loss = 1.06695342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12046579\n",
      "Iteration 2, loss = 1.08771384\n",
      "Iteration 3, loss = 1.07061444\n",
      "Iteration 4, loss = 1.06691385\n",
      "Iteration 5, loss = 1.06745108\n",
      "Iteration 6, loss = 1.06706597\n",
      "Iteration 7, loss = 1.06684070\n",
      "Iteration 8, loss = 1.06622478\n",
      "Iteration 9, loss = 1.06570574\n",
      "Iteration 10, loss = 1.06568397\n",
      "Iteration 11, loss = 1.06516742\n",
      "Iteration 12, loss = 1.06477421\n",
      "Iteration 13, loss = 1.06429058\n",
      "Iteration 14, loss = 1.06399698\n",
      "Iteration 15, loss = 1.06374971\n",
      "Iteration 16, loss = 1.06349820\n",
      "Iteration 17, loss = 1.06303512\n",
      "Iteration 18, loss = 1.06270246\n",
      "Iteration 19, loss = 1.06227780\n",
      "Iteration 20, loss = 1.06189499\n",
      "Iteration 21, loss = 1.06146748\n",
      "Iteration 22, loss = 1.06133790\n",
      "Iteration 23, loss = 1.06070435\n",
      "Iteration 24, loss = 1.06023014\n",
      "Iteration 25, loss = 1.06007665\n",
      "Iteration 26, loss = 1.05990066\n",
      "Iteration 27, loss = 1.05902038\n",
      "Iteration 28, loss = 1.05831649\n",
      "Iteration 29, loss = 1.05769912\n",
      "Iteration 30, loss = 1.05719645\n",
      "Iteration 31, loss = 1.05658205\n",
      "Iteration 32, loss = 1.05626799\n",
      "Iteration 33, loss = 1.05548265\n",
      "Iteration 34, loss = 1.05498555\n",
      "Iteration 35, loss = 1.05468017\n",
      "Iteration 36, loss = 1.05363199\n",
      "Iteration 37, loss = 1.05303244\n",
      "Iteration 38, loss = 1.05249643\n",
      "Iteration 39, loss = 1.05193777\n",
      "Iteration 40, loss = 1.05108512\n",
      "Iteration 41, loss = 1.05023931\n",
      "Iteration 42, loss = 1.05013733\n",
      "Iteration 43, loss = 1.04908602\n",
      "Iteration 44, loss = 1.04859973\n",
      "Iteration 45, loss = 1.04772667\n",
      "Iteration 46, loss = 1.04692906\n",
      "Iteration 47, loss = 1.04614696\n",
      "Iteration 48, loss = 1.04584711\n",
      "Iteration 49, loss = 1.04513022\n",
      "Iteration 50, loss = 1.04425123\n",
      "Iteration 51, loss = 1.04382684\n",
      "Iteration 52, loss = 1.04260636\n",
      "Iteration 53, loss = 1.04177301\n",
      "Iteration 54, loss = 1.04180922\n",
      "Iteration 55, loss = 1.04089936\n",
      "Iteration 56, loss = 1.04012593\n",
      "Iteration 57, loss = 1.03938690\n",
      "Iteration 58, loss = 1.03850713\n",
      "Iteration 59, loss = 1.03781448\n",
      "Iteration 60, loss = 1.03713442\n",
      "Iteration 61, loss = 1.03632014\n",
      "Iteration 62, loss = 1.03623163\n",
      "Iteration 63, loss = 1.03513029\n",
      "Iteration 64, loss = 1.03447515\n",
      "Iteration 65, loss = 1.03429341\n",
      "Iteration 66, loss = 1.03373811\n",
      "Iteration 67, loss = 1.03238933\n",
      "Iteration 68, loss = 1.03185492\n",
      "Iteration 69, loss = 1.03195881\n",
      "Iteration 70, loss = 1.03165813\n",
      "Iteration 71, loss = 1.03082794\n",
      "Iteration 72, loss = 1.02916792\n",
      "Iteration 73, loss = 1.02893681\n",
      "Iteration 74, loss = 1.02986636\n",
      "Iteration 75, loss = 1.02860047\n",
      "Iteration 76, loss = 1.02706130\n",
      "Iteration 77, loss = 1.02766352\n",
      "Iteration 78, loss = 1.02639107\n",
      "Iteration 79, loss = 1.02532381\n",
      "Iteration 80, loss = 1.02560888\n",
      "Iteration 81, loss = 1.02465133\n",
      "Iteration 82, loss = 1.02395440\n",
      "Iteration 83, loss = 1.02329038\n",
      "Iteration 84, loss = 1.02286031\n",
      "Iteration 85, loss = 1.02400440\n",
      "Iteration 86, loss = 1.02331843\n",
      "Iteration 87, loss = 1.02158072\n",
      "Iteration 88, loss = 1.02205090\n",
      "Iteration 89, loss = 1.02146950\n",
      "Iteration 90, loss = 1.01967744\n",
      "Iteration 91, loss = 1.01990849\n",
      "Iteration 92, loss = 1.01866663\n",
      "Iteration 93, loss = 1.01951398\n",
      "Iteration 94, loss = 1.01869812\n",
      "Iteration 95, loss = 1.01722840\n",
      "Iteration 96, loss = 1.01887218\n",
      "Iteration 97, loss = 1.01675741\n",
      "Iteration 98, loss = 1.01806833\n",
      "Iteration 99, loss = 1.01736006\n",
      "Iteration 100, loss = 1.01563705\n",
      "Iteration 101, loss = 1.01494749\n",
      "Iteration 102, loss = 1.01564710\n",
      "Iteration 103, loss = 1.01595165\n",
      "Iteration 104, loss = 1.01363664\n",
      "Iteration 105, loss = 1.01470484\n",
      "Iteration 106, loss = 1.01314915\n",
      "Iteration 107, loss = 1.01370339\n",
      "Iteration 108, loss = 1.01265056\n",
      "Iteration 109, loss = 1.01315879\n",
      "Iteration 110, loss = 1.01277487\n",
      "Iteration 111, loss = 1.01143439\n",
      "Iteration 112, loss = 1.01051414\n",
      "Iteration 113, loss = 1.00975316\n",
      "Iteration 114, loss = 1.01060663\n",
      "Iteration 115, loss = 1.01030265\n",
      "Iteration 116, loss = 1.01160700\n",
      "Iteration 117, loss = 1.01183641\n",
      "Iteration 118, loss = 1.00850159\n",
      "Iteration 119, loss = 1.00853115\n",
      "Iteration 120, loss = 1.00792113\n",
      "Iteration 121, loss = 1.00813534\n",
      "Iteration 122, loss = 1.00744647\n",
      "Iteration 123, loss = 1.00701223\n",
      "Iteration 124, loss = 1.00620547\n",
      "Iteration 125, loss = 1.00706780\n",
      "Iteration 126, loss = 1.00450471\n",
      "Iteration 127, loss = 1.00695060\n",
      "Iteration 128, loss = 1.00396093\n",
      "Iteration 129, loss = 1.00437229\n",
      "Iteration 130, loss = 1.00420746\n",
      "Iteration 131, loss = 1.00367291\n",
      "Iteration 132, loss = 1.00357244\n",
      "Iteration 133, loss = 1.00233919\n",
      "Iteration 134, loss = 1.00251431\n",
      "Iteration 135, loss = 1.00257687\n",
      "Iteration 136, loss = 1.00356392\n",
      "Iteration 137, loss = 1.00240183\n",
      "Iteration 138, loss = 1.00083884\n",
      "Iteration 139, loss = 1.00077452\n",
      "Iteration 140, loss = 1.00081866\n",
      "Iteration 141, loss = 1.00050169\n",
      "Iteration 142, loss = 1.00350072\n",
      "Iteration 143, loss = 1.00136978\n",
      "Iteration 144, loss = 1.00036973\n",
      "Iteration 145, loss = 0.99906166\n",
      "Iteration 146, loss = 0.99944640\n",
      "Iteration 147, loss = 0.99850144\n",
      "Iteration 148, loss = 0.99761399\n",
      "Iteration 149, loss = 1.00029544\n",
      "Iteration 150, loss = 0.99742913\n",
      "Iteration 151, loss = 0.99849755\n",
      "Iteration 152, loss = 0.99687306\n",
      "Iteration 153, loss = 0.99659570\n",
      "Iteration 154, loss = 0.99592898\n",
      "Iteration 155, loss = 0.99671702\n",
      "Iteration 156, loss = 0.99685517\n",
      "Iteration 157, loss = 0.99811495\n",
      "Iteration 158, loss = 0.99749641\n",
      "Iteration 159, loss = 0.99425056\n",
      "Iteration 160, loss = 0.99487753\n",
      "Iteration 161, loss = 0.99506657\n",
      "Iteration 162, loss = 0.99464199\n",
      "Iteration 163, loss = 0.99352849\n",
      "Iteration 164, loss = 0.99417632\n",
      "Iteration 165, loss = 0.99322107\n",
      "Iteration 166, loss = 0.99378498\n",
      "Iteration 167, loss = 0.99485565\n",
      "Iteration 168, loss = 0.99297117\n",
      "Iteration 169, loss = 0.99400967\n",
      "Iteration 170, loss = 0.99525933\n",
      "Iteration 171, loss = 0.99217666\n",
      "Iteration 172, loss = 0.99313975\n",
      "Iteration 173, loss = 0.99222639\n",
      "Iteration 174, loss = 0.99195898\n",
      "Iteration 175, loss = 0.99071544\n",
      "Iteration 176, loss = 0.98993059\n",
      "Iteration 177, loss = 0.99308366\n",
      "Iteration 178, loss = 0.99279059\n",
      "Iteration 179, loss = 0.98890625\n",
      "Iteration 180, loss = 0.99064318\n",
      "Iteration 181, loss = 0.98933162\n",
      "Iteration 182, loss = 0.99139219\n",
      "Iteration 183, loss = 0.99027370\n",
      "Iteration 184, loss = 0.98848340\n",
      "Iteration 185, loss = 0.98904458\n",
      "Iteration 186, loss = 0.98794154\n",
      "Iteration 187, loss = 0.98753646\n",
      "Iteration 188, loss = 0.98741864\n",
      "Iteration 189, loss = 0.98852753\n",
      "Iteration 190, loss = 0.98721594\n",
      "Iteration 191, loss = 0.98934538\n",
      "Iteration 192, loss = 0.98759878\n",
      "Iteration 193, loss = 0.98947856\n",
      "Iteration 194, loss = 0.98797532\n",
      "Iteration 195, loss = 0.98735245\n",
      "Iteration 196, loss = 0.98969173\n",
      "Iteration 197, loss = 0.98662631\n",
      "Iteration 198, loss = 0.98668737\n",
      "Iteration 199, loss = 0.99066809\n",
      "Iteration 200, loss = 0.98459728\n",
      "Iteration 201, loss = 0.98737267\n",
      "Iteration 202, loss = 0.98599846\n",
      "Iteration 203, loss = 0.98496756\n",
      "Iteration 204, loss = 0.98679946\n",
      "Iteration 205, loss = 0.98496066\n",
      "Iteration 206, loss = 0.98398628\n",
      "Iteration 207, loss = 0.98485449\n",
      "Iteration 208, loss = 0.98550536\n",
      "Iteration 209, loss = 0.98692717\n",
      "Iteration 210, loss = 0.98851515\n",
      "Iteration 211, loss = 0.98310058\n",
      "Iteration 212, loss = 0.98336636\n",
      "Iteration 213, loss = 0.98363426\n",
      "Iteration 214, loss = 0.98288654\n",
      "Iteration 215, loss = 0.98264372\n",
      "Iteration 216, loss = 0.98334569\n",
      "Iteration 217, loss = 0.98343261\n",
      "Iteration 218, loss = 0.98295911\n",
      "Iteration 219, loss = 0.98180570\n",
      "Iteration 220, loss = 0.98192700\n",
      "Iteration 221, loss = 0.98152083\n",
      "Iteration 222, loss = 0.98072311\n",
      "Iteration 223, loss = 0.98132516\n",
      "Iteration 224, loss = 0.98175251\n",
      "Iteration 225, loss = 0.98339807\n",
      "Iteration 226, loss = 0.97940246\n",
      "Iteration 227, loss = 0.98054580\n",
      "Iteration 228, loss = 0.98096346\n",
      "Iteration 229, loss = 0.98151280\n",
      "Iteration 230, loss = 0.97976940\n",
      "Iteration 231, loss = 0.98140548\n",
      "Iteration 232, loss = 0.97995687\n",
      "Iteration 233, loss = 0.97926913\n",
      "Iteration 234, loss = 0.97976048\n",
      "Iteration 235, loss = 0.97855776\n",
      "Iteration 236, loss = 0.97989607\n",
      "Iteration 237, loss = 0.97810442\n",
      "Iteration 238, loss = 0.97825101\n",
      "Iteration 239, loss = 0.97762697\n",
      "Iteration 240, loss = 0.97826234\n",
      "Iteration 241, loss = 0.97880491\n",
      "Iteration 242, loss = 0.97723982\n",
      "Iteration 243, loss = 0.97681036\n",
      "Iteration 244, loss = 0.97838967\n",
      "Iteration 245, loss = 0.97645768\n",
      "Iteration 246, loss = 0.97865787\n",
      "Iteration 247, loss = 0.98073631\n",
      "Iteration 248, loss = 0.97855400\n",
      "Iteration 249, loss = 0.97787413\n",
      "Iteration 250, loss = 0.98125732\n",
      "Iteration 251, loss = 0.97709470\n",
      "Iteration 252, loss = 0.97601400\n",
      "Iteration 253, loss = 0.97746974\n",
      "Iteration 254, loss = 0.97579908\n",
      "Iteration 255, loss = 0.97676246\n",
      "Iteration 256, loss = 0.97807127\n",
      "Iteration 257, loss = 0.97554136\n",
      "Iteration 258, loss = 0.97739354\n",
      "Iteration 259, loss = 0.97776198\n",
      "Iteration 260, loss = 0.97457210\n",
      "Iteration 261, loss = 0.97592760\n",
      "Iteration 262, loss = 0.97549850\n",
      "Iteration 263, loss = 0.97523606\n",
      "Iteration 264, loss = 0.97370884\n",
      "Iteration 265, loss = 0.97393493\n",
      "Iteration 266, loss = 0.97353972\n",
      "Iteration 267, loss = 0.97402026\n",
      "Iteration 268, loss = 0.97397589\n",
      "Iteration 269, loss = 0.97552085\n",
      "Iteration 270, loss = 0.97723122\n",
      "Iteration 271, loss = 0.97488661\n",
      "Iteration 272, loss = 0.97286887\n",
      "Iteration 273, loss = 0.97279740\n",
      "Iteration 274, loss = 0.97274291\n",
      "Iteration 275, loss = 0.97430881\n",
      "Iteration 276, loss = 0.97458056\n",
      "Iteration 277, loss = 0.97318830\n",
      "Iteration 278, loss = 0.97374270\n",
      "Iteration 279, loss = 0.97316308\n",
      "Iteration 280, loss = 0.97400977\n",
      "Iteration 281, loss = 0.97281980\n",
      "Iteration 282, loss = 0.97212426\n",
      "Iteration 283, loss = 0.97234618\n",
      "Iteration 284, loss = 0.97298906\n",
      "Iteration 285, loss = 0.97194910\n",
      "Iteration 286, loss = 0.97153809\n",
      "Iteration 287, loss = 0.97081690\n",
      "Iteration 288, loss = 0.97085580\n",
      "Iteration 289, loss = 0.97294877\n",
      "Iteration 290, loss = 0.97092100\n",
      "Iteration 291, loss = 0.97249359\n",
      "Iteration 292, loss = 0.97036024\n",
      "Iteration 293, loss = 0.97333822\n",
      "Iteration 294, loss = 0.97101884\n",
      "Iteration 295, loss = 0.97063194\n",
      "Iteration 296, loss = 0.97037611\n",
      "Iteration 297, loss = 0.97240662\n",
      "Iteration 298, loss = 0.96884025\n",
      "Iteration 299, loss = 0.97072841\n",
      "Iteration 300, loss = 0.97101086\n",
      "Iteration 1, loss = 1.09595578\n",
      "Iteration 2, loss = 1.07594188\n",
      "Iteration 3, loss = 1.07057186\n",
      "Iteration 4, loss = 1.06885064\n",
      "Iteration 5, loss = 1.06753553\n",
      "Iteration 6, loss = 1.06630917\n",
      "Iteration 7, loss = 1.06589827\n",
      "Iteration 8, loss = 1.06535746\n",
      "Iteration 9, loss = 1.06515469\n",
      "Iteration 10, loss = 1.06506628\n",
      "Iteration 11, loss = 1.06439443\n",
      "Iteration 12, loss = 1.06403525\n",
      "Iteration 13, loss = 1.06356241\n",
      "Iteration 14, loss = 1.06229156\n",
      "Iteration 15, loss = 1.06177199\n",
      "Iteration 16, loss = 1.06052016\n",
      "Iteration 17, loss = 1.05979932\n",
      "Iteration 18, loss = 1.05884365\n",
      "Iteration 19, loss = 1.05801057\n",
      "Iteration 20, loss = 1.05718296\n",
      "Iteration 21, loss = 1.05597346\n",
      "Iteration 22, loss = 1.05489980\n",
      "Iteration 23, loss = 1.05378203\n",
      "Iteration 24, loss = 1.05240529\n",
      "Iteration 25, loss = 1.05111464\n",
      "Iteration 26, loss = 1.05001670\n",
      "Iteration 27, loss = 1.04881576\n",
      "Iteration 28, loss = 1.04760645\n",
      "Iteration 29, loss = 1.04569724\n",
      "Iteration 30, loss = 1.04398080\n",
      "Iteration 31, loss = 1.04291330\n",
      "Iteration 32, loss = 1.04098188\n",
      "Iteration 33, loss = 1.03936530\n",
      "Iteration 34, loss = 1.03768782\n",
      "Iteration 35, loss = 1.03612866\n",
      "Iteration 36, loss = 1.03477391\n",
      "Iteration 37, loss = 1.03305270\n",
      "Iteration 38, loss = 1.03100158\n",
      "Iteration 39, loss = 1.02914353\n",
      "Iteration 40, loss = 1.02758866\n",
      "Iteration 41, loss = 1.02560934\n",
      "Iteration 42, loss = 1.02411120\n",
      "Iteration 43, loss = 1.02148367\n",
      "Iteration 44, loss = 1.01983703\n",
      "Iteration 45, loss = 1.01855192\n",
      "Iteration 46, loss = 1.01688211\n",
      "Iteration 47, loss = 1.01546192\n",
      "Iteration 48, loss = 1.01429218\n",
      "Iteration 49, loss = 1.01250431\n",
      "Iteration 50, loss = 1.00998717\n",
      "Iteration 51, loss = 1.00977777\n",
      "Iteration 52, loss = 1.00818862\n",
      "Iteration 53, loss = 1.00538524\n",
      "Iteration 54, loss = 1.00351510\n",
      "Iteration 55, loss = 1.00384555\n",
      "Iteration 56, loss = 1.00078073\n",
      "Iteration 57, loss = 0.99928502\n",
      "Iteration 58, loss = 0.99781127\n",
      "Iteration 59, loss = 0.99844667\n",
      "Iteration 60, loss = 0.99707840\n",
      "Iteration 61, loss = 0.99320004\n",
      "Iteration 62, loss = 0.99426556\n",
      "Iteration 63, loss = 0.99159053\n",
      "Iteration 64, loss = 0.99170685\n",
      "Iteration 65, loss = 0.98955744\n",
      "Iteration 66, loss = 0.98901736\n",
      "Iteration 67, loss = 0.98783609\n",
      "Iteration 68, loss = 0.98747445\n",
      "Iteration 69, loss = 0.98679940\n",
      "Iteration 70, loss = 0.98505553\n",
      "Iteration 71, loss = 0.98460958\n",
      "Iteration 72, loss = 0.98518171\n",
      "Iteration 73, loss = 0.98300224\n",
      "Iteration 74, loss = 0.98082069\n",
      "Iteration 75, loss = 0.98085911\n",
      "Iteration 76, loss = 0.97959114\n",
      "Iteration 77, loss = 0.98006295\n",
      "Iteration 78, loss = 0.97929221\n",
      "Iteration 79, loss = 0.97634938\n",
      "Iteration 80, loss = 0.97637268\n",
      "Iteration 81, loss = 0.97870555\n",
      "Iteration 82, loss = 0.97521820\n",
      "Iteration 83, loss = 0.97592956\n",
      "Iteration 84, loss = 0.97477472\n",
      "Iteration 85, loss = 0.97653110\n",
      "Iteration 86, loss = 0.97215643\n",
      "Iteration 87, loss = 0.97317753\n",
      "Iteration 88, loss = 0.97036893\n",
      "Iteration 89, loss = 0.97234322\n",
      "Iteration 90, loss = 0.97192180\n",
      "Iteration 91, loss = 0.97094135\n",
      "Iteration 92, loss = 0.97033635\n",
      "Iteration 93, loss = 0.96715406\n",
      "Iteration 94, loss = 0.96706756\n",
      "Iteration 95, loss = 0.96720941\n",
      "Iteration 96, loss = 0.96547520\n",
      "Iteration 97, loss = 0.96565252\n",
      "Iteration 98, loss = 0.96549882\n",
      "Iteration 99, loss = 0.96386409\n",
      "Iteration 100, loss = 0.96769014\n",
      "Iteration 101, loss = 0.96566649\n",
      "Iteration 102, loss = 0.96343580\n",
      "Iteration 103, loss = 0.96273407\n",
      "Iteration 104, loss = 0.96605763\n",
      "Iteration 105, loss = 0.96578758\n",
      "Iteration 106, loss = 0.96064696\n",
      "Iteration 107, loss = 0.96124540\n",
      "Iteration 108, loss = 0.96182669\n",
      "Iteration 109, loss = 0.96043692\n",
      "Iteration 110, loss = 0.95913816\n",
      "Iteration 111, loss = 0.95977783\n",
      "Iteration 112, loss = 0.95967500\n",
      "Iteration 113, loss = 0.95746753\n",
      "Iteration 114, loss = 0.95745475\n",
      "Iteration 115, loss = 0.96003283\n",
      "Iteration 116, loss = 0.95699116\n",
      "Iteration 117, loss = 0.95566031\n",
      "Iteration 118, loss = 0.95795590\n",
      "Iteration 119, loss = 0.95716931\n",
      "Iteration 120, loss = 0.95509019\n",
      "Iteration 121, loss = 0.95291417\n",
      "Iteration 122, loss = 0.95420821\n",
      "Iteration 123, loss = 0.95268360\n",
      "Iteration 124, loss = 0.95253569\n",
      "Iteration 125, loss = 0.95147919\n",
      "Iteration 126, loss = 0.95222093\n",
      "Iteration 127, loss = 0.95303971\n",
      "Iteration 128, loss = 0.95218640\n",
      "Iteration 129, loss = 0.95223159\n",
      "Iteration 130, loss = 0.95126643\n",
      "Iteration 131, loss = 0.95098005\n",
      "Iteration 132, loss = 0.95044525\n",
      "Iteration 133, loss = 0.95023449\n",
      "Iteration 134, loss = 0.95069746\n",
      "Iteration 135, loss = 0.95105454\n",
      "Iteration 136, loss = 0.95001368\n",
      "Iteration 137, loss = 0.94913602\n",
      "Iteration 138, loss = 0.95107220\n",
      "Iteration 139, loss = 0.95089772\n",
      "Iteration 140, loss = 0.94977126\n",
      "Iteration 141, loss = 0.94935999\n",
      "Iteration 142, loss = 0.94746542\n",
      "Iteration 143, loss = 0.94934141\n",
      "Iteration 144, loss = 0.94908468\n",
      "Iteration 145, loss = 0.94611702\n",
      "Iteration 146, loss = 0.94753178\n",
      "Iteration 147, loss = 0.94750236\n",
      "Iteration 148, loss = 0.94502280\n",
      "Iteration 149, loss = 0.94684650\n",
      "Iteration 150, loss = 0.94369003\n",
      "Iteration 151, loss = 0.94446243\n",
      "Iteration 152, loss = 0.94436537\n",
      "Iteration 153, loss = 0.94475778\n",
      "Iteration 154, loss = 0.94632124\n",
      "Iteration 155, loss = 0.94376087\n",
      "Iteration 156, loss = 0.94271236\n",
      "Iteration 157, loss = 0.94253088\n",
      "Iteration 158, loss = 0.94350367\n",
      "Iteration 159, loss = 0.94186527\n",
      "Iteration 160, loss = 0.94164098\n",
      "Iteration 161, loss = 0.94449259\n",
      "Iteration 162, loss = 0.93921353\n",
      "Iteration 163, loss = 0.94433638\n",
      "Iteration 164, loss = 0.94407257\n",
      "Iteration 165, loss = 0.94116073\n",
      "Iteration 166, loss = 0.94108079\n",
      "Iteration 167, loss = 0.93902995\n",
      "Iteration 168, loss = 0.94353963\n",
      "Iteration 169, loss = 0.93998452\n",
      "Iteration 170, loss = 0.93887155\n",
      "Iteration 171, loss = 0.94116499\n",
      "Iteration 172, loss = 0.93893020\n",
      "Iteration 173, loss = 0.94024895\n",
      "Iteration 174, loss = 0.94084983\n",
      "Iteration 175, loss = 0.93955461\n",
      "Iteration 176, loss = 0.93876539\n",
      "Iteration 177, loss = 0.93912567\n",
      "Iteration 178, loss = 0.93657221\n",
      "Iteration 179, loss = 0.94070396\n",
      "Iteration 180, loss = 0.94043930\n",
      "Iteration 181, loss = 0.93957448\n",
      "Iteration 182, loss = 0.93747051\n",
      "Iteration 183, loss = 0.93563101\n",
      "Iteration 184, loss = 0.93688191\n",
      "Iteration 185, loss = 0.93554185\n",
      "Iteration 186, loss = 0.93585539\n",
      "Iteration 187, loss = 0.93689550\n",
      "Iteration 188, loss = 0.93865037\n",
      "Iteration 189, loss = 0.93857986\n",
      "Iteration 190, loss = 0.93666113\n",
      "Iteration 191, loss = 0.93669573\n",
      "Iteration 192, loss = 0.93731248\n",
      "Iteration 193, loss = 0.93610187\n",
      "Iteration 194, loss = 0.93287598\n",
      "Iteration 195, loss = 0.93260849\n",
      "Iteration 196, loss = 0.93591402\n",
      "Iteration 197, loss = 0.93455492\n",
      "Iteration 198, loss = 0.93854701\n",
      "Iteration 199, loss = 0.93631830\n",
      "Iteration 200, loss = 0.93411000\n",
      "Iteration 201, loss = 0.93208320\n",
      "Iteration 202, loss = 0.93290826\n",
      "Iteration 203, loss = 0.93350639\n",
      "Iteration 204, loss = 0.93156687\n",
      "Iteration 205, loss = 0.93109467\n",
      "Iteration 206, loss = 0.93248583\n",
      "Iteration 207, loss = 0.93724148\n",
      "Iteration 208, loss = 0.93438983\n",
      "Iteration 209, loss = 0.93850111\n",
      "Iteration 210, loss = 0.93347973\n",
      "Iteration 211, loss = 0.93245031\n",
      "Iteration 212, loss = 0.93288515\n",
      "Iteration 213, loss = 0.93251395\n",
      "Iteration 214, loss = 0.93471739\n",
      "Iteration 215, loss = 0.93212536\n",
      "Iteration 216, loss = 0.92918304\n",
      "Iteration 217, loss = 0.93428467\n",
      "Iteration 218, loss = 0.93099805\n",
      "Iteration 219, loss = 0.92895573\n",
      "Iteration 220, loss = 0.92817690\n",
      "Iteration 221, loss = 0.93605975\n",
      "Iteration 222, loss = 0.92995627\n",
      "Iteration 223, loss = 0.93467323\n",
      "Iteration 224, loss = 0.93125654\n",
      "Iteration 225, loss = 0.92776388\n",
      "Iteration 226, loss = 0.92981794\n",
      "Iteration 227, loss = 0.92864845\n",
      "Iteration 228, loss = 0.92776189\n",
      "Iteration 229, loss = 0.92722061\n",
      "Iteration 230, loss = 0.92707268\n",
      "Iteration 231, loss = 0.92894558\n",
      "Iteration 232, loss = 0.93139334\n",
      "Iteration 233, loss = 0.92818858\n",
      "Iteration 234, loss = 0.92773379\n",
      "Iteration 235, loss = 0.93009206\n",
      "Iteration 236, loss = 0.92537253\n",
      "Iteration 237, loss = 0.92610125\n",
      "Iteration 238, loss = 0.92433429\n",
      "Iteration 239, loss = 0.92541729\n",
      "Iteration 240, loss = 0.92514215\n",
      "Iteration 241, loss = 0.92444364\n",
      "Iteration 242, loss = 0.92579104\n",
      "Iteration 243, loss = 0.92470794\n",
      "Iteration 244, loss = 0.92395535\n",
      "Iteration 245, loss = 0.92764846\n",
      "Iteration 246, loss = 0.92561017\n",
      "Iteration 247, loss = 0.92370075\n",
      "Iteration 248, loss = 0.92436853\n",
      "Iteration 249, loss = 0.92249039\n",
      "Iteration 250, loss = 0.92306578\n",
      "Iteration 251, loss = 0.92417567\n",
      "Iteration 252, loss = 0.92578040\n",
      "Iteration 253, loss = 0.92507666\n",
      "Iteration 254, loss = 0.92257434\n",
      "Iteration 255, loss = 0.91930790\n",
      "Iteration 256, loss = 0.92338642\n",
      "Iteration 257, loss = 0.92186417\n",
      "Iteration 258, loss = 0.92367310\n",
      "Iteration 259, loss = 0.91981870\n",
      "Iteration 260, loss = 0.92169362\n",
      "Iteration 261, loss = 0.92272787\n",
      "Iteration 262, loss = 0.92242396\n",
      "Iteration 263, loss = 0.92619451\n",
      "Iteration 264, loss = 0.92161684\n",
      "Iteration 265, loss = 0.91769630\n",
      "Iteration 266, loss = 0.91918973\n",
      "Iteration 267, loss = 0.92025303\n",
      "Iteration 268, loss = 0.92060635\n",
      "Iteration 269, loss = 0.91643773\n",
      "Iteration 270, loss = 0.91751303\n",
      "Iteration 271, loss = 0.91704106\n",
      "Iteration 272, loss = 0.91903557\n",
      "Iteration 273, loss = 0.92022281\n",
      "Iteration 274, loss = 0.91587793\n",
      "Iteration 275, loss = 0.91625300\n",
      "Iteration 276, loss = 0.91519641\n",
      "Iteration 277, loss = 0.91568634\n",
      "Iteration 278, loss = 0.91558204\n",
      "Iteration 279, loss = 0.92070577\n",
      "Iteration 280, loss = 0.91588601\n",
      "Iteration 281, loss = 0.91604799\n",
      "Iteration 282, loss = 0.91590598\n",
      "Iteration 283, loss = 0.91587749\n",
      "Iteration 284, loss = 0.92051281\n",
      "Iteration 285, loss = 0.91470132\n",
      "Iteration 286, loss = 0.91281062\n",
      "Iteration 287, loss = 0.91305464\n",
      "Iteration 288, loss = 0.91682054\n",
      "Iteration 289, loss = 0.91230554\n",
      "Iteration 290, loss = 0.91094352\n",
      "Iteration 291, loss = 0.91352305\n",
      "Iteration 292, loss = 0.91025481\n",
      "Iteration 293, loss = 0.91450787\n",
      "Iteration 294, loss = 0.91421255\n",
      "Iteration 295, loss = 0.90975568\n",
      "Iteration 296, loss = 0.91013170\n",
      "Iteration 297, loss = 0.91039284\n",
      "Iteration 298, loss = 0.90922950\n",
      "Iteration 299, loss = 0.91294151\n",
      "Iteration 300, loss = 0.90986822\n",
      "Iteration 1, loss = 1.06940514\n",
      "Iteration 2, loss = 1.06724587\n",
      "Iteration 3, loss = 1.06597632\n",
      "Iteration 4, loss = 1.06523931\n",
      "Iteration 5, loss = 1.06309315\n",
      "Iteration 6, loss = 1.06137221\n",
      "Iteration 7, loss = 1.05898948\n",
      "Iteration 8, loss = 1.05801344\n",
      "Iteration 9, loss = 1.05580576\n",
      "Iteration 10, loss = 1.05498226\n",
      "Iteration 11, loss = 1.05394220\n",
      "Iteration 12, loss = 1.05287753\n",
      "Iteration 13, loss = 1.05186723\n",
      "Iteration 14, loss = 1.05197373\n",
      "Iteration 15, loss = 1.05004955\n",
      "Iteration 16, loss = 1.04993458\n",
      "Iteration 17, loss = 1.04875212\n",
      "Iteration 18, loss = 1.04760373\n",
      "Iteration 19, loss = 1.04731019\n",
      "Iteration 20, loss = 1.04722000\n",
      "Iteration 21, loss = 1.04470238\n",
      "Iteration 22, loss = 1.04339307\n",
      "Iteration 23, loss = 1.04313363\n",
      "Iteration 24, loss = 1.04106806\n",
      "Iteration 25, loss = 1.04074632\n",
      "Iteration 26, loss = 1.03984145\n",
      "Iteration 27, loss = 1.03856899\n",
      "Iteration 28, loss = 1.03715310\n",
      "Iteration 29, loss = 1.03727202\n",
      "Iteration 30, loss = 1.03674772\n",
      "Iteration 31, loss = 1.03357812\n",
      "Iteration 32, loss = 1.03217794\n",
      "Iteration 33, loss = 1.03077728\n",
      "Iteration 34, loss = 1.03090700\n",
      "Iteration 35, loss = 1.03016048\n",
      "Iteration 36, loss = 1.02801598\n",
      "Iteration 37, loss = 1.02646913\n",
      "Iteration 38, loss = 1.02527756\n",
      "Iteration 39, loss = 1.02351621\n",
      "Iteration 40, loss = 1.02352945\n",
      "Iteration 41, loss = 1.02116164\n",
      "Iteration 42, loss = 1.02026158\n",
      "Iteration 43, loss = 1.01911206\n",
      "Iteration 44, loss = 1.01774204\n",
      "Iteration 45, loss = 1.01611553\n",
      "Iteration 46, loss = 1.01588039\n",
      "Iteration 47, loss = 1.01512937\n",
      "Iteration 48, loss = 1.01251419\n",
      "Iteration 49, loss = 1.01130794\n",
      "Iteration 50, loss = 1.01138611\n",
      "Iteration 51, loss = 1.00981918\n",
      "Iteration 52, loss = 1.00779122\n",
      "Iteration 53, loss = 1.00678540\n",
      "Iteration 54, loss = 1.00538597\n",
      "Iteration 55, loss = 1.00541082\n",
      "Iteration 56, loss = 1.00340543\n",
      "Iteration 57, loss = 1.00324462\n",
      "Iteration 58, loss = 1.00208749\n",
      "Iteration 59, loss = 1.00311431\n",
      "Iteration 60, loss = 0.99940716\n",
      "Iteration 61, loss = 0.99882084\n",
      "Iteration 62, loss = 0.99981915\n",
      "Iteration 63, loss = 0.99697350\n",
      "Iteration 64, loss = 0.99736877\n",
      "Iteration 65, loss = 0.99590710\n",
      "Iteration 66, loss = 0.99512385\n",
      "Iteration 67, loss = 0.99258925\n",
      "Iteration 68, loss = 0.99395603\n",
      "Iteration 69, loss = 0.99275647\n",
      "Iteration 70, loss = 0.99263454\n",
      "Iteration 71, loss = 0.99109535\n",
      "Iteration 72, loss = 0.98981192\n",
      "Iteration 73, loss = 0.98825960\n",
      "Iteration 74, loss = 0.99008702\n",
      "Iteration 75, loss = 0.98944189\n",
      "Iteration 76, loss = 0.98925878\n",
      "Iteration 77, loss = 0.98799846\n",
      "Iteration 78, loss = 0.98569317\n",
      "Iteration 79, loss = 0.98631589\n",
      "Iteration 80, loss = 0.98506921\n",
      "Iteration 81, loss = 0.98519863\n",
      "Iteration 82, loss = 0.98255677\n",
      "Iteration 83, loss = 0.98228699\n",
      "Iteration 84, loss = 0.98181578\n",
      "Iteration 85, loss = 0.98636143\n",
      "Iteration 86, loss = 0.98171856\n",
      "Iteration 87, loss = 0.98028238\n",
      "Iteration 88, loss = 0.98439823\n",
      "Iteration 89, loss = 0.98154046\n",
      "Iteration 90, loss = 0.98283098\n",
      "Iteration 91, loss = 0.98045464\n",
      "Iteration 92, loss = 0.98011970\n",
      "Iteration 93, loss = 0.97824513\n",
      "Iteration 94, loss = 0.97851402\n",
      "Iteration 95, loss = 0.97687994\n",
      "Iteration 96, loss = 0.97643286\n",
      "Iteration 97, loss = 0.97857311\n",
      "Iteration 98, loss = 0.97974722\n",
      "Iteration 99, loss = 0.97893176\n",
      "Iteration 100, loss = 0.97753616\n",
      "Iteration 101, loss = 0.97626238\n",
      "Iteration 102, loss = 0.97565156\n",
      "Iteration 103, loss = 0.97395626\n",
      "Iteration 104, loss = 0.97483647\n",
      "Iteration 105, loss = 0.97502333\n",
      "Iteration 106, loss = 0.97466939\n",
      "Iteration 107, loss = 0.97338641\n",
      "Iteration 108, loss = 0.97454809\n",
      "Iteration 109, loss = 0.97534252\n",
      "Iteration 110, loss = 0.97140971\n",
      "Iteration 111, loss = 0.97226703\n",
      "Iteration 112, loss = 0.97225842\n",
      "Iteration 113, loss = 0.97185662\n",
      "Iteration 114, loss = 0.97145236\n",
      "Iteration 115, loss = 0.97226126\n",
      "Iteration 116, loss = 0.97028298\n",
      "Iteration 117, loss = 0.97144118\n",
      "Iteration 118, loss = 0.97134593\n",
      "Iteration 119, loss = 0.97140129\n",
      "Iteration 120, loss = 0.97632446\n",
      "Iteration 121, loss = 0.97330217\n",
      "Iteration 122, loss = 0.97158012\n",
      "Iteration 123, loss = 0.97017407\n",
      "Iteration 124, loss = 0.97125082\n",
      "Iteration 125, loss = 0.97335440\n",
      "Iteration 126, loss = 0.96907261\n",
      "Iteration 127, loss = 0.96876515\n",
      "Iteration 128, loss = 0.97053076\n",
      "Iteration 129, loss = 0.97002861\n",
      "Iteration 130, loss = 0.96840218\n",
      "Iteration 131, loss = 0.96805466\n",
      "Iteration 132, loss = 0.96823709\n",
      "Iteration 133, loss = 0.96708684\n",
      "Iteration 134, loss = 0.96850054\n",
      "Iteration 135, loss = 0.96827455\n",
      "Iteration 136, loss = 0.96895822\n",
      "Iteration 137, loss = 0.96686215\n",
      "Iteration 138, loss = 0.96843896\n",
      "Iteration 139, loss = 0.96564915\n",
      "Iteration 140, loss = 0.97003591\n",
      "Iteration 141, loss = 0.96785238\n",
      "Iteration 142, loss = 0.96590827\n",
      "Iteration 143, loss = 0.96638375\n",
      "Iteration 144, loss = 0.96637908\n",
      "Iteration 145, loss = 0.96512046\n",
      "Iteration 146, loss = 0.96541039\n",
      "Iteration 147, loss = 0.96904661\n",
      "Iteration 148, loss = 0.96329877\n",
      "Iteration 149, loss = 0.96435798\n",
      "Iteration 150, loss = 0.96522084\n",
      "Iteration 151, loss = 0.96465145\n",
      "Iteration 152, loss = 0.96434238\n",
      "Iteration 153, loss = 0.96441470\n",
      "Iteration 154, loss = 0.96550502\n",
      "Iteration 155, loss = 0.96506695\n",
      "Iteration 156, loss = 0.96385616\n",
      "Iteration 157, loss = 0.96279890\n",
      "Iteration 158, loss = 0.96833212\n",
      "Iteration 159, loss = 0.96384440\n",
      "Iteration 160, loss = 0.96267038\n",
      "Iteration 161, loss = 0.96605079\n",
      "Iteration 162, loss = 0.96265281\n",
      "Iteration 163, loss = 0.96137093\n",
      "Iteration 164, loss = 0.96164654\n",
      "Iteration 165, loss = 0.96304925\n",
      "Iteration 166, loss = 0.96395138\n",
      "Iteration 167, loss = 0.96465447\n",
      "Iteration 168, loss = 0.96210105\n",
      "Iteration 169, loss = 0.96162891\n",
      "Iteration 170, loss = 0.96352592\n",
      "Iteration 171, loss = 0.96074794\n",
      "Iteration 172, loss = 0.96063612\n",
      "Iteration 173, loss = 0.96294744\n",
      "Iteration 174, loss = 0.96264842\n",
      "Iteration 175, loss = 0.96227217\n",
      "Iteration 176, loss = 0.96051494\n",
      "Iteration 177, loss = 0.96122528\n",
      "Iteration 178, loss = 0.96030264\n",
      "Iteration 179, loss = 0.96471750\n",
      "Iteration 180, loss = 0.96470936\n",
      "Iteration 181, loss = 0.96157486\n",
      "Iteration 182, loss = 0.96243897\n",
      "Iteration 183, loss = 0.96303695\n",
      "Iteration 184, loss = 0.95925460\n",
      "Iteration 185, loss = 0.96095170\n",
      "Iteration 186, loss = 0.96075561\n",
      "Iteration 187, loss = 0.95965999\n",
      "Iteration 188, loss = 0.96100518\n",
      "Iteration 189, loss = 0.96067769\n",
      "Iteration 190, loss = 0.96439744\n",
      "Iteration 191, loss = 0.96082736\n",
      "Iteration 192, loss = 0.95883441\n",
      "Iteration 193, loss = 0.95918543\n",
      "Iteration 194, loss = 0.95918613\n",
      "Iteration 195, loss = 0.95908971\n",
      "Iteration 196, loss = 0.95898194\n",
      "Iteration 197, loss = 0.96440302\n",
      "Iteration 198, loss = 0.95678793\n",
      "Iteration 199, loss = 0.96252229\n",
      "Iteration 200, loss = 0.95886889\n",
      "Iteration 201, loss = 0.96227635\n",
      "Iteration 202, loss = 0.96396832\n",
      "Iteration 203, loss = 0.96090319\n",
      "Iteration 204, loss = 0.95728712\n",
      "Iteration 205, loss = 0.96510776\n",
      "Iteration 206, loss = 0.95754623\n",
      "Iteration 207, loss = 0.95863074\n",
      "Iteration 208, loss = 0.95867819\n",
      "Iteration 209, loss = 0.95724162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14062126\n",
      "Iteration 2, loss = 1.10077162\n",
      "Iteration 3, loss = 1.07313599\n",
      "Iteration 4, loss = 1.06776409\n",
      "Iteration 5, loss = 1.06828890\n",
      "Iteration 6, loss = 1.06822313\n",
      "Iteration 7, loss = 1.06706502\n",
      "Iteration 8, loss = 1.06639983\n",
      "Iteration 9, loss = 1.06642863\n",
      "Iteration 10, loss = 1.06629208\n",
      "Iteration 11, loss = 1.06618595\n",
      "Iteration 12, loss = 1.06602512\n",
      "Iteration 13, loss = 1.06597553\n",
      "Iteration 14, loss = 1.06613680\n",
      "Iteration 15, loss = 1.06545219\n",
      "Iteration 16, loss = 1.06510633\n",
      "Iteration 17, loss = 1.06459355\n",
      "Iteration 18, loss = 1.06383555\n",
      "Iteration 19, loss = 1.06233644\n",
      "Iteration 20, loss = 1.06140320\n",
      "Iteration 21, loss = 1.05949697\n",
      "Iteration 22, loss = 1.05824450\n",
      "Iteration 23, loss = 1.05651666\n",
      "Iteration 24, loss = 1.05568621\n",
      "Iteration 25, loss = 1.05423326\n",
      "Iteration 26, loss = 1.05266756\n",
      "Iteration 27, loss = 1.05159114\n",
      "Iteration 28, loss = 1.05041643\n",
      "Iteration 29, loss = 1.04849556\n",
      "Iteration 30, loss = 1.04784563\n",
      "Iteration 31, loss = 1.04580247\n",
      "Iteration 32, loss = 1.04458455\n",
      "Iteration 33, loss = 1.04287714\n",
      "Iteration 34, loss = 1.04148777\n",
      "Iteration 35, loss = 1.04002628\n",
      "Iteration 36, loss = 1.03796016\n",
      "Iteration 37, loss = 1.03637496\n",
      "Iteration 38, loss = 1.03468781\n",
      "Iteration 39, loss = 1.03319831\n",
      "Iteration 40, loss = 1.03180113\n",
      "Iteration 41, loss = 1.02969417\n",
      "Iteration 42, loss = 1.02892179\n",
      "Iteration 43, loss = 1.02651015\n",
      "Iteration 44, loss = 1.02476753\n",
      "Iteration 45, loss = 1.02352133\n",
      "Iteration 46, loss = 1.02196945\n",
      "Iteration 47, loss = 1.01985448\n",
      "Iteration 48, loss = 1.01875096\n",
      "Iteration 49, loss = 1.01696672\n",
      "Iteration 50, loss = 1.01584743\n",
      "Iteration 51, loss = 1.01424232\n",
      "Iteration 52, loss = 1.01335885\n",
      "Iteration 53, loss = 1.01097953\n",
      "Iteration 54, loss = 1.00997253\n",
      "Iteration 55, loss = 1.01284689\n",
      "Iteration 56, loss = 1.00781572\n",
      "Iteration 57, loss = 1.00663195\n",
      "Iteration 58, loss = 1.00534495\n",
      "Iteration 59, loss = 1.00390155\n",
      "Iteration 60, loss = 1.00288935\n",
      "Iteration 61, loss = 1.00254414\n",
      "Iteration 62, loss = 1.00223771\n",
      "Iteration 63, loss = 1.00048463\n",
      "Iteration 64, loss = 0.99854575\n",
      "Iteration 65, loss = 0.99752539\n",
      "Iteration 66, loss = 0.99758074\n",
      "Iteration 67, loss = 0.99492472\n",
      "Iteration 68, loss = 1.00032823\n",
      "Iteration 69, loss = 0.99458323\n",
      "Iteration 70, loss = 0.99359145\n",
      "Iteration 71, loss = 0.99397136\n",
      "Iteration 72, loss = 0.99200369\n",
      "Iteration 73, loss = 0.99171571\n",
      "Iteration 74, loss = 0.99104158\n",
      "Iteration 75, loss = 0.99209056\n",
      "Iteration 76, loss = 0.98904741\n",
      "Iteration 77, loss = 0.98871329\n",
      "Iteration 78, loss = 0.98929861\n",
      "Iteration 79, loss = 0.98872765\n",
      "Iteration 80, loss = 0.98717802\n",
      "Iteration 81, loss = 0.98712507\n",
      "Iteration 82, loss = 0.99110052\n",
      "Iteration 83, loss = 0.98694980\n",
      "Iteration 84, loss = 0.98457969\n",
      "Iteration 85, loss = 0.98464543\n",
      "Iteration 86, loss = 0.98347828\n",
      "Iteration 87, loss = 0.98372017\n",
      "Iteration 88, loss = 0.98835957\n",
      "Iteration 89, loss = 0.98520081\n",
      "Iteration 90, loss = 0.98321314\n",
      "Iteration 91, loss = 0.98071949\n",
      "Iteration 92, loss = 0.98153526\n",
      "Iteration 93, loss = 0.98051111\n",
      "Iteration 94, loss = 0.98164893\n",
      "Iteration 95, loss = 0.98226544\n",
      "Iteration 96, loss = 0.98045789\n",
      "Iteration 97, loss = 0.98103678\n",
      "Iteration 98, loss = 0.97912210\n",
      "Iteration 99, loss = 0.98036577\n",
      "Iteration 100, loss = 0.98085521\n",
      "Iteration 101, loss = 0.97731492\n",
      "Iteration 102, loss = 0.98051754\n",
      "Iteration 103, loss = 0.97891666\n",
      "Iteration 104, loss = 0.97755967\n",
      "Iteration 105, loss = 0.97748759\n",
      "Iteration 106, loss = 0.97729863\n",
      "Iteration 107, loss = 0.97792894\n",
      "Iteration 108, loss = 0.97671135\n",
      "Iteration 109, loss = 0.97936503\n",
      "Iteration 110, loss = 0.97822504\n",
      "Iteration 111, loss = 0.97626144\n",
      "Iteration 112, loss = 0.97705198\n",
      "Iteration 113, loss = 0.97669604\n",
      "Iteration 114, loss = 0.97462174\n",
      "Iteration 115, loss = 0.97456543\n",
      "Iteration 116, loss = 0.97712579\n",
      "Iteration 117, loss = 0.97592358\n",
      "Iteration 118, loss = 0.97405606\n",
      "Iteration 119, loss = 0.97446812\n",
      "Iteration 120, loss = 0.97297504\n",
      "Iteration 121, loss = 0.97463142\n",
      "Iteration 122, loss = 0.97304790\n",
      "Iteration 123, loss = 0.97170787\n",
      "Iteration 124, loss = 0.97348270\n",
      "Iteration 125, loss = 0.97104220\n",
      "Iteration 126, loss = 0.97510910\n",
      "Iteration 127, loss = 0.96993003\n",
      "Iteration 128, loss = 0.96809567\n",
      "Iteration 129, loss = 0.97298526\n",
      "Iteration 130, loss = 0.97125889\n",
      "Iteration 131, loss = 0.97050014\n",
      "Iteration 132, loss = 0.96981227\n",
      "Iteration 133, loss = 0.96964955\n",
      "Iteration 134, loss = 0.96774355\n",
      "Iteration 135, loss = 0.96854421\n",
      "Iteration 136, loss = 0.96719285\n",
      "Iteration 137, loss = 0.96811350\n",
      "Iteration 138, loss = 0.96782121\n",
      "Iteration 139, loss = 0.96794425\n",
      "Iteration 140, loss = 0.96551112\n",
      "Iteration 141, loss = 0.96777274\n",
      "Iteration 142, loss = 0.96627248\n",
      "Iteration 143, loss = 0.96451244\n",
      "Iteration 144, loss = 0.96586446\n",
      "Iteration 145, loss = 0.96483199\n",
      "Iteration 146, loss = 0.96193913\n",
      "Iteration 147, loss = 0.96902437\n",
      "Iteration 148, loss = 0.96334215\n",
      "Iteration 149, loss = 0.96456034\n",
      "Iteration 150, loss = 0.96290619\n",
      "Iteration 151, loss = 0.96440258\n",
      "Iteration 152, loss = 0.96515226\n",
      "Iteration 153, loss = 0.96206785\n",
      "Iteration 154, loss = 0.96163392\n",
      "Iteration 155, loss = 0.96079533\n",
      "Iteration 156, loss = 0.96394744\n",
      "Iteration 157, loss = 0.96067983\n",
      "Iteration 158, loss = 0.96172864\n",
      "Iteration 159, loss = 0.95968919\n",
      "Iteration 160, loss = 0.96022385\n",
      "Iteration 161, loss = 0.96238221\n",
      "Iteration 162, loss = 0.96244722\n",
      "Iteration 163, loss = 0.95937537\n",
      "Iteration 164, loss = 0.95961105\n",
      "Iteration 165, loss = 0.95846966\n",
      "Iteration 166, loss = 0.95882086\n",
      "Iteration 167, loss = 0.95669632\n",
      "Iteration 168, loss = 0.95685749\n",
      "Iteration 169, loss = 0.95605868\n",
      "Iteration 170, loss = 0.95624873\n",
      "Iteration 171, loss = 0.95617073\n",
      "Iteration 172, loss = 0.95809806\n",
      "Iteration 173, loss = 0.95914041\n",
      "Iteration 174, loss = 0.95531187\n",
      "Iteration 175, loss = 0.95662025\n",
      "Iteration 176, loss = 0.95492108\n",
      "Iteration 177, loss = 0.95510348\n",
      "Iteration 178, loss = 0.95497940\n",
      "Iteration 179, loss = 0.95402483\n",
      "Iteration 180, loss = 0.95319876\n",
      "Iteration 181, loss = 0.95364657\n",
      "Iteration 182, loss = 0.95621265\n",
      "Iteration 183, loss = 0.95420231\n",
      "Iteration 184, loss = 0.95690766\n",
      "Iteration 185, loss = 0.95265910\n",
      "Iteration 186, loss = 0.95672054\n",
      "Iteration 187, loss = 0.95571649\n",
      "Iteration 188, loss = 0.95469663\n",
      "Iteration 189, loss = 0.95019816\n",
      "Iteration 190, loss = 0.95086947\n",
      "Iteration 191, loss = 0.95383834\n",
      "Iteration 192, loss = 0.95043843\n",
      "Iteration 193, loss = 0.95122020\n",
      "Iteration 194, loss = 0.94997809\n",
      "Iteration 195, loss = 0.94778293\n",
      "Iteration 196, loss = 0.94864828\n",
      "Iteration 197, loss = 0.94909137\n",
      "Iteration 198, loss = 0.94719611\n",
      "Iteration 199, loss = 0.95086464\n",
      "Iteration 200, loss = 0.94720593\n",
      "Iteration 201, loss = 0.94864276\n",
      "Iteration 202, loss = 0.94788097\n",
      "Iteration 203, loss = 0.94700509\n",
      "Iteration 204, loss = 0.94740124\n",
      "Iteration 205, loss = 0.94842433\n",
      "Iteration 206, loss = 0.94689023\n",
      "Iteration 207, loss = 0.94711980\n",
      "Iteration 208, loss = 0.94768305\n",
      "Iteration 209, loss = 0.94565721\n",
      "Iteration 210, loss = 0.94460832\n",
      "Iteration 211, loss = 0.94676934\n",
      "Iteration 212, loss = 0.94629870\n",
      "Iteration 213, loss = 0.94680217\n",
      "Iteration 214, loss = 0.94331313\n",
      "Iteration 215, loss = 0.94396250\n",
      "Iteration 216, loss = 0.94350813\n",
      "Iteration 217, loss = 0.94625466\n",
      "Iteration 218, loss = 0.94410131\n",
      "Iteration 219, loss = 0.94481082\n",
      "Iteration 220, loss = 0.94540986\n",
      "Iteration 221, loss = 0.94189472\n",
      "Iteration 222, loss = 0.94392498\n",
      "Iteration 223, loss = 0.94191563\n",
      "Iteration 224, loss = 0.94592193\n",
      "Iteration 225, loss = 0.94346479\n",
      "Iteration 226, loss = 0.94021443\n",
      "Iteration 227, loss = 0.94179801\n",
      "Iteration 228, loss = 0.94146960\n",
      "Iteration 229, loss = 0.94499599\n",
      "Iteration 230, loss = 0.94166210\n",
      "Iteration 231, loss = 0.94318522\n",
      "Iteration 232, loss = 0.93913316\n",
      "Iteration 233, loss = 0.94267709\n",
      "Iteration 234, loss = 0.94276902\n",
      "Iteration 235, loss = 0.93902783\n",
      "Iteration 236, loss = 0.93927486\n",
      "Iteration 237, loss = 0.94318912\n",
      "Iteration 238, loss = 0.94059600\n",
      "Iteration 239, loss = 0.94153516\n",
      "Iteration 240, loss = 0.94350203\n",
      "Iteration 241, loss = 0.93938895\n",
      "Iteration 242, loss = 0.93827355\n",
      "Iteration 243, loss = 0.94081806\n",
      "Iteration 244, loss = 0.93839105\n",
      "Iteration 245, loss = 0.94338125\n",
      "Iteration 246, loss = 0.93831663\n",
      "Iteration 247, loss = 0.93936149\n",
      "Iteration 248, loss = 0.93822951\n",
      "Iteration 249, loss = 0.93810954\n",
      "Iteration 250, loss = 0.93944702\n",
      "Iteration 251, loss = 0.93726185\n",
      "Iteration 252, loss = 0.93651041\n",
      "Iteration 253, loss = 0.93921768\n",
      "Iteration 254, loss = 0.93650359\n",
      "Iteration 255, loss = 0.93611246\n",
      "Iteration 256, loss = 0.93807081\n",
      "Iteration 257, loss = 0.93670277\n",
      "Iteration 258, loss = 0.93893177\n",
      "Iteration 259, loss = 0.93653659\n",
      "Iteration 260, loss = 0.93667563\n",
      "Iteration 261, loss = 0.93542081\n",
      "Iteration 262, loss = 0.93885277\n",
      "Iteration 263, loss = 0.93554270\n",
      "Iteration 264, loss = 0.94057945\n",
      "Iteration 265, loss = 0.93443098\n",
      "Iteration 266, loss = 0.93604402\n",
      "Iteration 267, loss = 0.94177662\n",
      "Iteration 268, loss = 0.93922284\n",
      "Iteration 269, loss = 0.93549196\n",
      "Iteration 270, loss = 0.93727575\n",
      "Iteration 271, loss = 0.93466146\n",
      "Iteration 272, loss = 0.93390463\n",
      "Iteration 273, loss = 0.93783437\n",
      "Iteration 274, loss = 0.93563846\n",
      "Iteration 275, loss = 0.93490648\n",
      "Iteration 276, loss = 0.93244426\n",
      "Iteration 277, loss = 0.93265369\n",
      "Iteration 278, loss = 0.93267841\n",
      "Iteration 279, loss = 0.93310069\n",
      "Iteration 280, loss = 0.93257085\n",
      "Iteration 281, loss = 0.93586709\n",
      "Iteration 282, loss = 0.94015226\n",
      "Iteration 283, loss = 0.93555807\n",
      "Iteration 284, loss = 0.93898614\n",
      "Iteration 285, loss = 0.93071490\n",
      "Iteration 286, loss = 0.93087660\n",
      "Iteration 287, loss = 0.93686847\n",
      "Iteration 288, loss = 0.93330952\n",
      "Iteration 289, loss = 0.93482415\n",
      "Iteration 290, loss = 0.93656653\n",
      "Iteration 291, loss = 0.93906611\n",
      "Iteration 292, loss = 0.93219901\n",
      "Iteration 293, loss = 0.93534286\n",
      "Iteration 294, loss = 0.93496240\n",
      "Iteration 295, loss = 0.93110985\n",
      "Iteration 296, loss = 0.93369491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(contraceptive_values, contraceptive_classes):\n",
    "    data_train, target_train = contraceptive_values[train], contraceptive_classes[train]\n",
    "    data_test, target_test = contraceptive_values[test], contraceptive_classes[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.59      0.58       629\n",
      "           2       0.35      0.37      0.36       333\n",
      "           3       0.43      0.41      0.42       511\n",
      "\n",
      "    accuracy                           0.48      1473\n",
      "   macro avg       0.45      0.45      0.45      1473\n",
      "weighted avg       0.47      0.48      0.48      1473\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[370  91 168]\n",
      " [104 122 107]\n",
      " [171 131 209]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.59      0.56       629\n",
      "           2       0.35      0.33      0.34       333\n",
      "           3       0.44      0.41      0.42       511\n",
      "\n",
      "    accuracy                           0.47      1473\n",
      "   macro avg       0.44      0.44      0.44      1473\n",
      "weighted avg       0.46      0.47      0.46      1473\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[370  98 161]\n",
      " [116 109 108]\n",
      " [200 101 210]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.40      0.48       629\n",
      "           2       0.36      0.63      0.46       333\n",
      "           3       0.47      0.46      0.47       511\n",
      "\n",
      "    accuracy                           0.47      1473\n",
      "   macro avg       0.49      0.50      0.47      1473\n",
      "weighted avg       0.51      0.47      0.47      1473\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[249 195 185]\n",
      " [ 47 210  76]\n",
      " [102 173 236]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.64      0.60       629\n",
      "           2       0.46      0.32      0.38       333\n",
      "           3       0.46      0.48      0.47       511\n",
      "\n",
      "    accuracy                           0.51      1473\n",
      "   macro avg       0.50      0.48      0.48      1473\n",
      "weighted avg       0.51      0.51      0.51      1473\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[401  52 176]\n",
      " [109 108 116]\n",
      " [191  75 245]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.67      0.62       629\n",
      "           2       0.63      0.07      0.12       333\n",
      "           3       0.43      0.59      0.50       511\n",
      "\n",
      "    accuracy                           0.51      1473\n",
      "   macro avg       0.54      0.44      0.41      1473\n",
      "weighted avg       0.54      0.51      0.46      1473\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[422   4 203]\n",
      " [115  22 196]\n",
      " [200   9 302]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(contraceptive_classes, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusÃ£o: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(contraceptive_classes, predicted_classes[classificador]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}