{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Acute Inflammations:\n"
     ]
    },
    {
     "data": {
      "text/plain": "     temperature  occurrence_of_nausea  lumbar_pain  urine_pushing  \\\n0           35.5                     0            1              0   \n1           35.9                     0            0              1   \n2           35.9                     0            1              0   \n3           36.0                     0            0              1   \n4           36.0                     0            1              0   \n..           ...                   ...          ...            ...   \n115         41.4                     0            1              1   \n116         41.5                     0            0              0   \n117         41.5                     1            1              0   \n118         41.5                     0            1              1   \n119         41.5                     0            1              1   \n\n     micturition_pains  burning_of_urethra  decision_1  decision_2  \n0                    0                   0           0           0  \n1                    1                   1           1           0  \n2                    0                   0           0           0  \n3                    1                   1           1           0  \n4                    0                   0           0           0  \n..                 ...                 ...         ...         ...  \n115                  0                   1           0           1  \n116                  0                   0           0           0  \n117                  1                   0           0           1  \n118                  0                   1           0           1  \n119                  0                   1           0           1  \n\n[120 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temperature</th>\n      <th>occurrence_of_nausea</th>\n      <th>lumbar_pain</th>\n      <th>urine_pushing</th>\n      <th>micturition_pains</th>\n      <th>burning_of_urethra</th>\n      <th>decision_1</th>\n      <th>decision_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>35.5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35.9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>36.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>41.4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>41.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>41.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>41.5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>41.5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>120 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# col_names = ['temperature','occurrence_of_nausea','lumbar_pain','urine_pushing','micturition_pains','burning_of_urethra','decision_1','decision_2']\n",
    "dataset_inflammation = pd.read_table('AcuteInflammations.data', sep=',')\n",
    "print(\"\\nDataset Acute Inflammations:\")\n",
    "dataset_inflammation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['temperature', 'occurrence_of_nausea', 'lumbar_pain', 'urine_pushing',\n       'micturition_pains', 'burning_of_urethra', 'decision_1', 'decision_2'],\n      dtype='object')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_inflammation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acute Inflammations features:\n",
      "\n",
      "[[35.5  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [35.9  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [35.9  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.   0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.2  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.2  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.3  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.6  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.6  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.6  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.6  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.7  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.7  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.7  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [36.8  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.8  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.9  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [36.9  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.   0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.   0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.   0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.   0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.   0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.   0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.   0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.1  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.1  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.1  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.2  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.2  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.2  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.3  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.3  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.3  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.4  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.4  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.5  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.5  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.5  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.5  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.5  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.5  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.6  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.6  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.6  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.7  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.7  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.7  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.7  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.8  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.8  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.8  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [37.9  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.9  0.   0.   1.   1.   0.   1.   0. ]\n",
      " [37.9  0.   1.   0.   0.   0.   0.   0. ]\n",
      " [37.9  0.   0.   1.   1.   1.   1.   0. ]\n",
      " [37.9  0.   0.   1.   0.   0.   1.   0. ]\n",
      " [38.   0.   1.   1.   0.   1.   0.   1. ]\n",
      " [38.   0.   1.   1.   0.   1.   0.   1. ]\n",
      " [38.1  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [38.3  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [38.5  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [38.7  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [38.9  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [39.   0.   1.   1.   0.   1.   0.   1. ]\n",
      " [39.4  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [39.7  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [40.   1.   1.   1.   1.   1.   1.   1. ]\n",
      " [40.   1.   1.   1.   1.   1.   1.   1. ]\n",
      " [40.   1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [40.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [40.   1.   1.   0.   1.   0.   0.   1. ]\n",
      " [40.   1.   1.   0.   1.   0.   0.   1. ]\n",
      " [40.   0.   1.   1.   0.   1.   0.   1. ]\n",
      " [40.1  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.2  1.   1.   1.   1.   1.   1.   1. ]\n",
      " [40.2  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [40.2  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [40.3  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [40.4  1.   1.   1.   1.   1.   1.   1. ]\n",
      " [40.4  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.4  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.4  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [40.4  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [40.5  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.6  1.   1.   1.   1.   1.   1.   1. ]\n",
      " [40.6  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [40.6  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [40.7  1.   1.   1.   1.   1.   1.   1. ]\n",
      " [40.7  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.7  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [40.7  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [40.7  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [40.8  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [40.9  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.9  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [40.9  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [41.   1.   1.   1.   1.   1.   1.   1. ]\n",
      " [41.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [41.   1.   1.   0.   1.   0.   0.   1. ]\n",
      " [41.   0.   1.   1.   0.   1.   0.   1. ]\n",
      " [41.1  1.   1.   1.   1.   1.   1.   1. ]\n",
      " [41.1  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [41.1  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [41.1  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [41.1  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [41.2  1.   1.   1.   1.   1.   1.   1. ]\n",
      " [41.2  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [41.2  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [41.2  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [41.3  1.   1.   1.   1.   0.   1.   1. ]\n",
      " [41.4  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [41.5  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [41.5  1.   1.   0.   1.   0.   0.   1. ]\n",
      " [41.5  0.   1.   1.   0.   1.   0.   1. ]\n",
      " [41.5  0.   1.   1.   0.   1.   0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "inflammation_values = dataset_inflammation.iloc[:,0:8].values\n",
    "print(\"\\nAcute Inflammations features:\\n\")\n",
    "print(inflammation_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Acute Inflammation normalizada:\n"
     ]
    },
    {
     "data": {
      "text/plain": "     temperature  occurrence_of_nausea  lumbar_pain  urine_pushing  \\\n0      -1.779796                     0            1              0   \n1      -1.558989                     0            0              1   \n2      -1.558989                     0            1              0   \n3      -1.503788                     0            0              1   \n4      -1.503788                     0            1              0   \n..           ...                   ...          ...            ...   \n115     1.477107                     0            1              1   \n116     1.532308                     0            0              0   \n117     1.532308                     1            1              0   \n118     1.532308                     0            1              1   \n119     1.532308                     0            1              1   \n\n     micturition_pains  burning_of_urethra  decision_1  decision_2  \n0                    0                   0           0           0  \n1                    1                   1           1           0  \n2                    0                   0           0           0  \n3                    1                   1           1           0  \n4                    0                   0           0           0  \n..                 ...                 ...         ...         ...  \n115                  0                   1           0           1  \n116                  0                   0           0           0  \n117                  1                   0           0           1  \n118                  0                   1           0           1  \n119                  0                   1           0           1  \n\n[120 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temperature</th>\n      <th>occurrence_of_nausea</th>\n      <th>lumbar_pain</th>\n      <th>urine_pushing</th>\n      <th>micturition_pains</th>\n      <th>burning_of_urethra</th>\n      <th>decision_1</th>\n      <th>decision_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.779796</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.558989</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.558989</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.503788</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.503788</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>1.477107</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>1.532308</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>1.532308</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>1.532308</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>1.532308</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>120 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inflammation_normalizada = dataset_inflammation.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "inflammation_normalizada = normalizador.fit_transform(dataset_inflammation)\n",
    "dataset_inflammation['temperature'] = inflammation_normalizada[:,0]\n",
    "\n",
    "print(\"\\nDataset Acute Inflammation normalizada:\")\n",
    "dataset_inflammation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acute Inflammations classes:\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1]\n",
      "\n",
      "Acute Inflammations classes shape:\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "inflammation_classes = dataset_inflammation.iloc[:,7].values\n",
    "print(\"\\nAcute Inflammations classes:\\n\")\n",
    "print(inflammation_classes)\n",
    "print(\"\\nAcute Inflammations classes shape:\")\n",
    "print(inflammation_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 2)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(inflammation_classes.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(inflammation_classes.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(inflammation_classes.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(inflammation_classes.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(inflammation_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68974962\n",
      "Iteration 2, loss = 0.68933206\n",
      "Iteration 3, loss = 0.68872567\n",
      "Iteration 4, loss = 0.68793792\n",
      "Iteration 5, loss = 0.68697514\n",
      "Iteration 6, loss = 0.68584921\n",
      "Iteration 7, loss = 0.68459114\n",
      "Iteration 8, loss = 0.68327225\n",
      "Iteration 9, loss = 0.68202267\n",
      "Iteration 10, loss = 0.68101530\n",
      "Iteration 11, loss = 0.68038708\n",
      "Iteration 12, loss = 0.68014649\n",
      "Iteration 13, loss = 0.68017190\n",
      "Iteration 14, loss = 0.68029988\n",
      "Iteration 15, loss = 0.68040150\n",
      "Iteration 16, loss = 0.68040262\n",
      "Iteration 17, loss = 0.68027404\n",
      "Iteration 18, loss = 0.68001866\n",
      "Iteration 19, loss = 0.67966378\n",
      "Iteration 20, loss = 0.67925493\n",
      "Iteration 21, loss = 0.67884575\n",
      "Iteration 22, loss = 0.67848284\n",
      "Iteration 23, loss = 0.67819174\n",
      "Iteration 24, loss = 0.67797224\n",
      "Iteration 25, loss = 0.67780506\n",
      "Iteration 26, loss = 0.67766368\n",
      "Iteration 27, loss = 0.67752425\n",
      "Iteration 28, loss = 0.67737076\n",
      "Iteration 29, loss = 0.67719614\n",
      "Iteration 30, loss = 0.67700098\n",
      "Iteration 31, loss = 0.67679128\n",
      "Iteration 32, loss = 0.67657565\n",
      "Iteration 33, loss = 0.67636263\n",
      "Iteration 34, loss = 0.67615841\n",
      "Iteration 35, loss = 0.67596554\n",
      "Iteration 36, loss = 0.67578290\n",
      "Iteration 37, loss = 0.67560687\n",
      "Iteration 38, loss = 0.67543294\n",
      "Iteration 39, loss = 0.67525737\n",
      "Iteration 40, loss = 0.67507811\n",
      "Iteration 41, loss = 0.67489493\n",
      "Iteration 42, loss = 0.67470897\n",
      "Iteration 43, loss = 0.67452188\n",
      "Iteration 44, loss = 0.67433508\n",
      "Iteration 45, loss = 0.67414933\n",
      "Iteration 46, loss = 0.67396459\n",
      "Iteration 47, loss = 0.67378028\n",
      "Iteration 48, loss = 0.67359556\n",
      "Iteration 49, loss = 0.67340971\n",
      "Iteration 50, loss = 0.67322225\n",
      "Iteration 51, loss = 0.67303307\n",
      "Iteration 52, loss = 0.67284227\n",
      "Iteration 53, loss = 0.67265005\n",
      "Iteration 54, loss = 0.67245658\n",
      "Iteration 55, loss = 0.67226189\n",
      "Iteration 56, loss = 0.67206587\n",
      "Iteration 57, loss = 0.67186834\n",
      "Iteration 58, loss = 0.67166910\n",
      "Iteration 59, loss = 0.67146797\n",
      "Iteration 60, loss = 0.67126485\n",
      "Iteration 61, loss = 0.67105970\n",
      "Iteration 62, loss = 0.67085249\n",
      "Iteration 63, loss = 0.67064322\n",
      "Iteration 64, loss = 0.67043183\n",
      "Iteration 65, loss = 0.67021827\n",
      "Iteration 66, loss = 0.67000247\n",
      "Iteration 67, loss = 0.66978435\n",
      "Iteration 68, loss = 0.66956384\n",
      "Iteration 69, loss = 0.66934089\n",
      "Iteration 70, loss = 0.66911545\n",
      "Iteration 71, loss = 0.66888747\n",
      "Iteration 72, loss = 0.66865692\n",
      "Iteration 73, loss = 0.66842374\n",
      "Iteration 74, loss = 0.66818788\n",
      "Iteration 75, loss = 0.66794930\n",
      "Iteration 76, loss = 0.66770793\n",
      "Iteration 77, loss = 0.66746374\n",
      "Iteration 78, loss = 0.66721669\n",
      "Iteration 79, loss = 0.66696672\n",
      "Iteration 80, loss = 0.66671380\n",
      "Iteration 81, loss = 0.66645789\n",
      "Iteration 82, loss = 0.66619894\n",
      "Iteration 83, loss = 0.66593691\n",
      "Iteration 84, loss = 0.66567176\n",
      "Iteration 85, loss = 0.66540344\n",
      "Iteration 86, loss = 0.66513191\n",
      "Iteration 87, loss = 0.66485713\n",
      "Iteration 88, loss = 0.66457907\n",
      "Iteration 89, loss = 0.66429767\n",
      "Iteration 90, loss = 0.66401290\n",
      "Iteration 91, loss = 0.66372470\n",
      "Iteration 92, loss = 0.66343305\n",
      "Iteration 93, loss = 0.66313790\n",
      "Iteration 94, loss = 0.66283920\n",
      "Iteration 95, loss = 0.66253691\n",
      "Iteration 96, loss = 0.66223100\n",
      "Iteration 97, loss = 0.66192141\n",
      "Iteration 98, loss = 0.66160810\n",
      "Iteration 99, loss = 0.66129102\n",
      "Iteration 100, loss = 0.66097014\n",
      "Iteration 101, loss = 0.66064541\n",
      "Iteration 102, loss = 0.66031678\n",
      "Iteration 103, loss = 0.65998421\n",
      "Iteration 104, loss = 0.65964765\n",
      "Iteration 105, loss = 0.65930706\n",
      "Iteration 106, loss = 0.65896238\n",
      "Iteration 107, loss = 0.65861358\n",
      "Iteration 108, loss = 0.65826060\n",
      "Iteration 109, loss = 0.65790339\n",
      "Iteration 110, loss = 0.65754192\n",
      "Iteration 111, loss = 0.65717612\n",
      "Iteration 112, loss = 0.65680595\n",
      "Iteration 113, loss = 0.65643137\n",
      "Iteration 114, loss = 0.65605231\n",
      "Iteration 115, loss = 0.65566874\n",
      "Iteration 116, loss = 0.65528059\n",
      "Iteration 117, loss = 0.65488783\n",
      "Iteration 118, loss = 0.65449040\n",
      "Iteration 119, loss = 0.65408824\n",
      "Iteration 120, loss = 0.65368131\n",
      "Iteration 121, loss = 0.65326955\n",
      "Iteration 122, loss = 0.65285292\n",
      "Iteration 123, loss = 0.65243135\n",
      "Iteration 124, loss = 0.65200480\n",
      "Iteration 125, loss = 0.65157321\n",
      "Iteration 126, loss = 0.65113654\n",
      "Iteration 127, loss = 0.65069472\n",
      "Iteration 128, loss = 0.65024770\n",
      "Iteration 129, loss = 0.64979544\n",
      "Iteration 130, loss = 0.64933787\n",
      "Iteration 131, loss = 0.64887494\n",
      "Iteration 132, loss = 0.64840659\n",
      "Iteration 133, loss = 0.64793278\n",
      "Iteration 134, loss = 0.64745345\n",
      "Iteration 135, loss = 0.64696854\n",
      "Iteration 136, loss = 0.64647800\n",
      "Iteration 137, loss = 0.64598178\n",
      "Iteration 138, loss = 0.64547981\n",
      "Iteration 139, loss = 0.64497205\n",
      "Iteration 140, loss = 0.64445844\n",
      "Iteration 141, loss = 0.64393893\n",
      "Iteration 142, loss = 0.64341345\n",
      "Iteration 143, loss = 0.64288197\n",
      "Iteration 144, loss = 0.64234441\n",
      "Iteration 145, loss = 0.64180073\n",
      "Iteration 146, loss = 0.64125087\n",
      "Iteration 147, loss = 0.64069478\n",
      "Iteration 148, loss = 0.64013241\n",
      "Iteration 149, loss = 0.63956369\n",
      "Iteration 150, loss = 0.63898858\n",
      "Iteration 151, loss = 0.63840702\n",
      "Iteration 152, loss = 0.63781896\n",
      "Iteration 153, loss = 0.63722434\n",
      "Iteration 154, loss = 0.63662311\n",
      "Iteration 155, loss = 0.63601522\n",
      "Iteration 156, loss = 0.63540062\n",
      "Iteration 157, loss = 0.63477924\n",
      "Iteration 158, loss = 0.63415105\n",
      "Iteration 159, loss = 0.63351598\n",
      "Iteration 160, loss = 0.63287398\n",
      "Iteration 161, loss = 0.63222501\n",
      "Iteration 162, loss = 0.63156900\n",
      "Iteration 163, loss = 0.63090592\n",
      "Iteration 164, loss = 0.63023570\n",
      "Iteration 165, loss = 0.62955829\n",
      "Iteration 166, loss = 0.62887365\n",
      "Iteration 167, loss = 0.62818171\n",
      "Iteration 168, loss = 0.62748244\n",
      "Iteration 169, loss = 0.62677578\n",
      "Iteration 170, loss = 0.62606167\n",
      "Iteration 171, loss = 0.62534007\n",
      "Iteration 172, loss = 0.62461092\n",
      "Iteration 173, loss = 0.62387418\n",
      "Iteration 174, loss = 0.62312978\n",
      "Iteration 175, loss = 0.62237769\n",
      "Iteration 176, loss = 0.62161784\n",
      "Iteration 177, loss = 0.62085018\n",
      "Iteration 178, loss = 0.62007466\n",
      "Iteration 179, loss = 0.61929123\n",
      "Iteration 180, loss = 0.61849983\n",
      "Iteration 181, loss = 0.61770041\n",
      "Iteration 182, loss = 0.61689290\n",
      "Iteration 183, loss = 0.61607725\n",
      "Iteration 184, loss = 0.61525341\n",
      "Iteration 185, loss = 0.61442131\n",
      "Iteration 186, loss = 0.61358089\n",
      "Iteration 187, loss = 0.61273209\n",
      "Iteration 188, loss = 0.61187484\n",
      "Iteration 189, loss = 0.61100907\n",
      "Iteration 190, loss = 0.61013472\n",
      "Iteration 191, loss = 0.60925172\n",
      "Iteration 192, loss = 0.60835999\n",
      "Iteration 193, loss = 0.60745946\n",
      "Iteration 194, loss = 0.60655005\n",
      "Iteration 195, loss = 0.60563168\n",
      "Iteration 196, loss = 0.60470427\n",
      "Iteration 197, loss = 0.60376774\n",
      "Iteration 198, loss = 0.60282201\n",
      "Iteration 199, loss = 0.60186698\n",
      "Iteration 200, loss = 0.60090259\n",
      "Iteration 201, loss = 0.59992874\n",
      "Iteration 202, loss = 0.59894536\n",
      "Iteration 203, loss = 0.59795238\n",
      "Iteration 204, loss = 0.59694974\n",
      "Iteration 205, loss = 0.59593737\n",
      "Iteration 206, loss = 0.59491524\n",
      "Iteration 207, loss = 0.59388332\n",
      "Iteration 208, loss = 0.59284160\n",
      "Iteration 209, loss = 0.59179010\n",
      "Iteration 210, loss = 0.59072886\n",
      "Iteration 211, loss = 0.58965796\n",
      "Iteration 212, loss = 0.58857750\n",
      "Iteration 213, loss = 0.58748761\n",
      "Iteration 214, loss = 0.58638847\n",
      "Iteration 215, loss = 0.58528028\n",
      "Iteration 216, loss = 0.58416326\n",
      "Iteration 217, loss = 0.58303768\n",
      "Iteration 218, loss = 0.58190378\n",
      "Iteration 219, loss = 0.58076185\n",
      "Iteration 220, loss = 0.57961215\n",
      "Iteration 221, loss = 0.57845492\n",
      "Iteration 222, loss = 0.57729038\n",
      "Iteration 223, loss = 0.57611873\n",
      "Iteration 224, loss = 0.57494011\n",
      "Iteration 225, loss = 0.57375463\n",
      "Iteration 226, loss = 0.57256235\n",
      "Iteration 227, loss = 0.57136330\n",
      "Iteration 228, loss = 0.57015746\n",
      "Iteration 229, loss = 0.56894479\n",
      "Iteration 230, loss = 0.56772524\n",
      "Iteration 231, loss = 0.56649874\n",
      "Iteration 232, loss = 0.56526519\n",
      "Iteration 233, loss = 0.56402453\n",
      "Iteration 234, loss = 0.56277667\n",
      "Iteration 235, loss = 0.56152156\n",
      "Iteration 236, loss = 0.56025914\n",
      "Iteration 237, loss = 0.55898939\n",
      "Iteration 238, loss = 0.55771227\n",
      "Iteration 239, loss = 0.55642779\n",
      "Iteration 240, loss = 0.55513597\n",
      "Iteration 241, loss = 0.55383683\n",
      "Iteration 242, loss = 0.55253042\n",
      "Iteration 243, loss = 0.55121680\n",
      "Iteration 244, loss = 0.54989604\n",
      "Iteration 245, loss = 0.54856822\n",
      "Iteration 246, loss = 0.54723344\n",
      "Iteration 247, loss = 0.54589180\n",
      "Iteration 248, loss = 0.54454339\n",
      "Iteration 249, loss = 0.54318833\n",
      "Iteration 250, loss = 0.54182672\n",
      "Iteration 251, loss = 0.54045869\n",
      "Iteration 252, loss = 0.53908433\n",
      "Iteration 253, loss = 0.53770377\n",
      "Iteration 254, loss = 0.53631712\n",
      "Iteration 255, loss = 0.53492447\n",
      "Iteration 256, loss = 0.53352594\n",
      "Iteration 257, loss = 0.53212164\n",
      "Iteration 258, loss = 0.53071165\n",
      "Iteration 259, loss = 0.52929607\n",
      "Iteration 260, loss = 0.52787501\n",
      "Iteration 261, loss = 0.52644855\n",
      "Iteration 262, loss = 0.52501679\n",
      "Iteration 263, loss = 0.52357982\n",
      "Iteration 264, loss = 0.52213772\n",
      "Iteration 265, loss = 0.52069060\n",
      "Iteration 266, loss = 0.51923855\n",
      "Iteration 267, loss = 0.51778166\n",
      "Iteration 268, loss = 0.51632003\n",
      "Iteration 269, loss = 0.51485376\n",
      "Iteration 270, loss = 0.51338295\n",
      "Iteration 271, loss = 0.51190772\n",
      "Iteration 272, loss = 0.51042816\n",
      "Iteration 273, loss = 0.50894438\n",
      "Iteration 274, loss = 0.50745650\n",
      "Iteration 275, loss = 0.50596463\n",
      "Iteration 276, loss = 0.50446887\n",
      "Iteration 277, loss = 0.50296934\n",
      "Iteration 278, loss = 0.50146615\n",
      "Iteration 279, loss = 0.49995941\n",
      "Iteration 280, loss = 0.49844924\n",
      "Iteration 281, loss = 0.49693575\n",
      "Iteration 282, loss = 0.49541904\n",
      "Iteration 283, loss = 0.49389923\n",
      "Iteration 284, loss = 0.49237643\n",
      "Iteration 285, loss = 0.49085075\n",
      "Iteration 286, loss = 0.48932230\n",
      "Iteration 287, loss = 0.48779119\n",
      "Iteration 288, loss = 0.48625753\n",
      "Iteration 289, loss = 0.48472143\n",
      "Iteration 290, loss = 0.48318300\n",
      "Iteration 291, loss = 0.48164235\n",
      "Iteration 292, loss = 0.48009960\n",
      "Iteration 293, loss = 0.47855485\n",
      "Iteration 294, loss = 0.47700821\n",
      "Iteration 295, loss = 0.47545980\n",
      "Iteration 296, loss = 0.47390973\n",
      "Iteration 297, loss = 0.47235810\n",
      "Iteration 298, loss = 0.47080503\n",
      "Iteration 299, loss = 0.46925062\n",
      "Iteration 300, loss = 0.46769499\n",
      "Iteration 1, loss = 0.68052746\n",
      "Iteration 2, loss = 0.68043019\n",
      "Iteration 3, loss = 0.68029698\n",
      "Iteration 4, loss = 0.68013666\n",
      "Iteration 5, loss = 0.67995766\n",
      "Iteration 6, loss = 0.67976761\n",
      "Iteration 7, loss = 0.67957306\n",
      "Iteration 8, loss = 0.67937933\n",
      "Iteration 9, loss = 0.67919038\n",
      "Iteration 10, loss = 0.67900881\n",
      "Iteration 11, loss = 0.67883584\n",
      "Iteration 12, loss = 0.67867138\n",
      "Iteration 13, loss = 0.67851404\n",
      "Iteration 14, loss = 0.67836115\n",
      "Iteration 15, loss = 0.67820868\n",
      "Iteration 16, loss = 0.67805113\n",
      "Iteration 17, loss = 0.67788118\n",
      "Iteration 18, loss = 0.67768924\n",
      "Iteration 19, loss = 0.67746251\n",
      "Iteration 20, loss = 0.67718373\n",
      "Iteration 21, loss = 0.67682920\n",
      "Iteration 22, loss = 0.67636630\n",
      "Iteration 23, loss = 0.67575101\n",
      "Iteration 24, loss = 0.67492879\n",
      "Iteration 25, loss = 0.67384819\n",
      "Iteration 26, loss = 0.67250874\n",
      "Iteration 27, loss = 0.67106104\n",
      "Iteration 28, loss = 0.66987352\n",
      "Iteration 29, loss = 0.66929745\n",
      "Iteration 30, loss = 0.66921682\n",
      "Iteration 31, loss = 0.66914399\n",
      "Iteration 32, loss = 0.66874658\n",
      "Iteration 33, loss = 0.66803220\n",
      "Iteration 34, loss = 0.66720319\n",
      "Iteration 35, loss = 0.66646301\n",
      "Iteration 36, loss = 0.66590137\n",
      "Iteration 37, loss = 0.66549082\n",
      "Iteration 38, loss = 0.66515191\n",
      "Iteration 39, loss = 0.66481720\n",
      "Iteration 40, loss = 0.66446039\n",
      "Iteration 41, loss = 0.66409263\n",
      "Iteration 42, loss = 0.66374069\n",
      "Iteration 43, loss = 0.66342388\n",
      "Iteration 44, loss = 0.66314363\n",
      "Iteration 45, loss = 0.66288821\n",
      "Iteration 46, loss = 0.66264386\n",
      "Iteration 47, loss = 0.66240251\n",
      "Iteration 48, loss = 0.66216268\n",
      "Iteration 49, loss = 0.66192612\n",
      "Iteration 50, loss = 0.66169407\n",
      "Iteration 51, loss = 0.66146552\n",
      "Iteration 52, loss = 0.66123781\n",
      "Iteration 53, loss = 0.66100812\n",
      "Iteration 54, loss = 0.66077455\n",
      "Iteration 55, loss = 0.66053631\n",
      "Iteration 56, loss = 0.66029326\n",
      "Iteration 57, loss = 0.66004538\n",
      "Iteration 58, loss = 0.65979248\n",
      "Iteration 59, loss = 0.65953432\n",
      "Iteration 60, loss = 0.65927075\n",
      "Iteration 61, loss = 0.65900180\n",
      "Iteration 62, loss = 0.65872769\n",
      "Iteration 63, loss = 0.65844871\n",
      "Iteration 64, loss = 0.65816511\n",
      "Iteration 65, loss = 0.65787712\n",
      "Iteration 66, loss = 0.65758492\n",
      "Iteration 67, loss = 0.65728869\n",
      "Iteration 68, loss = 0.65698860\n",
      "Iteration 69, loss = 0.65668478\n",
      "Iteration 70, loss = 0.65637734\n",
      "Iteration 71, loss = 0.65606636\n",
      "Iteration 72, loss = 0.65575186\n",
      "Iteration 73, loss = 0.65543385\n",
      "Iteration 74, loss = 0.65511233\n",
      "Iteration 75, loss = 0.65478726\n",
      "Iteration 76, loss = 0.65445859\n",
      "Iteration 77, loss = 0.65412625\n",
      "Iteration 78, loss = 0.65379018\n",
      "Iteration 79, loss = 0.65345030\n",
      "Iteration 80, loss = 0.65310652\n",
      "Iteration 81, loss = 0.65275877\n",
      "Iteration 82, loss = 0.65240695\n",
      "Iteration 83, loss = 0.65205099\n",
      "Iteration 84, loss = 0.65169081\n",
      "Iteration 85, loss = 0.65132634\n",
      "Iteration 86, loss = 0.65095750\n",
      "Iteration 87, loss = 0.65058424\n",
      "Iteration 88, loss = 0.65020648\n",
      "Iteration 89, loss = 0.64982418\n",
      "Iteration 90, loss = 0.64943727\n",
      "Iteration 91, loss = 0.64904572\n",
      "Iteration 92, loss = 0.64864946\n",
      "Iteration 93, loss = 0.64824847\n",
      "Iteration 94, loss = 0.64784270\n",
      "Iteration 95, loss = 0.64743210\n",
      "Iteration 96, loss = 0.64701664\n",
      "Iteration 97, loss = 0.64659628\n",
      "Iteration 98, loss = 0.64617099\n",
      "Iteration 99, loss = 0.64574071\n",
      "Iteration 100, loss = 0.64530542\n",
      "Iteration 101, loss = 0.64486507\n",
      "Iteration 102, loss = 0.64441962\n",
      "Iteration 103, loss = 0.64396903\n",
      "Iteration 104, loss = 0.64351326\n",
      "Iteration 105, loss = 0.64305227\n",
      "Iteration 106, loss = 0.64258601\n",
      "Iteration 107, loss = 0.64211443\n",
      "Iteration 108, loss = 0.64163749\n",
      "Iteration 109, loss = 0.64115514\n",
      "Iteration 110, loss = 0.64066734\n",
      "Iteration 111, loss = 0.64017404\n",
      "Iteration 112, loss = 0.63967519\n",
      "Iteration 113, loss = 0.63917075\n",
      "Iteration 114, loss = 0.63866066\n",
      "Iteration 115, loss = 0.63814489\n",
      "Iteration 116, loss = 0.63762338\n",
      "Iteration 117, loss = 0.63709608\n",
      "Iteration 118, loss = 0.63656296\n",
      "Iteration 119, loss = 0.63602397\n",
      "Iteration 120, loss = 0.63547906\n",
      "Iteration 121, loss = 0.63492819\n",
      "Iteration 122, loss = 0.63437131\n",
      "Iteration 123, loss = 0.63380838\n",
      "Iteration 124, loss = 0.63323936\n",
      "Iteration 125, loss = 0.63266420\n",
      "Iteration 126, loss = 0.63208286\n",
      "Iteration 127, loss = 0.63149531\n",
      "Iteration 128, loss = 0.63090149\n",
      "Iteration 129, loss = 0.63030137\n",
      "Iteration 130, loss = 0.62969490\n",
      "Iteration 131, loss = 0.62908205\n",
      "Iteration 132, loss = 0.62846277\n",
      "Iteration 133, loss = 0.62783703\n",
      "Iteration 134, loss = 0.62720478\n",
      "Iteration 135, loss = 0.62656599\n",
      "Iteration 136, loss = 0.62592062\n",
      "Iteration 137, loss = 0.62526862\n",
      "Iteration 138, loss = 0.62460997\n",
      "Iteration 139, loss = 0.62394462\n",
      "Iteration 140, loss = 0.62327254\n",
      "Iteration 141, loss = 0.62259369\n",
      "Iteration 142, loss = 0.62190804\n",
      "Iteration 143, loss = 0.62121555\n",
      "Iteration 144, loss = 0.62051618\n",
      "Iteration 145, loss = 0.61980990\n",
      "Iteration 146, loss = 0.61909669\n",
      "Iteration 147, loss = 0.61837650\n",
      "Iteration 148, loss = 0.61764931\n",
      "Iteration 149, loss = 0.61691508\n",
      "Iteration 150, loss = 0.61617379\n",
      "Iteration 151, loss = 0.61542540\n",
      "Iteration 152, loss = 0.61466988\n",
      "Iteration 153, loss = 0.61390722\n",
      "Iteration 154, loss = 0.61313738\n",
      "Iteration 155, loss = 0.61236033\n",
      "Iteration 156, loss = 0.61157605\n",
      "Iteration 157, loss = 0.61078451\n",
      "Iteration 158, loss = 0.60998570\n",
      "Iteration 159, loss = 0.60917958\n",
      "Iteration 160, loss = 0.60836614\n",
      "Iteration 161, loss = 0.60754536\n",
      "Iteration 162, loss = 0.60671721\n",
      "Iteration 163, loss = 0.60588168\n",
      "Iteration 164, loss = 0.60503874\n",
      "Iteration 165, loss = 0.60418839\n",
      "Iteration 166, loss = 0.60333060\n",
      "Iteration 167, loss = 0.60246536\n",
      "Iteration 168, loss = 0.60159266\n",
      "Iteration 169, loss = 0.60071248\n",
      "Iteration 170, loss = 0.59982482\n",
      "Iteration 171, loss = 0.59892965\n",
      "Iteration 172, loss = 0.59802698\n",
      "Iteration 173, loss = 0.59711679\n",
      "Iteration 174, loss = 0.59619907\n",
      "Iteration 175, loss = 0.59527382\n",
      "Iteration 176, loss = 0.59434104\n",
      "Iteration 177, loss = 0.59340072\n",
      "Iteration 178, loss = 0.59245286\n",
      "Iteration 179, loss = 0.59149746\n",
      "Iteration 180, loss = 0.59053452\n",
      "Iteration 181, loss = 0.58956403\n",
      "Iteration 182, loss = 0.58858601\n",
      "Iteration 183, loss = 0.58760045\n",
      "Iteration 184, loss = 0.58660736\n",
      "Iteration 185, loss = 0.58560675\n",
      "Iteration 186, loss = 0.58459863\n",
      "Iteration 187, loss = 0.58358300\n",
      "Iteration 188, loss = 0.58255987\n",
      "Iteration 189, loss = 0.58152926\n",
      "Iteration 190, loss = 0.58049118\n",
      "Iteration 191, loss = 0.57944564\n",
      "Iteration 192, loss = 0.57839266\n",
      "Iteration 193, loss = 0.57733226\n",
      "Iteration 194, loss = 0.57626446\n",
      "Iteration 195, loss = 0.57518927\n",
      "Iteration 196, loss = 0.57410671\n",
      "Iteration 197, loss = 0.57301682\n",
      "Iteration 198, loss = 0.57191961\n",
      "Iteration 199, loss = 0.57081510\n",
      "Iteration 200, loss = 0.56970334\n",
      "Iteration 201, loss = 0.56858434\n",
      "Iteration 202, loss = 0.56745813\n",
      "Iteration 203, loss = 0.56632475\n",
      "Iteration 204, loss = 0.56518423\n",
      "Iteration 205, loss = 0.56403660\n",
      "Iteration 206, loss = 0.56288189\n",
      "Iteration 207, loss = 0.56172015\n",
      "Iteration 208, loss = 0.56055142\n",
      "Iteration 209, loss = 0.55937572\n",
      "Iteration 210, loss = 0.55819311\n",
      "Iteration 211, loss = 0.55700363\n",
      "Iteration 212, loss = 0.55580731\n",
      "Iteration 213, loss = 0.55460421\n",
      "Iteration 214, loss = 0.55339437\n",
      "Iteration 215, loss = 0.55217785\n",
      "Iteration 216, loss = 0.55095468\n",
      "Iteration 217, loss = 0.54972492\n",
      "Iteration 218, loss = 0.54848863\n",
      "Iteration 219, loss = 0.54724585\n",
      "Iteration 220, loss = 0.54599664\n",
      "Iteration 221, loss = 0.54474106\n",
      "Iteration 222, loss = 0.54347916\n",
      "Iteration 223, loss = 0.54221101\n",
      "Iteration 224, loss = 0.54093666\n",
      "Iteration 225, loss = 0.53965617\n",
      "Iteration 226, loss = 0.53836962\n",
      "Iteration 227, loss = 0.53707705\n",
      "Iteration 228, loss = 0.53577855\n",
      "Iteration 229, loss = 0.53447416\n",
      "Iteration 230, loss = 0.53316397\n",
      "Iteration 231, loss = 0.53184804\n",
      "Iteration 232, loss = 0.53052644\n",
      "Iteration 233, loss = 0.52919923\n",
      "Iteration 234, loss = 0.52786651\n",
      "Iteration 235, loss = 0.52652833\n",
      "Iteration 236, loss = 0.52518477\n",
      "Iteration 237, loss = 0.52383591\n",
      "Iteration 238, loss = 0.52248182\n",
      "Iteration 239, loss = 0.52112259\n",
      "Iteration 240, loss = 0.51975829\n",
      "Iteration 241, loss = 0.51838900\n",
      "Iteration 242, loss = 0.51701480\n",
      "Iteration 243, loss = 0.51563577\n",
      "Iteration 244, loss = 0.51425200\n",
      "Iteration 245, loss = 0.51286357\n",
      "Iteration 246, loss = 0.51147056\n",
      "Iteration 247, loss = 0.51007307\n",
      "Iteration 248, loss = 0.50867117\n",
      "Iteration 249, loss = 0.50726495\n",
      "Iteration 250, loss = 0.50585450\n",
      "Iteration 251, loss = 0.50443992\n",
      "Iteration 252, loss = 0.50302128\n",
      "Iteration 253, loss = 0.50159868\n",
      "Iteration 254, loss = 0.50017221\n",
      "Iteration 255, loss = 0.49874196\n",
      "Iteration 256, loss = 0.49730802\n",
      "Iteration 257, loss = 0.49587049\n",
      "Iteration 258, loss = 0.49442945\n",
      "Iteration 259, loss = 0.49298501\n",
      "Iteration 260, loss = 0.49153726\n",
      "Iteration 261, loss = 0.49008628\n",
      "Iteration 262, loss = 0.48863218\n",
      "Iteration 263, loss = 0.48717505\n",
      "Iteration 264, loss = 0.48571499\n",
      "Iteration 265, loss = 0.48425209\n",
      "Iteration 266, loss = 0.48278645\n",
      "Iteration 267, loss = 0.48131817\n",
      "Iteration 268, loss = 0.47984734\n",
      "Iteration 269, loss = 0.47837407\n",
      "Iteration 270, loss = 0.47689844\n",
      "Iteration 271, loss = 0.47542056\n",
      "Iteration 272, loss = 0.47394052\n",
      "Iteration 273, loss = 0.47245843\n",
      "Iteration 274, loss = 0.47097438\n",
      "Iteration 275, loss = 0.46948846\n",
      "Iteration 276, loss = 0.46800078\n",
      "Iteration 277, loss = 0.46651144\n",
      "Iteration 278, loss = 0.46502053\n",
      "Iteration 279, loss = 0.46352816\n",
      "Iteration 280, loss = 0.46203441\n",
      "Iteration 281, loss = 0.46053939\n",
      "Iteration 282, loss = 0.45904320\n",
      "Iteration 283, loss = 0.45754592\n",
      "Iteration 284, loss = 0.45604767\n",
      "Iteration 285, loss = 0.45454854\n",
      "Iteration 286, loss = 0.45304862\n",
      "Iteration 287, loss = 0.45154801\n",
      "Iteration 288, loss = 0.45004681\n",
      "Iteration 289, loss = 0.44854512\n",
      "Iteration 290, loss = 0.44704302\n",
      "Iteration 291, loss = 0.44554061\n",
      "Iteration 292, loss = 0.44403800\n",
      "Iteration 293, loss = 0.44253527\n",
      "Iteration 294, loss = 0.44103252\n",
      "Iteration 295, loss = 0.43952983\n",
      "Iteration 296, loss = 0.43802731\n",
      "Iteration 297, loss = 0.43652505\n",
      "Iteration 298, loss = 0.43502314\n",
      "Iteration 299, loss = 0.43352166\n",
      "Iteration 300, loss = 0.43202072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for train, test in kf.split(inflammation_values, inflammation_classes):\n",
    "    data_train, target_train = inflammation_values[train], inflammation_classes[train]\n",
    "    data_test, target_test = inflammation_values[test], inflammation_classes[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.69      0.81        70\n",
      "           1       0.69      1.00      0.82        50\n",
      "\n",
      "    accuracy                           0.82       120\n",
      "   macro avg       0.85      0.84      0.82       120\n",
      "weighted avg       0.87      0.82      0.82       120\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[48 22]\n",
      " [ 0 50]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88        70\n",
      "           1       0.81      0.88      0.85        50\n",
      "\n",
      "    accuracy                           0.87       120\n",
      "   macro avg       0.86      0.87      0.86       120\n",
      "weighted avg       0.87      0.87      0.87       120\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[60 10]\n",
      " [ 6 44]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        70\n",
      "           1       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       120\n",
      "   macro avg       1.00      1.00      1.00       120\n",
      "weighted avg       1.00      1.00      1.00       120\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[70  0]\n",
      " [ 0 50]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        70\n",
      "           1       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       120\n",
      "   macro avg       1.00      1.00      1.00       120\n",
      "weighted avg       1.00      1.00      1.00       120\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[70  0]\n",
      " [ 0 50]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        70\n",
      "           1       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       120\n",
      "   macro avg       1.00      1.00      1.00       120\n",
      "weighted avg       1.00      1.00      1.00       120\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[70  0]\n",
      " [ 0 50]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(inflammation_classes, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusão: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(inflammation_classes, predicted_classes[classificador]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}