{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Monkeys:\n"
     ]
    },
    {
     "data": {
      "text/plain": "      Class  A1  A2  A3  A4  A5  A6\n0         1   1   1   1   1   1   1\n1         1   1   1   1   1   1   2\n2         1   1   1   1   1   2   1\n3         1   1   1   1   1   2   2\n4         1   1   1   1   1   3   1\n...     ...  ..  ..  ..  ..  ..  ..\n1707      0   3   3   2   2   2   2\n1708      0   3   3   2   2   3   2\n1709      0   3   3   2   3   1   1\n1710      0   3   3   2   3   3   2\n1711      0   3   3   2   3   4   2\n\n[1712 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>A1</th>\n      <th>A2</th>\n      <th>A3</th>\n      <th>A4</th>\n      <th>A5</th>\n      <th>A6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1707</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1708</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1709</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1710</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1711</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1712 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "monkeys = pd.read_table('monkeys.data', sep=',')\n",
    "print(\"\\nDataset Monkeys:\")\n",
    "monkeys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Class', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6'], dtype='object')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monkeys.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Monkeys Normalized:\n"
     ]
    },
    {
     "data": {
      "text/plain": "      Class  A1  A2  A3  A4  A5  A6\n0         1   1   1   1   1   1   1\n1         1   1   1   1   1   1   2\n2         1   1   1   1   1   2   1\n3         1   1   1   1   1   2   2\n4         1   1   1   1   1   3   1\n...     ...  ..  ..  ..  ..  ..  ..\n1707      0   3   3   2   2   2   2\n1708      0   3   3   2   2   3   2\n1709      0   3   3   2   3   1   1\n1710      0   3   3   2   3   3   2\n1711      0   3   3   2   3   4   2\n\n[1712 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>A1</th>\n      <th>A2</th>\n      <th>A3</th>\n      <th>A4</th>\n      <th>A5</th>\n      <th>A6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1707</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1708</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1709</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1710</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1711</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1712 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monkeysNormalized = monkeys.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "# monkeysNormalized = normalizador.fit_transform(monkeys)\n",
    "# monkeys['Class'] = monkeysNormalized[:,0]\n",
    "# monkeys['A1'] = monkeysNormalized[:,1]\n",
    "# monkeys['A2'] = monkeysNormalized[:,2]\n",
    "# monkeys['A3'] = monkeysNormalized[:,3]\n",
    "# monkeys['A4'] = monkeysNormalized[:,4]\n",
    "# monkeys['A5'] = monkeysNormalized[:,5]\n",
    "# monkeys['A6'] = monkeysNormalized[:,6]\n",
    "\n",
    "\n",
    "print(\"\\nDataset Monkeys Normalized:\")\n",
    "monkeys\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Monkeys features:\n",
      "\n",
      "[[1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 2]\n",
      " ...\n",
      " [0 3 3 2 3 1]\n",
      " [0 3 3 2 3 3]\n",
      " [0 3 3 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "monkeysValues = monkeys.iloc[:,0:6].values\n",
    "print(\"\\n Monkeys features:\\n\")\n",
    "print(monkeysValues)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monkeys classes:\n",
      "\n",
      "[1 1 2 ... 1 3 4]\n",
      "\n",
      "Monkeys classes shape:\n",
      "(1712,)\n"
     ]
    }
   ],
   "source": [
    "monkeysClasses = monkeys.iloc[:,5].values\n",
    "print(\"\\nMonkeys classes:\\n\")\n",
    "print(monkeysClasses)\n",
    "print(\"\\nMonkeys classes shape:\")\n",
    "print(monkeysClasses.shape)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(monkeysClasses.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(monkeysClasses.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(monkeysClasses.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(monkeysClasses.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(monkeysClasses.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.45235193\n",
      "Iteration 2, loss = 1.41806187\n",
      "Iteration 3, loss = 1.38795226\n",
      "Iteration 4, loss = 1.37197899\n",
      "Iteration 5, loss = 1.36385913\n",
      "Iteration 6, loss = 1.35780584\n",
      "Iteration 7, loss = 1.35080107\n",
      "Iteration 8, loss = 1.34310703\n",
      "Iteration 9, loss = 1.33448478\n",
      "Iteration 10, loss = 1.32517358\n",
      "Iteration 11, loss = 1.31522534\n",
      "Iteration 12, loss = 1.30416860\n",
      "Iteration 13, loss = 1.29247092\n",
      "Iteration 14, loss = 1.28008928\n",
      "Iteration 15, loss = 1.26702916\n",
      "Iteration 16, loss = 1.25316236\n",
      "Iteration 17, loss = 1.23891557\n",
      "Iteration 18, loss = 1.22440042\n",
      "Iteration 19, loss = 1.20953563\n",
      "Iteration 20, loss = 1.19430861\n",
      "Iteration 21, loss = 1.17869224\n",
      "Iteration 22, loss = 1.16300388\n",
      "Iteration 23, loss = 1.14707334\n",
      "Iteration 24, loss = 1.13128784\n",
      "Iteration 25, loss = 1.11539426\n",
      "Iteration 26, loss = 1.09982747\n",
      "Iteration 27, loss = 1.08437817\n",
      "Iteration 28, loss = 1.06933103\n",
      "Iteration 29, loss = 1.05448442\n",
      "Iteration 30, loss = 1.04003180\n",
      "Iteration 31, loss = 1.02603224\n",
      "Iteration 32, loss = 1.01250705\n",
      "Iteration 33, loss = 0.99936381\n",
      "Iteration 34, loss = 0.98671374\n",
      "Iteration 35, loss = 0.97445669\n",
      "Iteration 36, loss = 0.96272807\n",
      "Iteration 37, loss = 0.95143773\n",
      "Iteration 38, loss = 0.94040773\n",
      "Iteration 39, loss = 0.92987325\n",
      "Iteration 40, loss = 0.91989191\n",
      "Iteration 41, loss = 0.91029597\n",
      "Iteration 42, loss = 0.90073416\n",
      "Iteration 43, loss = 0.89164580\n",
      "Iteration 44, loss = 0.88295844\n",
      "Iteration 45, loss = 0.87458531\n",
      "Iteration 46, loss = 0.86658861\n",
      "Iteration 47, loss = 0.85875949\n",
      "Iteration 48, loss = 0.85126107\n",
      "Iteration 49, loss = 0.84392233\n",
      "Iteration 50, loss = 0.83693009\n",
      "Iteration 51, loss = 0.83005500\n",
      "Iteration 52, loss = 0.82358930\n",
      "Iteration 53, loss = 0.81714295\n",
      "Iteration 54, loss = 0.81088011\n",
      "Iteration 55, loss = 0.80492839\n",
      "Iteration 56, loss = 0.79900408\n",
      "Iteration 57, loss = 0.79331818\n",
      "Iteration 58, loss = 0.78773475\n",
      "Iteration 59, loss = 0.78224451\n",
      "Iteration 60, loss = 0.77706893\n",
      "Iteration 61, loss = 0.77181662\n",
      "Iteration 62, loss = 0.76674186\n",
      "Iteration 63, loss = 0.76176560\n",
      "Iteration 64, loss = 0.75694500\n",
      "Iteration 65, loss = 0.75226309\n",
      "Iteration 66, loss = 0.74757297\n",
      "Iteration 67, loss = 0.74303387\n",
      "Iteration 68, loss = 0.73858762\n",
      "Iteration 69, loss = 0.73409448\n",
      "Iteration 70, loss = 0.72977206\n",
      "Iteration 71, loss = 0.72548389\n",
      "Iteration 72, loss = 0.72137645\n",
      "Iteration 73, loss = 0.71735586\n",
      "Iteration 74, loss = 0.71313339\n",
      "Iteration 75, loss = 0.70915054\n",
      "Iteration 76, loss = 0.70526429\n",
      "Iteration 77, loss = 0.70130925\n",
      "Iteration 78, loss = 0.69739159\n",
      "Iteration 79, loss = 0.69350419\n",
      "Iteration 80, loss = 0.68976296\n",
      "Iteration 81, loss = 0.68610651\n",
      "Iteration 82, loss = 0.68234458\n",
      "Iteration 83, loss = 0.67867582\n",
      "Iteration 84, loss = 0.67512315\n",
      "Iteration 85, loss = 0.67149343\n",
      "Iteration 86, loss = 0.66797008\n",
      "Iteration 87, loss = 0.66448819\n",
      "Iteration 88, loss = 0.66100982\n",
      "Iteration 89, loss = 0.65755228\n",
      "Iteration 90, loss = 0.65411362\n",
      "Iteration 91, loss = 0.65082852\n",
      "Iteration 92, loss = 0.64747626\n",
      "Iteration 93, loss = 0.64418110\n",
      "Iteration 94, loss = 0.64082857\n",
      "Iteration 95, loss = 0.63762945\n",
      "Iteration 96, loss = 0.63442711\n",
      "Iteration 97, loss = 0.63107623\n",
      "Iteration 98, loss = 0.62790528\n",
      "Iteration 99, loss = 0.62483412\n",
      "Iteration 100, loss = 0.62168165\n",
      "Iteration 101, loss = 0.61861331\n",
      "Iteration 102, loss = 0.61541709\n",
      "Iteration 103, loss = 0.61237287\n",
      "Iteration 104, loss = 0.60944447\n",
      "Iteration 105, loss = 0.60648277\n",
      "Iteration 106, loss = 0.60334263\n",
      "Iteration 107, loss = 0.60042691\n",
      "Iteration 108, loss = 0.59746080\n",
      "Iteration 109, loss = 0.59464585\n",
      "Iteration 110, loss = 0.59171717\n",
      "Iteration 111, loss = 0.58880338\n",
      "Iteration 112, loss = 0.58601354\n",
      "Iteration 113, loss = 0.58308509\n",
      "Iteration 114, loss = 0.58027093\n",
      "Iteration 115, loss = 0.57748565\n",
      "Iteration 116, loss = 0.57469438\n",
      "Iteration 117, loss = 0.57195980\n",
      "Iteration 118, loss = 0.56927202\n",
      "Iteration 119, loss = 0.56657344\n",
      "Iteration 120, loss = 0.56382249\n",
      "Iteration 121, loss = 0.56122110\n",
      "Iteration 122, loss = 0.55851503\n",
      "Iteration 123, loss = 0.55589606\n",
      "Iteration 124, loss = 0.55327077\n",
      "Iteration 125, loss = 0.55068109\n",
      "Iteration 126, loss = 0.54818171\n",
      "Iteration 127, loss = 0.54558610\n",
      "Iteration 128, loss = 0.54317941\n",
      "Iteration 129, loss = 0.54055954\n",
      "Iteration 130, loss = 0.53814150\n",
      "Iteration 131, loss = 0.53555816\n",
      "Iteration 132, loss = 0.53312279\n",
      "Iteration 133, loss = 0.53065938\n",
      "Iteration 134, loss = 0.52825671\n",
      "Iteration 135, loss = 0.52590644\n",
      "Iteration 136, loss = 0.52354523\n",
      "Iteration 137, loss = 0.52114180\n",
      "Iteration 138, loss = 0.51878753\n",
      "Iteration 139, loss = 0.51643067\n",
      "Iteration 140, loss = 0.51408454\n",
      "Iteration 141, loss = 0.51182410\n",
      "Iteration 142, loss = 0.50947963\n",
      "Iteration 143, loss = 0.50732261\n",
      "Iteration 144, loss = 0.50501764\n",
      "Iteration 145, loss = 0.50299534\n",
      "Iteration 146, loss = 0.50068881\n",
      "Iteration 147, loss = 0.49833652\n",
      "Iteration 148, loss = 0.49615833\n",
      "Iteration 149, loss = 0.49400073\n",
      "Iteration 150, loss = 0.49179300\n",
      "Iteration 151, loss = 0.48967278\n",
      "Iteration 152, loss = 0.48754341\n",
      "Iteration 153, loss = 0.48541438\n",
      "Iteration 154, loss = 0.48336405\n",
      "Iteration 155, loss = 0.48123416\n",
      "Iteration 156, loss = 0.47914780\n",
      "Iteration 157, loss = 0.47706944\n",
      "Iteration 158, loss = 0.47507528\n",
      "Iteration 159, loss = 0.47312833\n",
      "Iteration 160, loss = 0.47101898\n",
      "Iteration 161, loss = 0.46905466\n",
      "Iteration 162, loss = 0.46702452\n",
      "Iteration 163, loss = 0.46503353\n",
      "Iteration 164, loss = 0.46303706\n",
      "Iteration 165, loss = 0.46116219\n",
      "Iteration 166, loss = 0.45926874\n",
      "Iteration 167, loss = 0.45717233\n",
      "Iteration 168, loss = 0.45543469\n",
      "Iteration 169, loss = 0.45354570\n",
      "Iteration 170, loss = 0.45157569\n",
      "Iteration 171, loss = 0.44964078\n",
      "Iteration 172, loss = 0.44779594\n",
      "Iteration 173, loss = 0.44592855\n",
      "Iteration 174, loss = 0.44408799\n",
      "Iteration 175, loss = 0.44228568\n",
      "Iteration 176, loss = 0.44044611\n",
      "Iteration 177, loss = 0.43862267\n",
      "Iteration 178, loss = 0.43686440\n",
      "Iteration 179, loss = 0.43516227\n",
      "Iteration 180, loss = 0.43336725\n",
      "Iteration 181, loss = 0.43171231\n",
      "Iteration 182, loss = 0.42979800\n",
      "Iteration 183, loss = 0.42806418\n",
      "Iteration 184, loss = 0.42644950\n",
      "Iteration 185, loss = 0.42461828\n",
      "Iteration 186, loss = 0.42284571\n",
      "Iteration 187, loss = 0.42113952\n",
      "Iteration 188, loss = 0.41957570\n",
      "Iteration 189, loss = 0.41788388\n",
      "Iteration 190, loss = 0.41612799\n",
      "Iteration 191, loss = 0.41447189\n",
      "Iteration 192, loss = 0.41290622\n",
      "Iteration 193, loss = 0.41122444\n",
      "Iteration 194, loss = 0.40959232\n",
      "Iteration 195, loss = 0.40784487\n",
      "Iteration 196, loss = 0.40628573\n",
      "Iteration 197, loss = 0.40471721\n",
      "Iteration 198, loss = 0.40309359\n",
      "Iteration 199, loss = 0.40147919\n",
      "Iteration 200, loss = 0.39991787\n",
      "Iteration 201, loss = 0.39836712\n",
      "Iteration 202, loss = 0.39675524\n",
      "Iteration 203, loss = 0.39530295\n",
      "Iteration 204, loss = 0.39365346\n",
      "Iteration 205, loss = 0.39209668\n",
      "Iteration 206, loss = 0.39065420\n",
      "Iteration 207, loss = 0.38909864\n",
      "Iteration 208, loss = 0.38752580\n",
      "Iteration 209, loss = 0.38612419\n",
      "Iteration 210, loss = 0.38457215\n",
      "Iteration 211, loss = 0.38306786\n",
      "Iteration 212, loss = 0.38158330\n",
      "Iteration 213, loss = 0.38006753\n",
      "Iteration 214, loss = 0.37866745\n",
      "Iteration 215, loss = 0.37722696\n",
      "Iteration 216, loss = 0.37574531\n",
      "Iteration 217, loss = 0.37425392\n",
      "Iteration 218, loss = 0.37290718\n",
      "Iteration 219, loss = 0.37133862\n",
      "Iteration 220, loss = 0.37014034\n",
      "Iteration 221, loss = 0.36856317\n",
      "Iteration 222, loss = 0.36714946\n",
      "Iteration 223, loss = 0.36587203\n",
      "Iteration 224, loss = 0.36435156\n",
      "Iteration 225, loss = 0.36296457\n",
      "Iteration 226, loss = 0.36177855\n",
      "Iteration 227, loss = 0.36020968\n",
      "Iteration 228, loss = 0.35891983\n",
      "Iteration 229, loss = 0.35742346\n",
      "Iteration 230, loss = 0.35616805\n",
      "Iteration 231, loss = 0.35478203\n",
      "Iteration 232, loss = 0.35348458\n",
      "Iteration 233, loss = 0.35219704\n",
      "Iteration 234, loss = 0.35073987\n",
      "Iteration 235, loss = 0.34949392\n",
      "Iteration 236, loss = 0.34805263\n",
      "Iteration 237, loss = 0.34682601\n",
      "Iteration 238, loss = 0.34552009\n",
      "Iteration 239, loss = 0.34419765\n",
      "Iteration 240, loss = 0.34298273\n",
      "Iteration 241, loss = 0.34165271\n",
      "Iteration 242, loss = 0.34036539\n",
      "Iteration 243, loss = 0.33913830\n",
      "Iteration 244, loss = 0.33774692\n",
      "Iteration 245, loss = 0.33652872\n",
      "Iteration 246, loss = 0.33515850\n",
      "Iteration 247, loss = 0.33407439\n",
      "Iteration 248, loss = 0.33286694\n",
      "Iteration 249, loss = 0.33153764\n",
      "Iteration 250, loss = 0.33024985\n",
      "Iteration 251, loss = 0.32917680\n",
      "Iteration 252, loss = 0.32781300\n",
      "Iteration 253, loss = 0.32660066\n",
      "Iteration 254, loss = 0.32557375\n",
      "Iteration 255, loss = 0.32410200\n",
      "Iteration 256, loss = 0.32299199\n",
      "Iteration 257, loss = 0.32171567\n",
      "Iteration 258, loss = 0.32049263\n",
      "Iteration 259, loss = 0.31932058\n",
      "Iteration 260, loss = 0.31843168\n",
      "Iteration 261, loss = 0.31695731\n",
      "Iteration 262, loss = 0.31590622\n",
      "Iteration 263, loss = 0.31474251\n",
      "Iteration 264, loss = 0.31349926\n",
      "Iteration 265, loss = 0.31231526\n",
      "Iteration 266, loss = 0.31121424\n",
      "Iteration 267, loss = 0.31004133\n",
      "Iteration 268, loss = 0.30888417\n",
      "Iteration 269, loss = 0.30778303\n",
      "Iteration 270, loss = 0.30669760\n",
      "Iteration 271, loss = 0.30559289\n",
      "Iteration 272, loss = 0.30434337\n",
      "Iteration 273, loss = 0.30317021\n",
      "Iteration 274, loss = 0.30203701\n",
      "Iteration 275, loss = 0.30094386\n",
      "Iteration 276, loss = 0.29989067\n",
      "Iteration 277, loss = 0.29871973\n",
      "Iteration 278, loss = 0.29768089\n",
      "Iteration 279, loss = 0.29650377\n",
      "Iteration 280, loss = 0.29545946\n",
      "Iteration 281, loss = 0.29426871\n",
      "Iteration 282, loss = 0.29326873\n",
      "Iteration 283, loss = 0.29226512\n",
      "Iteration 284, loss = 0.29112347\n",
      "Iteration 285, loss = 0.29012643\n",
      "Iteration 286, loss = 0.28894690\n",
      "Iteration 287, loss = 0.28787461\n",
      "Iteration 288, loss = 0.28694972\n",
      "Iteration 289, loss = 0.28571552\n",
      "Iteration 290, loss = 0.28482843\n",
      "Iteration 291, loss = 0.28370157\n",
      "Iteration 292, loss = 0.28262879\n",
      "Iteration 293, loss = 0.28163420\n",
      "Iteration 294, loss = 0.28061051\n",
      "Iteration 295, loss = 0.27971038\n",
      "Iteration 296, loss = 0.27857693\n",
      "Iteration 297, loss = 0.27750152\n",
      "Iteration 298, loss = 0.27643873\n",
      "Iteration 299, loss = 0.27545400\n",
      "Iteration 300, loss = 0.27445954\n",
      "Iteration 1, loss = 1.48961388\n",
      "Iteration 2, loss = 1.45926958\n",
      "Iteration 3, loss = 1.43251117\n",
      "Iteration 4, loss = 1.41765532\n",
      "Iteration 5, loss = 1.40886899\n",
      "Iteration 6, loss = 1.40482811\n",
      "Iteration 7, loss = 1.40149207\n",
      "Iteration 8, loss = 1.39832949\n",
      "Iteration 9, loss = 1.39514252\n",
      "Iteration 10, loss = 1.39198373\n",
      "Iteration 11, loss = 1.38907255\n",
      "Iteration 12, loss = 1.38630416\n",
      "Iteration 13, loss = 1.38349748\n",
      "Iteration 14, loss = 1.38065426\n",
      "Iteration 15, loss = 1.37795881\n",
      "Iteration 16, loss = 1.37500426\n",
      "Iteration 17, loss = 1.37222681\n",
      "Iteration 18, loss = 1.36884325\n",
      "Iteration 19, loss = 1.36550761\n",
      "Iteration 20, loss = 1.36186486\n",
      "Iteration 21, loss = 1.35783447\n",
      "Iteration 22, loss = 1.35357048\n",
      "Iteration 23, loss = 1.34881746\n",
      "Iteration 24, loss = 1.34348943\n",
      "Iteration 25, loss = 1.33763646\n",
      "Iteration 26, loss = 1.33116744\n",
      "Iteration 27, loss = 1.32414999\n",
      "Iteration 28, loss = 1.31595134\n",
      "Iteration 29, loss = 1.30703112\n",
      "Iteration 30, loss = 1.29716865\n",
      "Iteration 31, loss = 1.28635732\n",
      "Iteration 32, loss = 1.27445606\n",
      "Iteration 33, loss = 1.26167155\n",
      "Iteration 34, loss = 1.24782359\n",
      "Iteration 35, loss = 1.23361085\n",
      "Iteration 36, loss = 1.21801558\n",
      "Iteration 37, loss = 1.20220863\n",
      "Iteration 38, loss = 1.18617080\n",
      "Iteration 39, loss = 1.17001530\n",
      "Iteration 40, loss = 1.15341045\n",
      "Iteration 41, loss = 1.13691700\n",
      "Iteration 42, loss = 1.12046000\n",
      "Iteration 43, loss = 1.10388189\n",
      "Iteration 44, loss = 1.08764214\n",
      "Iteration 45, loss = 1.07164456\n",
      "Iteration 46, loss = 1.05563387\n",
      "Iteration 47, loss = 1.04030946\n",
      "Iteration 48, loss = 1.02518178\n",
      "Iteration 49, loss = 1.01035005\n",
      "Iteration 50, loss = 0.99600777\n",
      "Iteration 51, loss = 0.98213311\n",
      "Iteration 52, loss = 0.96884445\n",
      "Iteration 53, loss = 0.95589545\n",
      "Iteration 54, loss = 0.94336882\n",
      "Iteration 55, loss = 0.93137073\n",
      "Iteration 56, loss = 0.91986923\n",
      "Iteration 57, loss = 0.90855404\n",
      "Iteration 58, loss = 0.89778528\n",
      "Iteration 59, loss = 0.88744708\n",
      "Iteration 60, loss = 0.87738664\n",
      "Iteration 61, loss = 0.86787569\n",
      "Iteration 62, loss = 0.85852282\n",
      "Iteration 63, loss = 0.84978369\n",
      "Iteration 64, loss = 0.84105429\n",
      "Iteration 65, loss = 0.83259988\n",
      "Iteration 66, loss = 0.82457604\n",
      "Iteration 67, loss = 0.81689475\n",
      "Iteration 68, loss = 0.80928478\n",
      "Iteration 69, loss = 0.80201058\n",
      "Iteration 70, loss = 0.79479033\n",
      "Iteration 71, loss = 0.78791623\n",
      "Iteration 72, loss = 0.78126234\n",
      "Iteration 73, loss = 0.77499587\n",
      "Iteration 74, loss = 0.76848947\n",
      "Iteration 75, loss = 0.76221838\n",
      "Iteration 76, loss = 0.75632239\n",
      "Iteration 77, loss = 0.75041801\n",
      "Iteration 78, loss = 0.74471397\n",
      "Iteration 79, loss = 0.73911566\n",
      "Iteration 80, loss = 0.73366986\n",
      "Iteration 81, loss = 0.72841983\n",
      "Iteration 82, loss = 0.72323994\n",
      "Iteration 83, loss = 0.71799630\n",
      "Iteration 84, loss = 0.71295662\n",
      "Iteration 85, loss = 0.70801707\n",
      "Iteration 86, loss = 0.70315174\n",
      "Iteration 87, loss = 0.69844866\n",
      "Iteration 88, loss = 0.69374915\n",
      "Iteration 89, loss = 0.68920489\n",
      "Iteration 90, loss = 0.68472536\n",
      "Iteration 91, loss = 0.68046507\n",
      "Iteration 92, loss = 0.67588091\n",
      "Iteration 93, loss = 0.67159793\n",
      "Iteration 94, loss = 0.66729970\n",
      "Iteration 95, loss = 0.66317767\n",
      "Iteration 96, loss = 0.65909182\n",
      "Iteration 97, loss = 0.65515391\n",
      "Iteration 98, loss = 0.65094904\n",
      "Iteration 99, loss = 0.64696476\n",
      "Iteration 100, loss = 0.64316819\n",
      "Iteration 101, loss = 0.63930006\n",
      "Iteration 102, loss = 0.63545797\n",
      "Iteration 103, loss = 0.63166709\n",
      "Iteration 104, loss = 0.62801148\n",
      "Iteration 105, loss = 0.62438473\n",
      "Iteration 106, loss = 0.62084379\n",
      "Iteration 107, loss = 0.61719008\n",
      "Iteration 108, loss = 0.61371225\n",
      "Iteration 109, loss = 0.61025859\n",
      "Iteration 110, loss = 0.60672822\n",
      "Iteration 111, loss = 0.60342349\n",
      "Iteration 112, loss = 0.60002344\n",
      "Iteration 113, loss = 0.59670668\n",
      "Iteration 114, loss = 0.59346339\n",
      "Iteration 115, loss = 0.59022482\n",
      "Iteration 116, loss = 0.58693233\n",
      "Iteration 117, loss = 0.58375958\n",
      "Iteration 118, loss = 0.58072977\n",
      "Iteration 119, loss = 0.57755070\n",
      "Iteration 120, loss = 0.57442128\n",
      "Iteration 121, loss = 0.57144685\n",
      "Iteration 122, loss = 0.56847661\n",
      "Iteration 123, loss = 0.56551908\n",
      "Iteration 124, loss = 0.56251050\n",
      "Iteration 125, loss = 0.55954885\n",
      "Iteration 126, loss = 0.55681769\n",
      "Iteration 127, loss = 0.55392408\n",
      "Iteration 128, loss = 0.55106589\n",
      "Iteration 129, loss = 0.54819258\n",
      "Iteration 130, loss = 0.54552595\n",
      "Iteration 131, loss = 0.54274504\n",
      "Iteration 132, loss = 0.54002819\n",
      "Iteration 133, loss = 0.53740223\n",
      "Iteration 134, loss = 0.53471151\n",
      "Iteration 135, loss = 0.53209943\n",
      "Iteration 136, loss = 0.52942589\n",
      "Iteration 137, loss = 0.52691724\n",
      "Iteration 138, loss = 0.52426354\n",
      "Iteration 139, loss = 0.52175454\n",
      "Iteration 140, loss = 0.51924201\n",
      "Iteration 141, loss = 0.51683618\n",
      "Iteration 142, loss = 0.51425171\n",
      "Iteration 143, loss = 0.51184453\n",
      "Iteration 144, loss = 0.50942768\n",
      "Iteration 145, loss = 0.50708253\n",
      "Iteration 146, loss = 0.50461664\n",
      "Iteration 147, loss = 0.50229084\n",
      "Iteration 148, loss = 0.50002373\n",
      "Iteration 149, loss = 0.49766738\n",
      "Iteration 150, loss = 0.49542993\n",
      "Iteration 151, loss = 0.49309863\n",
      "Iteration 152, loss = 0.49101819\n",
      "Iteration 153, loss = 0.48866124\n",
      "Iteration 154, loss = 0.48643146\n",
      "Iteration 155, loss = 0.48418710\n",
      "Iteration 156, loss = 0.48205122\n",
      "Iteration 157, loss = 0.47990171\n",
      "Iteration 158, loss = 0.47767561\n",
      "Iteration 159, loss = 0.47565722\n",
      "Iteration 160, loss = 0.47346149\n",
      "Iteration 161, loss = 0.47133519\n",
      "Iteration 162, loss = 0.46928596\n",
      "Iteration 163, loss = 0.46720856\n",
      "Iteration 164, loss = 0.46516953\n",
      "Iteration 165, loss = 0.46316509\n",
      "Iteration 166, loss = 0.46115513\n",
      "Iteration 167, loss = 0.45913938\n",
      "Iteration 168, loss = 0.45716349\n",
      "Iteration 169, loss = 0.45516141\n",
      "Iteration 170, loss = 0.45330116\n",
      "Iteration 171, loss = 0.45123620\n",
      "Iteration 172, loss = 0.44928193\n",
      "Iteration 173, loss = 0.44741099\n",
      "Iteration 174, loss = 0.44554924\n",
      "Iteration 175, loss = 0.44364321\n",
      "Iteration 176, loss = 0.44181613\n",
      "Iteration 177, loss = 0.43996330\n",
      "Iteration 178, loss = 0.43804263\n",
      "Iteration 179, loss = 0.43635332\n",
      "Iteration 180, loss = 0.43438465\n",
      "Iteration 181, loss = 0.43267010\n",
      "Iteration 182, loss = 0.43085716\n",
      "Iteration 183, loss = 0.42894882\n",
      "Iteration 184, loss = 0.42728432\n",
      "Iteration 185, loss = 0.42543504\n",
      "Iteration 186, loss = 0.42371379\n",
      "Iteration 187, loss = 0.42196988\n",
      "Iteration 188, loss = 0.42022512\n",
      "Iteration 189, loss = 0.41859710\n",
      "Iteration 190, loss = 0.41676403\n",
      "Iteration 191, loss = 0.41512422\n",
      "Iteration 192, loss = 0.41343719\n",
      "Iteration 193, loss = 0.41176173\n",
      "Iteration 194, loss = 0.40999806\n",
      "Iteration 195, loss = 0.40844945\n",
      "Iteration 196, loss = 0.40679914\n",
      "Iteration 197, loss = 0.40507644\n",
      "Iteration 198, loss = 0.40346637\n",
      "Iteration 199, loss = 0.40189139\n",
      "Iteration 200, loss = 0.40014275\n",
      "Iteration 201, loss = 0.39865491\n",
      "Iteration 202, loss = 0.39708143\n",
      "Iteration 203, loss = 0.39551943\n",
      "Iteration 204, loss = 0.39380507\n",
      "Iteration 205, loss = 0.39229158\n",
      "Iteration 206, loss = 0.39083319\n",
      "Iteration 207, loss = 0.38931388\n",
      "Iteration 208, loss = 0.38783832\n",
      "Iteration 209, loss = 0.38623898\n",
      "Iteration 210, loss = 0.38455929\n",
      "Iteration 211, loss = 0.38313977\n",
      "Iteration 212, loss = 0.38168090\n",
      "Iteration 213, loss = 0.38024076\n",
      "Iteration 214, loss = 0.37862275\n",
      "Iteration 215, loss = 0.37714279\n",
      "Iteration 216, loss = 0.37566889\n",
      "Iteration 217, loss = 0.37416247\n",
      "Iteration 218, loss = 0.37268294\n",
      "Iteration 219, loss = 0.37124558\n",
      "Iteration 220, loss = 0.36995231\n",
      "Iteration 221, loss = 0.36842321\n",
      "Iteration 222, loss = 0.36691059\n",
      "Iteration 223, loss = 0.36547426\n",
      "Iteration 224, loss = 0.36414368\n",
      "Iteration 225, loss = 0.36267620\n",
      "Iteration 226, loss = 0.36130207\n",
      "Iteration 227, loss = 0.35996701\n",
      "Iteration 228, loss = 0.35849320\n",
      "Iteration 229, loss = 0.35715783\n",
      "Iteration 230, loss = 0.35574594\n",
      "Iteration 231, loss = 0.35432661\n",
      "Iteration 232, loss = 0.35306558\n",
      "Iteration 233, loss = 0.35169547\n",
      "Iteration 234, loss = 0.35035478\n",
      "Iteration 235, loss = 0.34908017\n",
      "Iteration 236, loss = 0.34770374\n",
      "Iteration 237, loss = 0.34642194\n",
      "Iteration 238, loss = 0.34494041\n",
      "Iteration 239, loss = 0.34371621\n",
      "Iteration 240, loss = 0.34247757\n",
      "Iteration 241, loss = 0.34100830\n",
      "Iteration 242, loss = 0.33978025\n",
      "Iteration 243, loss = 0.33850216\n",
      "Iteration 244, loss = 0.33726214\n",
      "Iteration 245, loss = 0.33591496\n",
      "Iteration 246, loss = 0.33463961\n",
      "Iteration 247, loss = 0.33338550\n",
      "Iteration 248, loss = 0.33206353\n",
      "Iteration 249, loss = 0.33080666\n",
      "Iteration 250, loss = 0.32965892\n",
      "Iteration 251, loss = 0.32838994\n",
      "Iteration 252, loss = 0.32718720\n",
      "Iteration 253, loss = 0.32593707\n",
      "Iteration 254, loss = 0.32467316\n",
      "Iteration 255, loss = 0.32345159\n",
      "Iteration 256, loss = 0.32232440\n",
      "Iteration 257, loss = 0.32110002\n",
      "Iteration 258, loss = 0.31983815\n",
      "Iteration 259, loss = 0.31861610\n",
      "Iteration 260, loss = 0.31738168\n",
      "Iteration 261, loss = 0.31626712\n",
      "Iteration 262, loss = 0.31499274\n",
      "Iteration 263, loss = 0.31380748\n",
      "Iteration 264, loss = 0.31270934\n",
      "Iteration 265, loss = 0.31150144\n",
      "Iteration 266, loss = 0.31031796\n",
      "Iteration 267, loss = 0.30927826\n",
      "Iteration 268, loss = 0.30801305\n",
      "Iteration 269, loss = 0.30696597\n",
      "Iteration 270, loss = 0.30579480\n",
      "Iteration 271, loss = 0.30467932\n",
      "Iteration 272, loss = 0.30353070\n",
      "Iteration 273, loss = 0.30229074\n",
      "Iteration 274, loss = 0.30124304\n",
      "Iteration 275, loss = 0.30018343\n",
      "Iteration 276, loss = 0.29896180\n",
      "Iteration 277, loss = 0.29796180\n",
      "Iteration 278, loss = 0.29673392\n",
      "Iteration 279, loss = 0.29593442\n",
      "Iteration 280, loss = 0.29462197\n",
      "Iteration 281, loss = 0.29350033\n",
      "Iteration 282, loss = 0.29237392\n",
      "Iteration 283, loss = 0.29124393\n",
      "Iteration 284, loss = 0.29024787\n",
      "Iteration 285, loss = 0.28910821\n",
      "Iteration 286, loss = 0.28812494\n",
      "Iteration 287, loss = 0.28702016\n",
      "Iteration 288, loss = 0.28594644\n",
      "Iteration 289, loss = 0.28497782\n",
      "Iteration 290, loss = 0.28383616\n",
      "Iteration 291, loss = 0.28292403\n",
      "Iteration 292, loss = 0.28176144\n",
      "Iteration 293, loss = 0.28068630\n",
      "Iteration 294, loss = 0.27965205\n",
      "Iteration 295, loss = 0.27863056\n",
      "Iteration 296, loss = 0.27767067\n",
      "Iteration 297, loss = 0.27656640\n",
      "Iteration 298, loss = 0.27556649\n",
      "Iteration 299, loss = 0.27452810\n",
      "Iteration 300, loss = 0.27358239\n",
      "Iteration 1, loss = 1.43554833\n",
      "Iteration 2, loss = 1.40757063\n",
      "Iteration 3, loss = 1.38719277\n",
      "Iteration 4, loss = 1.37684152\n",
      "Iteration 5, loss = 1.37070087\n",
      "Iteration 6, loss = 1.36533762\n",
      "Iteration 7, loss = 1.35933764\n",
      "Iteration 8, loss = 1.35265502\n",
      "Iteration 9, loss = 1.34561896\n",
      "Iteration 10, loss = 1.33828163\n",
      "Iteration 11, loss = 1.33022587\n",
      "Iteration 12, loss = 1.32201054\n",
      "Iteration 13, loss = 1.31262718\n",
      "Iteration 14, loss = 1.30308595\n",
      "Iteration 15, loss = 1.29240755\n",
      "Iteration 16, loss = 1.28117149\n",
      "Iteration 17, loss = 1.26934648\n",
      "Iteration 18, loss = 1.25684654\n",
      "Iteration 19, loss = 1.24373728\n",
      "Iteration 20, loss = 1.22997132\n",
      "Iteration 21, loss = 1.21571268\n",
      "Iteration 22, loss = 1.20117481\n",
      "Iteration 23, loss = 1.18611343\n",
      "Iteration 24, loss = 1.17100147\n",
      "Iteration 25, loss = 1.15564010\n",
      "Iteration 26, loss = 1.14014932\n",
      "Iteration 27, loss = 1.12466239\n",
      "Iteration 28, loss = 1.10928431\n",
      "Iteration 29, loss = 1.09409925\n",
      "Iteration 30, loss = 1.07891690\n",
      "Iteration 31, loss = 1.06420539\n",
      "Iteration 32, loss = 1.04957176\n",
      "Iteration 33, loss = 1.03557449\n",
      "Iteration 34, loss = 1.02157567\n",
      "Iteration 35, loss = 1.00817489\n",
      "Iteration 36, loss = 0.99515178\n",
      "Iteration 37, loss = 0.98255042\n",
      "Iteration 38, loss = 0.97019540\n",
      "Iteration 39, loss = 0.95835210\n",
      "Iteration 40, loss = 0.94696668\n",
      "Iteration 41, loss = 0.93593617\n",
      "Iteration 42, loss = 0.92528175\n",
      "Iteration 43, loss = 0.91487089\n",
      "Iteration 44, loss = 0.90495967\n",
      "Iteration 45, loss = 0.89536850\n",
      "Iteration 46, loss = 0.88609438\n",
      "Iteration 47, loss = 0.87706560\n",
      "Iteration 48, loss = 0.86845040\n",
      "Iteration 49, loss = 0.86005647\n",
      "Iteration 50, loss = 0.85193558\n",
      "Iteration 51, loss = 0.84398309\n",
      "Iteration 52, loss = 0.83633568\n",
      "Iteration 53, loss = 0.82889254\n",
      "Iteration 54, loss = 0.82183876\n",
      "Iteration 55, loss = 0.81481108\n",
      "Iteration 56, loss = 0.80807734\n",
      "Iteration 57, loss = 0.80131770\n",
      "Iteration 58, loss = 0.79487720\n",
      "Iteration 59, loss = 0.78853956\n",
      "Iteration 60, loss = 0.78245093\n",
      "Iteration 61, loss = 0.77657701\n",
      "Iteration 62, loss = 0.77068386\n",
      "Iteration 63, loss = 0.76494446\n",
      "Iteration 64, loss = 0.75942644\n",
      "Iteration 65, loss = 0.75396553\n",
      "Iteration 66, loss = 0.74872433\n",
      "Iteration 67, loss = 0.74334547\n",
      "Iteration 68, loss = 0.73815060\n",
      "Iteration 69, loss = 0.73315563\n",
      "Iteration 70, loss = 0.72824076\n",
      "Iteration 71, loss = 0.72341083\n",
      "Iteration 72, loss = 0.71865273\n",
      "Iteration 73, loss = 0.71395206\n",
      "Iteration 74, loss = 0.70936582\n",
      "Iteration 75, loss = 0.70478956\n",
      "Iteration 76, loss = 0.70044437\n",
      "Iteration 77, loss = 0.69588200\n",
      "Iteration 78, loss = 0.69156978\n",
      "Iteration 79, loss = 0.68735482\n",
      "Iteration 80, loss = 0.68308394\n",
      "Iteration 81, loss = 0.67889981\n",
      "Iteration 82, loss = 0.67494551\n",
      "Iteration 83, loss = 0.67086458\n",
      "Iteration 84, loss = 0.66685742\n",
      "Iteration 85, loss = 0.66291271\n",
      "Iteration 86, loss = 0.65907712\n",
      "Iteration 87, loss = 0.65528893\n",
      "Iteration 88, loss = 0.65151613\n",
      "Iteration 89, loss = 0.64776259\n",
      "Iteration 90, loss = 0.64417228\n",
      "Iteration 91, loss = 0.64042010\n",
      "Iteration 92, loss = 0.63686776\n",
      "Iteration 93, loss = 0.63328781\n",
      "Iteration 94, loss = 0.62974972\n",
      "Iteration 95, loss = 0.62635456\n",
      "Iteration 96, loss = 0.62295167\n",
      "Iteration 97, loss = 0.61964227\n",
      "Iteration 98, loss = 0.61622999\n",
      "Iteration 99, loss = 0.61294172\n",
      "Iteration 100, loss = 0.60961550\n",
      "Iteration 101, loss = 0.60646181\n",
      "Iteration 102, loss = 0.60322953\n",
      "Iteration 103, loss = 0.60001762\n",
      "Iteration 104, loss = 0.59690439\n",
      "Iteration 105, loss = 0.59390336\n",
      "Iteration 106, loss = 0.59088192\n",
      "Iteration 107, loss = 0.58780885\n",
      "Iteration 108, loss = 0.58485729\n",
      "Iteration 109, loss = 0.58202858\n",
      "Iteration 110, loss = 0.57896464\n",
      "Iteration 111, loss = 0.57612122\n",
      "Iteration 112, loss = 0.57328376\n",
      "Iteration 113, loss = 0.57050700\n",
      "Iteration 114, loss = 0.56766761\n",
      "Iteration 115, loss = 0.56490349\n",
      "Iteration 116, loss = 0.56229405\n",
      "Iteration 117, loss = 0.55958601\n",
      "Iteration 118, loss = 0.55687128\n",
      "Iteration 119, loss = 0.55425548\n",
      "Iteration 120, loss = 0.55166806\n",
      "Iteration 121, loss = 0.54905222\n",
      "Iteration 122, loss = 0.54655666\n",
      "Iteration 123, loss = 0.54396724\n",
      "Iteration 124, loss = 0.54153743\n",
      "Iteration 125, loss = 0.53906402\n",
      "Iteration 126, loss = 0.53664998\n",
      "Iteration 127, loss = 0.53414702\n",
      "Iteration 128, loss = 0.53178766\n",
      "Iteration 129, loss = 0.52943513\n",
      "Iteration 130, loss = 0.52715586\n",
      "Iteration 131, loss = 0.52481626\n",
      "Iteration 132, loss = 0.52247096\n",
      "Iteration 133, loss = 0.52035106\n",
      "Iteration 134, loss = 0.51803107\n",
      "Iteration 135, loss = 0.51570241\n",
      "Iteration 136, loss = 0.51349100\n",
      "Iteration 137, loss = 0.51143522\n",
      "Iteration 138, loss = 0.50919445\n",
      "Iteration 139, loss = 0.50708267\n",
      "Iteration 140, loss = 0.50492889\n",
      "Iteration 141, loss = 0.50283806\n",
      "Iteration 142, loss = 0.50069845\n",
      "Iteration 143, loss = 0.49860697\n",
      "Iteration 144, loss = 0.49662929\n",
      "Iteration 145, loss = 0.49460016\n",
      "Iteration 146, loss = 0.49255445\n",
      "Iteration 147, loss = 0.49060171\n",
      "Iteration 148, loss = 0.48859928\n",
      "Iteration 149, loss = 0.48658751\n",
      "Iteration 150, loss = 0.48468061\n",
      "Iteration 151, loss = 0.48276601\n",
      "Iteration 152, loss = 0.48103289\n",
      "Iteration 153, loss = 0.47893711\n",
      "Iteration 154, loss = 0.47709585\n",
      "Iteration 155, loss = 0.47523310\n",
      "Iteration 156, loss = 0.47336380\n",
      "Iteration 157, loss = 0.47154984\n",
      "Iteration 158, loss = 0.46967087\n",
      "Iteration 159, loss = 0.46793326\n",
      "Iteration 160, loss = 0.46616299\n",
      "Iteration 161, loss = 0.46432611\n",
      "Iteration 162, loss = 0.46261234\n",
      "Iteration 163, loss = 0.46087663\n",
      "Iteration 164, loss = 0.45909978\n",
      "Iteration 165, loss = 0.45736706\n",
      "Iteration 166, loss = 0.45569597\n",
      "Iteration 167, loss = 0.45394987\n",
      "Iteration 168, loss = 0.45227595\n",
      "Iteration 169, loss = 0.45071324\n",
      "Iteration 170, loss = 0.44914741\n",
      "Iteration 171, loss = 0.44724635\n",
      "Iteration 172, loss = 0.44583147\n",
      "Iteration 173, loss = 0.44423876\n",
      "Iteration 174, loss = 0.44241398\n",
      "Iteration 175, loss = 0.44084779\n",
      "Iteration 176, loss = 0.43928325\n",
      "Iteration 177, loss = 0.43762898\n",
      "Iteration 178, loss = 0.43601092\n",
      "Iteration 179, loss = 0.43445393\n",
      "Iteration 180, loss = 0.43289870\n",
      "Iteration 181, loss = 0.43135542\n",
      "Iteration 182, loss = 0.42986485\n",
      "Iteration 183, loss = 0.42832655\n",
      "Iteration 184, loss = 0.42681970\n",
      "Iteration 185, loss = 0.42530149\n",
      "Iteration 186, loss = 0.42378473\n",
      "Iteration 187, loss = 0.42227015\n",
      "Iteration 188, loss = 0.42088625\n",
      "Iteration 189, loss = 0.41934781\n",
      "Iteration 190, loss = 0.41790292\n",
      "Iteration 191, loss = 0.41641160\n",
      "Iteration 192, loss = 0.41500601\n",
      "Iteration 193, loss = 0.41359461\n",
      "Iteration 194, loss = 0.41211963\n",
      "Iteration 195, loss = 0.41072850\n",
      "Iteration 196, loss = 0.40942591\n",
      "Iteration 197, loss = 0.40775702\n",
      "Iteration 198, loss = 0.40650315\n",
      "Iteration 199, loss = 0.40499178\n",
      "Iteration 200, loss = 0.40367290\n",
      "Iteration 201, loss = 0.40221709\n",
      "Iteration 202, loss = 0.40081206\n",
      "Iteration 203, loss = 0.39951846\n",
      "Iteration 204, loss = 0.39804083\n",
      "Iteration 205, loss = 0.39677960\n",
      "Iteration 206, loss = 0.39542455\n",
      "Iteration 207, loss = 0.39404475\n",
      "Iteration 208, loss = 0.39277108\n",
      "Iteration 209, loss = 0.39158589\n",
      "Iteration 210, loss = 0.39003474\n",
      "Iteration 211, loss = 0.38890387\n",
      "Iteration 212, loss = 0.38741394\n",
      "Iteration 213, loss = 0.38606287\n",
      "Iteration 214, loss = 0.38478521\n",
      "Iteration 215, loss = 0.38363420\n",
      "Iteration 216, loss = 0.38218540\n",
      "Iteration 217, loss = 0.38084828\n",
      "Iteration 218, loss = 0.37958675\n",
      "Iteration 219, loss = 0.37832992\n",
      "Iteration 220, loss = 0.37704991\n",
      "Iteration 221, loss = 0.37591406\n",
      "Iteration 222, loss = 0.37451148\n",
      "Iteration 223, loss = 0.37329481\n",
      "Iteration 224, loss = 0.37202244\n",
      "Iteration 225, loss = 0.37070290\n",
      "Iteration 226, loss = 0.36946154\n",
      "Iteration 227, loss = 0.36824046\n",
      "Iteration 228, loss = 0.36694030\n",
      "Iteration 229, loss = 0.36588423\n",
      "Iteration 230, loss = 0.36462039\n",
      "Iteration 231, loss = 0.36327560\n",
      "Iteration 232, loss = 0.36207178\n",
      "Iteration 233, loss = 0.36089085\n",
      "Iteration 234, loss = 0.35962504\n",
      "Iteration 235, loss = 0.35848470\n",
      "Iteration 236, loss = 0.35729673\n",
      "Iteration 237, loss = 0.35602587\n",
      "Iteration 238, loss = 0.35477915\n",
      "Iteration 239, loss = 0.35370866\n",
      "Iteration 240, loss = 0.35238275\n",
      "Iteration 241, loss = 0.35126102\n",
      "Iteration 242, loss = 0.35019565\n",
      "Iteration 243, loss = 0.34885636\n",
      "Iteration 244, loss = 0.34790486\n",
      "Iteration 245, loss = 0.34649898\n",
      "Iteration 246, loss = 0.34543673\n",
      "Iteration 247, loss = 0.34414877\n",
      "Iteration 248, loss = 0.34297795\n",
      "Iteration 249, loss = 0.34187846\n",
      "Iteration 250, loss = 0.34076864\n",
      "Iteration 251, loss = 0.33958102\n",
      "Iteration 252, loss = 0.33848066\n",
      "Iteration 253, loss = 0.33723055\n",
      "Iteration 254, loss = 0.33609663\n",
      "Iteration 255, loss = 0.33491022\n",
      "Iteration 256, loss = 0.33387999\n",
      "Iteration 257, loss = 0.33265183\n",
      "Iteration 258, loss = 0.33158386\n",
      "Iteration 259, loss = 0.33039836\n",
      "Iteration 260, loss = 0.32930650\n",
      "Iteration 261, loss = 0.32818408\n",
      "Iteration 262, loss = 0.32704069\n",
      "Iteration 263, loss = 0.32595443\n",
      "Iteration 264, loss = 0.32482029\n",
      "Iteration 265, loss = 0.32368051\n",
      "Iteration 266, loss = 0.32276847\n",
      "Iteration 267, loss = 0.32149627\n",
      "Iteration 268, loss = 0.32037126\n",
      "Iteration 269, loss = 0.31927833\n",
      "Iteration 270, loss = 0.31821378\n",
      "Iteration 271, loss = 0.31711069\n",
      "Iteration 272, loss = 0.31616298\n",
      "Iteration 273, loss = 0.31491610\n",
      "Iteration 274, loss = 0.31393549\n",
      "Iteration 275, loss = 0.31275718\n",
      "Iteration 276, loss = 0.31163175\n",
      "Iteration 277, loss = 0.31062197\n",
      "Iteration 278, loss = 0.30958427\n",
      "Iteration 279, loss = 0.30848555\n",
      "Iteration 280, loss = 0.30747406\n",
      "Iteration 281, loss = 0.30626489\n",
      "Iteration 282, loss = 0.30522569\n",
      "Iteration 283, loss = 0.30417457\n",
      "Iteration 284, loss = 0.30310584\n",
      "Iteration 285, loss = 0.30201655\n",
      "Iteration 286, loss = 0.30099918\n",
      "Iteration 287, loss = 0.29989544\n",
      "Iteration 288, loss = 0.29883755\n",
      "Iteration 289, loss = 0.29778074\n",
      "Iteration 290, loss = 0.29685374\n",
      "Iteration 291, loss = 0.29568537\n",
      "Iteration 292, loss = 0.29467450\n",
      "Iteration 293, loss = 0.29367038\n",
      "Iteration 294, loss = 0.29264899\n",
      "Iteration 295, loss = 0.29163272\n",
      "Iteration 296, loss = 0.29077095\n",
      "Iteration 297, loss = 0.28961888\n",
      "Iteration 298, loss = 0.28842514\n",
      "Iteration 299, loss = 0.28758208\n",
      "Iteration 300, loss = 0.28647548\n",
      "Iteration 1, loss = 1.54121794\n",
      "Iteration 2, loss = 1.46981682\n",
      "Iteration 3, loss = 1.41542940\n",
      "Iteration 4, loss = 1.38708918\n",
      "Iteration 5, loss = 1.37492155\n",
      "Iteration 6, loss = 1.36928383\n",
      "Iteration 7, loss = 1.36503179\n",
      "Iteration 8, loss = 1.36030902\n",
      "Iteration 9, loss = 1.35511277\n",
      "Iteration 10, loss = 1.34934201\n",
      "Iteration 11, loss = 1.34347606\n",
      "Iteration 12, loss = 1.33699449\n",
      "Iteration 13, loss = 1.33019768\n",
      "Iteration 14, loss = 1.32301353\n",
      "Iteration 15, loss = 1.31524070\n",
      "Iteration 16, loss = 1.30690690\n",
      "Iteration 17, loss = 1.29813500\n",
      "Iteration 18, loss = 1.28877848\n",
      "Iteration 19, loss = 1.27886277\n",
      "Iteration 20, loss = 1.26846549\n",
      "Iteration 21, loss = 1.25772437\n",
      "Iteration 22, loss = 1.24628529\n",
      "Iteration 23, loss = 1.23432331\n",
      "Iteration 24, loss = 1.22228869\n",
      "Iteration 25, loss = 1.20963420\n",
      "Iteration 26, loss = 1.19692574\n",
      "Iteration 27, loss = 1.18385664\n",
      "Iteration 28, loss = 1.17062031\n",
      "Iteration 29, loss = 1.15737383\n",
      "Iteration 30, loss = 1.14401105\n",
      "Iteration 31, loss = 1.13068418\n",
      "Iteration 32, loss = 1.11742623\n",
      "Iteration 33, loss = 1.10439068\n",
      "Iteration 34, loss = 1.09118894\n",
      "Iteration 35, loss = 1.07866460\n",
      "Iteration 36, loss = 1.06592919\n",
      "Iteration 37, loss = 1.05361353\n",
      "Iteration 38, loss = 1.04169422\n",
      "Iteration 39, loss = 1.03005595\n",
      "Iteration 40, loss = 1.01868933\n",
      "Iteration 41, loss = 1.00755581\n",
      "Iteration 42, loss = 0.99693381\n",
      "Iteration 43, loss = 0.98651941\n",
      "Iteration 44, loss = 0.97647686\n",
      "Iteration 45, loss = 0.96684738\n",
      "Iteration 46, loss = 0.95743985\n",
      "Iteration 47, loss = 0.94837411\n",
      "Iteration 48, loss = 0.93954601\n",
      "Iteration 49, loss = 0.93102404\n",
      "Iteration 50, loss = 0.92286991\n",
      "Iteration 51, loss = 0.91495173\n",
      "Iteration 52, loss = 0.90723771\n",
      "Iteration 53, loss = 0.89980023\n",
      "Iteration 54, loss = 0.89249834\n",
      "Iteration 55, loss = 0.88567514\n",
      "Iteration 56, loss = 0.87880838\n",
      "Iteration 57, loss = 0.87232432\n",
      "Iteration 58, loss = 0.86604589\n",
      "Iteration 59, loss = 0.85969651\n",
      "Iteration 60, loss = 0.85357685\n",
      "Iteration 61, loss = 0.84780318\n",
      "Iteration 62, loss = 0.84210236\n",
      "Iteration 63, loss = 0.83646558\n",
      "Iteration 64, loss = 0.83108089\n",
      "Iteration 65, loss = 0.82596481\n",
      "Iteration 66, loss = 0.82067027\n",
      "Iteration 67, loss = 0.81566569\n",
      "Iteration 68, loss = 0.81075182\n",
      "Iteration 69, loss = 0.80594541\n",
      "Iteration 70, loss = 0.80124732\n",
      "Iteration 71, loss = 0.79665376\n",
      "Iteration 72, loss = 0.79209490\n",
      "Iteration 73, loss = 0.78757163\n",
      "Iteration 74, loss = 0.78319919\n",
      "Iteration 75, loss = 0.77899188\n",
      "Iteration 76, loss = 0.77468727\n",
      "Iteration 77, loss = 0.77059321\n",
      "Iteration 78, loss = 0.76642421\n",
      "Iteration 79, loss = 0.76243739\n",
      "Iteration 80, loss = 0.75844568\n",
      "Iteration 81, loss = 0.75447897\n",
      "Iteration 82, loss = 0.75070888\n",
      "Iteration 83, loss = 0.74688670\n",
      "Iteration 84, loss = 0.74305193\n",
      "Iteration 85, loss = 0.73928172\n",
      "Iteration 86, loss = 0.73580792\n",
      "Iteration 87, loss = 0.73209400\n",
      "Iteration 88, loss = 0.72836375\n",
      "Iteration 89, loss = 0.72471835\n",
      "Iteration 90, loss = 0.72123295\n",
      "Iteration 91, loss = 0.71770844\n",
      "Iteration 92, loss = 0.71427839\n",
      "Iteration 93, loss = 0.71071100\n",
      "Iteration 94, loss = 0.70735207\n",
      "Iteration 95, loss = 0.70390050\n",
      "Iteration 96, loss = 0.70054549\n",
      "Iteration 97, loss = 0.69730554\n",
      "Iteration 98, loss = 0.69382761\n",
      "Iteration 99, loss = 0.69069164\n",
      "Iteration 100, loss = 0.68738521\n",
      "Iteration 101, loss = 0.68415901\n",
      "Iteration 102, loss = 0.68098745\n",
      "Iteration 103, loss = 0.67750028\n",
      "Iteration 104, loss = 0.67433096\n",
      "Iteration 105, loss = 0.67107533\n",
      "Iteration 106, loss = 0.66798516\n",
      "Iteration 107, loss = 0.66473864\n",
      "Iteration 108, loss = 0.66176992\n",
      "Iteration 109, loss = 0.65847310\n",
      "Iteration 110, loss = 0.65543542\n",
      "Iteration 111, loss = 0.65235583\n",
      "Iteration 112, loss = 0.64917475\n",
      "Iteration 113, loss = 0.64624132\n",
      "Iteration 114, loss = 0.64344874\n",
      "Iteration 115, loss = 0.64005852\n",
      "Iteration 116, loss = 0.63714891\n",
      "Iteration 117, loss = 0.63409900\n",
      "Iteration 118, loss = 0.63116345\n",
      "Iteration 119, loss = 0.62814581\n",
      "Iteration 120, loss = 0.62518865\n",
      "Iteration 121, loss = 0.62221002\n",
      "Iteration 122, loss = 0.61957914\n",
      "Iteration 123, loss = 0.61659411\n",
      "Iteration 124, loss = 0.61355294\n",
      "Iteration 125, loss = 0.61088375\n",
      "Iteration 126, loss = 0.60792386\n",
      "Iteration 127, loss = 0.60509782\n",
      "Iteration 128, loss = 0.60219384\n",
      "Iteration 129, loss = 0.59956304\n",
      "Iteration 130, loss = 0.59659617\n",
      "Iteration 131, loss = 0.59391609\n",
      "Iteration 132, loss = 0.59107066\n",
      "Iteration 133, loss = 0.58828842\n",
      "Iteration 134, loss = 0.58564139\n",
      "Iteration 135, loss = 0.58298940\n",
      "Iteration 136, loss = 0.58020177\n",
      "Iteration 137, loss = 0.57753192\n",
      "Iteration 138, loss = 0.57482744\n",
      "Iteration 139, loss = 0.57217701\n",
      "Iteration 140, loss = 0.56957566\n",
      "Iteration 141, loss = 0.56698071\n",
      "Iteration 142, loss = 0.56433674\n",
      "Iteration 143, loss = 0.56178108\n",
      "Iteration 144, loss = 0.55920274\n",
      "Iteration 145, loss = 0.55694150\n",
      "Iteration 146, loss = 0.55417839\n",
      "Iteration 147, loss = 0.55165332\n",
      "Iteration 148, loss = 0.54917660\n",
      "Iteration 149, loss = 0.54665454\n",
      "Iteration 150, loss = 0.54422566\n",
      "Iteration 151, loss = 0.54182572\n",
      "Iteration 152, loss = 0.53925315\n",
      "Iteration 153, loss = 0.53681935\n",
      "Iteration 154, loss = 0.53439878\n",
      "Iteration 155, loss = 0.53191874\n",
      "Iteration 156, loss = 0.52975984\n",
      "Iteration 157, loss = 0.52736161\n",
      "Iteration 158, loss = 0.52485431\n",
      "Iteration 159, loss = 0.52263792\n",
      "Iteration 160, loss = 0.52043410\n",
      "Iteration 161, loss = 0.51781657\n",
      "Iteration 162, loss = 0.51553463\n",
      "Iteration 163, loss = 0.51330610\n",
      "Iteration 164, loss = 0.51117687\n",
      "Iteration 165, loss = 0.50873933\n",
      "Iteration 166, loss = 0.50650021\n",
      "Iteration 167, loss = 0.50423812\n",
      "Iteration 168, loss = 0.50204698\n",
      "Iteration 169, loss = 0.49978512\n",
      "Iteration 170, loss = 0.49760479\n",
      "Iteration 171, loss = 0.49544030\n",
      "Iteration 172, loss = 0.49319343\n",
      "Iteration 173, loss = 0.49128142\n",
      "Iteration 174, loss = 0.48906683\n",
      "Iteration 175, loss = 0.48709103\n",
      "Iteration 176, loss = 0.48489705\n",
      "Iteration 177, loss = 0.48273780\n",
      "Iteration 178, loss = 0.48057805\n",
      "Iteration 179, loss = 0.47871203\n",
      "Iteration 180, loss = 0.47644058\n",
      "Iteration 181, loss = 0.47455308\n",
      "Iteration 182, loss = 0.47236358\n",
      "Iteration 183, loss = 0.47039842\n",
      "Iteration 184, loss = 0.46850270\n",
      "Iteration 185, loss = 0.46649658\n",
      "Iteration 186, loss = 0.46445411\n",
      "Iteration 187, loss = 0.46249312\n",
      "Iteration 188, loss = 0.46044077\n",
      "Iteration 189, loss = 0.45853610\n",
      "Iteration 190, loss = 0.45647298\n",
      "Iteration 191, loss = 0.45458892\n",
      "Iteration 192, loss = 0.45263004\n",
      "Iteration 193, loss = 0.45073551\n",
      "Iteration 194, loss = 0.44880809\n",
      "Iteration 195, loss = 0.44692392\n",
      "Iteration 196, loss = 0.44513783\n",
      "Iteration 197, loss = 0.44326798\n",
      "Iteration 198, loss = 0.44163760\n",
      "Iteration 199, loss = 0.43948205\n",
      "Iteration 200, loss = 0.43765102\n",
      "Iteration 201, loss = 0.43594657\n",
      "Iteration 202, loss = 0.43408354\n",
      "Iteration 203, loss = 0.43233994\n",
      "Iteration 204, loss = 0.43048685\n",
      "Iteration 205, loss = 0.42877241\n",
      "Iteration 206, loss = 0.42692757\n",
      "Iteration 207, loss = 0.42509430\n",
      "Iteration 208, loss = 0.42355110\n",
      "Iteration 209, loss = 0.42163942\n",
      "Iteration 210, loss = 0.41997875\n",
      "Iteration 211, loss = 0.41827765\n",
      "Iteration 212, loss = 0.41645663\n",
      "Iteration 213, loss = 0.41465763\n",
      "Iteration 214, loss = 0.41296683\n",
      "Iteration 215, loss = 0.41136676\n",
      "Iteration 216, loss = 0.40977578\n",
      "Iteration 217, loss = 0.40795273\n",
      "Iteration 218, loss = 0.40631318\n",
      "Iteration 219, loss = 0.40456710\n",
      "Iteration 220, loss = 0.40295831\n",
      "Iteration 221, loss = 0.40160773\n",
      "Iteration 222, loss = 0.39976333\n",
      "Iteration 223, loss = 0.39807298\n",
      "Iteration 224, loss = 0.39653644\n",
      "Iteration 225, loss = 0.39480874\n",
      "Iteration 226, loss = 0.39334901\n",
      "Iteration 227, loss = 0.39189644\n",
      "Iteration 228, loss = 0.39021966\n",
      "Iteration 229, loss = 0.38849300\n",
      "Iteration 230, loss = 0.38695491\n",
      "Iteration 231, loss = 0.38544157\n",
      "Iteration 232, loss = 0.38397382\n",
      "Iteration 233, loss = 0.38225319\n",
      "Iteration 234, loss = 0.38082743\n",
      "Iteration 235, loss = 0.37919676\n",
      "Iteration 236, loss = 0.37784167\n",
      "Iteration 237, loss = 0.37626232\n",
      "Iteration 238, loss = 0.37488006\n",
      "Iteration 239, loss = 0.37322437\n",
      "Iteration 240, loss = 0.37179690\n",
      "Iteration 241, loss = 0.37050138\n",
      "Iteration 242, loss = 0.36885787\n",
      "Iteration 243, loss = 0.36719068\n",
      "Iteration 244, loss = 0.36578137\n",
      "Iteration 245, loss = 0.36438258\n",
      "Iteration 246, loss = 0.36285516\n",
      "Iteration 247, loss = 0.36141135\n",
      "Iteration 248, loss = 0.35999895\n",
      "Iteration 249, loss = 0.35850085\n",
      "Iteration 250, loss = 0.35710662\n",
      "Iteration 251, loss = 0.35566615\n",
      "Iteration 252, loss = 0.35439831\n",
      "Iteration 253, loss = 0.35281241\n",
      "Iteration 254, loss = 0.35146025\n",
      "Iteration 255, loss = 0.35010266\n",
      "Iteration 256, loss = 0.34865231\n",
      "Iteration 257, loss = 0.34728584\n",
      "Iteration 258, loss = 0.34605320\n",
      "Iteration 259, loss = 0.34464919\n",
      "Iteration 260, loss = 0.34319927\n",
      "Iteration 261, loss = 0.34178387\n",
      "Iteration 262, loss = 0.34049807\n",
      "Iteration 263, loss = 0.33915226\n",
      "Iteration 264, loss = 0.33769446\n",
      "Iteration 265, loss = 0.33637084\n",
      "Iteration 266, loss = 0.33526144\n",
      "Iteration 267, loss = 0.33375269\n",
      "Iteration 268, loss = 0.33242886\n",
      "Iteration 269, loss = 0.33109040\n",
      "Iteration 270, loss = 0.32975001\n",
      "Iteration 271, loss = 0.32844602\n",
      "Iteration 272, loss = 0.32713628\n",
      "Iteration 273, loss = 0.32580700\n",
      "Iteration 274, loss = 0.32471609\n",
      "Iteration 275, loss = 0.32327007\n",
      "Iteration 276, loss = 0.32193047\n",
      "Iteration 277, loss = 0.32077501\n",
      "Iteration 278, loss = 0.31942720\n",
      "Iteration 279, loss = 0.31816716\n",
      "Iteration 280, loss = 0.31692131\n",
      "Iteration 281, loss = 0.31562011\n",
      "Iteration 282, loss = 0.31441450\n",
      "Iteration 283, loss = 0.31321501\n",
      "Iteration 284, loss = 0.31190583\n",
      "Iteration 285, loss = 0.31063543\n",
      "Iteration 286, loss = 0.30931832\n",
      "Iteration 287, loss = 0.30819330\n",
      "Iteration 288, loss = 0.30685331\n",
      "Iteration 289, loss = 0.30584491\n",
      "Iteration 290, loss = 0.30460779\n",
      "Iteration 291, loss = 0.30325164\n",
      "Iteration 292, loss = 0.30206624\n",
      "Iteration 293, loss = 0.30094820\n",
      "Iteration 294, loss = 0.29958135\n",
      "Iteration 295, loss = 0.29843955\n",
      "Iteration 296, loss = 0.29721224\n",
      "Iteration 297, loss = 0.29602730\n",
      "Iteration 298, loss = 0.29479273\n",
      "Iteration 299, loss = 0.29367323\n",
      "Iteration 300, loss = 0.29248177\n",
      "Iteration 1, loss = 1.47125747\n",
      "Iteration 2, loss = 1.44272456\n",
      "Iteration 3, loss = 1.41389433\n",
      "Iteration 4, loss = 1.39306937\n",
      "Iteration 5, loss = 1.38072056\n",
      "Iteration 6, loss = 1.37079259\n",
      "Iteration 7, loss = 1.36198352\n",
      "Iteration 8, loss = 1.35425751\n",
      "Iteration 9, loss = 1.34652693\n",
      "Iteration 10, loss = 1.33837355\n",
      "Iteration 11, loss = 1.32998844\n",
      "Iteration 12, loss = 1.32121582\n",
      "Iteration 13, loss = 1.31198669\n",
      "Iteration 14, loss = 1.30225671\n",
      "Iteration 15, loss = 1.29178547\n",
      "Iteration 16, loss = 1.28083037\n",
      "Iteration 17, loss = 1.26892562\n",
      "Iteration 18, loss = 1.25673883\n",
      "Iteration 19, loss = 1.24379193\n",
      "Iteration 20, loss = 1.23014264\n",
      "Iteration 21, loss = 1.21625873\n",
      "Iteration 22, loss = 1.20200942\n",
      "Iteration 23, loss = 1.18733102\n",
      "Iteration 24, loss = 1.17259109\n",
      "Iteration 25, loss = 1.15767435\n",
      "Iteration 26, loss = 1.14289160\n",
      "Iteration 27, loss = 1.12809537\n",
      "Iteration 28, loss = 1.11352000\n",
      "Iteration 29, loss = 1.09898041\n",
      "Iteration 30, loss = 1.08502044\n",
      "Iteration 31, loss = 1.07110572\n",
      "Iteration 32, loss = 1.05747467\n",
      "Iteration 33, loss = 1.04441847\n",
      "Iteration 34, loss = 1.03168859\n",
      "Iteration 35, loss = 1.01932600\n",
      "Iteration 36, loss = 1.00739238\n",
      "Iteration 37, loss = 0.99588812\n",
      "Iteration 38, loss = 0.98487637\n",
      "Iteration 39, loss = 0.97401940\n",
      "Iteration 40, loss = 0.96375504\n",
      "Iteration 41, loss = 0.95374378\n",
      "Iteration 42, loss = 0.94416138\n",
      "Iteration 43, loss = 0.93481013\n",
      "Iteration 44, loss = 0.92592683\n",
      "Iteration 45, loss = 0.91732633\n",
      "Iteration 46, loss = 0.90903844\n",
      "Iteration 47, loss = 0.90098818\n",
      "Iteration 48, loss = 0.89321000\n",
      "Iteration 49, loss = 0.88574257\n",
      "Iteration 50, loss = 0.87851913\n",
      "Iteration 51, loss = 0.87141377\n",
      "Iteration 52, loss = 0.86472329\n",
      "Iteration 53, loss = 0.85808945\n",
      "Iteration 54, loss = 0.85168803\n",
      "Iteration 55, loss = 0.84548879\n",
      "Iteration 56, loss = 0.83946118\n",
      "Iteration 57, loss = 0.83366767\n",
      "Iteration 58, loss = 0.82795082\n",
      "Iteration 59, loss = 0.82241939\n",
      "Iteration 60, loss = 0.81698095\n",
      "Iteration 61, loss = 0.81170852\n",
      "Iteration 62, loss = 0.80652126\n",
      "Iteration 63, loss = 0.80156408\n",
      "Iteration 64, loss = 0.79661012\n",
      "Iteration 65, loss = 0.79183622\n",
      "Iteration 66, loss = 0.78724495\n",
      "Iteration 67, loss = 0.78256836\n",
      "Iteration 68, loss = 0.77802897\n",
      "Iteration 69, loss = 0.77358595\n",
      "Iteration 70, loss = 0.76929039\n",
      "Iteration 71, loss = 0.76501377\n",
      "Iteration 72, loss = 0.76081943\n",
      "Iteration 73, loss = 0.75670462\n",
      "Iteration 74, loss = 0.75263046\n",
      "Iteration 75, loss = 0.74863821\n",
      "Iteration 76, loss = 0.74465350\n",
      "Iteration 77, loss = 0.74098505\n",
      "Iteration 78, loss = 0.73696958\n",
      "Iteration 79, loss = 0.73327132\n",
      "Iteration 80, loss = 0.72953543\n",
      "Iteration 81, loss = 0.72582981\n",
      "Iteration 82, loss = 0.72216455\n",
      "Iteration 83, loss = 0.71860257\n",
      "Iteration 84, loss = 0.71509649\n",
      "Iteration 85, loss = 0.71157299\n",
      "Iteration 86, loss = 0.70811228\n",
      "Iteration 87, loss = 0.70466677\n",
      "Iteration 88, loss = 0.70125076\n",
      "Iteration 89, loss = 0.69797028\n",
      "Iteration 90, loss = 0.69467521\n",
      "Iteration 91, loss = 0.69136102\n",
      "Iteration 92, loss = 0.68802660\n",
      "Iteration 93, loss = 0.68481660\n",
      "Iteration 94, loss = 0.68157512\n",
      "Iteration 95, loss = 0.67847853\n",
      "Iteration 96, loss = 0.67525334\n",
      "Iteration 97, loss = 0.67226602\n",
      "Iteration 98, loss = 0.66905853\n",
      "Iteration 99, loss = 0.66606327\n",
      "Iteration 100, loss = 0.66299457\n",
      "Iteration 101, loss = 0.65998976\n",
      "Iteration 102, loss = 0.65697284\n",
      "Iteration 103, loss = 0.65405063\n",
      "Iteration 104, loss = 0.65114128\n",
      "Iteration 105, loss = 0.64825175\n",
      "Iteration 106, loss = 0.64523916\n",
      "Iteration 107, loss = 0.64247877\n",
      "Iteration 108, loss = 0.63955325\n",
      "Iteration 109, loss = 0.63678691\n",
      "Iteration 110, loss = 0.63395805\n",
      "Iteration 111, loss = 0.63122821\n",
      "Iteration 112, loss = 0.62848593\n",
      "Iteration 113, loss = 0.62572825\n",
      "Iteration 114, loss = 0.62310883\n",
      "Iteration 115, loss = 0.62032658\n",
      "Iteration 116, loss = 0.61768509\n",
      "Iteration 117, loss = 0.61513936\n",
      "Iteration 118, loss = 0.61246329\n",
      "Iteration 119, loss = 0.60983375\n",
      "Iteration 120, loss = 0.60730231\n",
      "Iteration 121, loss = 0.60477058\n",
      "Iteration 122, loss = 0.60223231\n",
      "Iteration 123, loss = 0.59979482\n",
      "Iteration 124, loss = 0.59721405\n",
      "Iteration 125, loss = 0.59477853\n",
      "Iteration 126, loss = 0.59227999\n",
      "Iteration 127, loss = 0.58984137\n",
      "Iteration 128, loss = 0.58744312\n",
      "Iteration 129, loss = 0.58507499\n",
      "Iteration 130, loss = 0.58282052\n",
      "Iteration 131, loss = 0.58032421\n",
      "Iteration 132, loss = 0.57802796\n",
      "Iteration 133, loss = 0.57570790\n",
      "Iteration 134, loss = 0.57339770\n",
      "Iteration 135, loss = 0.57113253\n",
      "Iteration 136, loss = 0.56882756\n",
      "Iteration 137, loss = 0.56655543\n",
      "Iteration 138, loss = 0.56440059\n",
      "Iteration 139, loss = 0.56219164\n",
      "Iteration 140, loss = 0.56001993\n",
      "Iteration 141, loss = 0.55773749\n",
      "Iteration 142, loss = 0.55573713\n",
      "Iteration 143, loss = 0.55340772\n",
      "Iteration 144, loss = 0.55136397\n",
      "Iteration 145, loss = 0.54929573\n",
      "Iteration 146, loss = 0.54713231\n",
      "Iteration 147, loss = 0.54501289\n",
      "Iteration 148, loss = 0.54296291\n",
      "Iteration 149, loss = 0.54092649\n",
      "Iteration 150, loss = 0.53887251\n",
      "Iteration 151, loss = 0.53681806\n",
      "Iteration 152, loss = 0.53498529\n",
      "Iteration 153, loss = 0.53288338\n",
      "Iteration 154, loss = 0.53090247\n",
      "Iteration 155, loss = 0.52887432\n",
      "Iteration 156, loss = 0.52696399\n",
      "Iteration 157, loss = 0.52504300\n",
      "Iteration 158, loss = 0.52308532\n",
      "Iteration 159, loss = 0.52113363\n",
      "Iteration 160, loss = 0.51925445\n",
      "Iteration 161, loss = 0.51735876\n",
      "Iteration 162, loss = 0.51558819\n",
      "Iteration 163, loss = 0.51358448\n",
      "Iteration 164, loss = 0.51175932\n",
      "Iteration 165, loss = 0.51004499\n",
      "Iteration 166, loss = 0.50811385\n",
      "Iteration 167, loss = 0.50628459\n",
      "Iteration 168, loss = 0.50452082\n",
      "Iteration 169, loss = 0.50268741\n",
      "Iteration 170, loss = 0.50101781\n",
      "Iteration 171, loss = 0.49912722\n",
      "Iteration 172, loss = 0.49748471\n",
      "Iteration 173, loss = 0.49562340\n",
      "Iteration 174, loss = 0.49398733\n",
      "Iteration 175, loss = 0.49230500\n",
      "Iteration 176, loss = 0.49047059\n",
      "Iteration 177, loss = 0.48883531\n",
      "Iteration 178, loss = 0.48714133\n",
      "Iteration 179, loss = 0.48542960\n",
      "Iteration 180, loss = 0.48376924\n",
      "Iteration 181, loss = 0.48211332\n",
      "Iteration 182, loss = 0.48045738\n",
      "Iteration 183, loss = 0.47875337\n",
      "Iteration 184, loss = 0.47714029\n",
      "Iteration 185, loss = 0.47557883\n",
      "Iteration 186, loss = 0.47402169\n",
      "Iteration 187, loss = 0.47228053\n",
      "Iteration 188, loss = 0.47072567\n",
      "Iteration 189, loss = 0.46910887\n",
      "Iteration 190, loss = 0.46760438\n",
      "Iteration 191, loss = 0.46601523\n",
      "Iteration 192, loss = 0.46441829\n",
      "Iteration 193, loss = 0.46288556\n",
      "Iteration 194, loss = 0.46136362\n",
      "Iteration 195, loss = 0.45984761\n",
      "Iteration 196, loss = 0.45828414\n",
      "Iteration 197, loss = 0.45679826\n",
      "Iteration 198, loss = 0.45529545\n",
      "Iteration 199, loss = 0.45380622\n",
      "Iteration 200, loss = 0.45228016\n",
      "Iteration 201, loss = 0.45103560\n",
      "Iteration 202, loss = 0.44936811\n",
      "Iteration 203, loss = 0.44794853\n",
      "Iteration 204, loss = 0.44655780\n",
      "Iteration 205, loss = 0.44509911\n",
      "Iteration 206, loss = 0.44367667\n",
      "Iteration 207, loss = 0.44209625\n",
      "Iteration 208, loss = 0.44069755\n",
      "Iteration 209, loss = 0.43922638\n",
      "Iteration 210, loss = 0.43784044\n",
      "Iteration 211, loss = 0.43645964\n",
      "Iteration 212, loss = 0.43510729\n",
      "Iteration 213, loss = 0.43370536\n",
      "Iteration 214, loss = 0.43234612\n",
      "Iteration 215, loss = 0.43094841\n",
      "Iteration 216, loss = 0.42960223\n",
      "Iteration 217, loss = 0.42820962\n",
      "Iteration 218, loss = 0.42683295\n",
      "Iteration 219, loss = 0.42561829\n",
      "Iteration 220, loss = 0.42414845\n",
      "Iteration 221, loss = 0.42272839\n",
      "Iteration 222, loss = 0.42143664\n",
      "Iteration 223, loss = 0.42025621\n",
      "Iteration 224, loss = 0.41881777\n",
      "Iteration 225, loss = 0.41761503\n",
      "Iteration 226, loss = 0.41617053\n",
      "Iteration 227, loss = 0.41491513\n",
      "Iteration 228, loss = 0.41362835\n",
      "Iteration 229, loss = 0.41235210\n",
      "Iteration 230, loss = 0.41097279\n",
      "Iteration 231, loss = 0.40971125\n",
      "Iteration 232, loss = 0.40849341\n",
      "Iteration 233, loss = 0.40728678\n",
      "Iteration 234, loss = 0.40590411\n",
      "Iteration 235, loss = 0.40477704\n",
      "Iteration 236, loss = 0.40343428\n",
      "Iteration 237, loss = 0.40213850\n",
      "Iteration 238, loss = 0.40098810\n",
      "Iteration 239, loss = 0.39979296\n",
      "Iteration 240, loss = 0.39849116\n",
      "Iteration 241, loss = 0.39725422\n",
      "Iteration 242, loss = 0.39601767\n",
      "Iteration 243, loss = 0.39481242\n",
      "Iteration 244, loss = 0.39357279\n",
      "Iteration 245, loss = 0.39245110\n",
      "Iteration 246, loss = 0.39128481\n",
      "Iteration 247, loss = 0.39005144\n",
      "Iteration 248, loss = 0.38885441\n",
      "Iteration 249, loss = 0.38767850\n",
      "Iteration 250, loss = 0.38652054\n",
      "Iteration 251, loss = 0.38530570\n",
      "Iteration 252, loss = 0.38412129\n",
      "Iteration 253, loss = 0.38306609\n",
      "Iteration 254, loss = 0.38210162\n",
      "Iteration 255, loss = 0.38073752\n",
      "Iteration 256, loss = 0.37949047\n",
      "Iteration 257, loss = 0.37835804\n",
      "Iteration 258, loss = 0.37721076\n",
      "Iteration 259, loss = 0.37604673\n",
      "Iteration 260, loss = 0.37510402\n",
      "Iteration 261, loss = 0.37381406\n",
      "Iteration 262, loss = 0.37270455\n",
      "Iteration 263, loss = 0.37156423\n",
      "Iteration 264, loss = 0.37051027\n",
      "Iteration 265, loss = 0.36928749\n",
      "Iteration 266, loss = 0.36822522\n",
      "Iteration 267, loss = 0.36714118\n",
      "Iteration 268, loss = 0.36607938\n",
      "Iteration 269, loss = 0.36496371\n",
      "Iteration 270, loss = 0.36386575\n",
      "Iteration 271, loss = 0.36278918\n",
      "Iteration 272, loss = 0.36184238\n",
      "Iteration 273, loss = 0.36087294\n",
      "Iteration 274, loss = 0.35956280\n",
      "Iteration 275, loss = 0.35856945\n",
      "Iteration 276, loss = 0.35741477\n",
      "Iteration 277, loss = 0.35630019\n",
      "Iteration 278, loss = 0.35525490\n",
      "Iteration 279, loss = 0.35424764\n",
      "Iteration 280, loss = 0.35312680\n",
      "Iteration 281, loss = 0.35217538\n",
      "Iteration 282, loss = 0.35131182\n",
      "Iteration 283, loss = 0.35003987\n",
      "Iteration 284, loss = 0.34890801\n",
      "Iteration 285, loss = 0.34796344\n",
      "Iteration 286, loss = 0.34681598\n",
      "Iteration 287, loss = 0.34596483\n",
      "Iteration 288, loss = 0.34482845\n",
      "Iteration 289, loss = 0.34378584\n",
      "Iteration 290, loss = 0.34276000\n",
      "Iteration 291, loss = 0.34179384\n",
      "Iteration 292, loss = 0.34067179\n",
      "Iteration 293, loss = 0.33979852\n",
      "Iteration 294, loss = 0.33876857\n",
      "Iteration 295, loss = 0.33778442\n",
      "Iteration 296, loss = 0.33668549\n",
      "Iteration 297, loss = 0.33582853\n",
      "Iteration 298, loss = 0.33474181\n",
      "Iteration 299, loss = 0.33366823\n",
      "Iteration 300, loss = 0.33297481\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(monkeysValues, monkeysClasses):\n",
    "    data_train, target_train = monkeysValues[train], monkeysClasses[train]\n",
    "    data_test, target_test = monkeysValues[test], monkeysClasses[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       428\n",
      "           2       1.00      1.00      1.00       426\n",
      "           3       1.00      1.00      1.00       432\n",
      "           4       1.00      1.00      1.00       426\n",
      "\n",
      "    accuracy                           1.00      1712\n",
      "   macro avg       1.00      1.00      1.00      1712\n",
      "weighted avg       1.00      1.00      1.00      1712\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[428   0   0   0]\n",
      " [  0 426   0   0]\n",
      " [  0   0 432   0]\n",
      " [  0   0   0 426]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.99      0.98       428\n",
      "           2       0.98      0.98      0.98       426\n",
      "           3       0.99      0.99      0.99       432\n",
      "           4       1.00      0.99      0.99       426\n",
      "\n",
      "    accuracy                           0.99      1712\n",
      "   macro avg       0.99      0.99      0.99      1712\n",
      "weighted avg       0.99      0.99      0.99      1712\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[424   4   0   0]\n",
      " [ 10 416   0   0]\n",
      " [  0   4 426   2]\n",
      " [  0   0   5 421]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       428\n",
      "           2       1.00      1.00      1.00       426\n",
      "           3       1.00      1.00      1.00       432\n",
      "           4       1.00      1.00      1.00       426\n",
      "\n",
      "    accuracy                           1.00      1712\n",
      "   macro avg       1.00      1.00      1.00      1712\n",
      "weighted avg       1.00      1.00      1.00      1712\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[428   0   0   0]\n",
      " [  0 426   0   0]\n",
      " [  0   0 432   0]\n",
      " [  0   0   0 426]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       428\n",
      "           2       1.00      1.00      1.00       426\n",
      "           3       1.00      1.00      1.00       432\n",
      "           4       1.00      1.00      1.00       426\n",
      "\n",
      "    accuracy                           1.00      1712\n",
      "   macro avg       1.00      1.00      1.00      1712\n",
      "weighted avg       1.00      1.00      1.00      1712\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[428   0   0   0]\n",
      " [  0 426   0   0]\n",
      " [  0   0 432   0]\n",
      " [  0   0   0 426]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      0.99       428\n",
      "           2       0.96      0.95      0.96       426\n",
      "           3       0.91      0.84      0.87       432\n",
      "           4       0.88      0.95      0.91       426\n",
      "\n",
      "    accuracy                           0.93      1712\n",
      "   macro avg       0.93      0.93      0.93      1712\n",
      "weighted avg       0.93      0.93      0.93      1712\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[426   2   0   0]\n",
      " [  3 405  18   0]\n",
      " [  0  13 362  57]\n",
      " [  0   0  20 406]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(monkeysClasses, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusão: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(monkeysClasses, predicted_classes[classificador]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}