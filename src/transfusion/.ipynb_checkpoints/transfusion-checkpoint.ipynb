{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Transfusion:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "0                   2                 50                  12500   \n",
       "1                   0                 13                   3250   \n",
       "2                   1                 16                   4000   \n",
       "3                   2                 20                   5000   \n",
       "4                   1                 24                   6000   \n",
       "..                ...                ...                    ...   \n",
       "743                23                  2                    500   \n",
       "744                21                  2                    500   \n",
       "745                23                  3                    750   \n",
       "746                39                  1                    250   \n",
       "747                72                  1                    250   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "0               98                                           1  \n",
       "1               28                                           1  \n",
       "2               35                                           1  \n",
       "3               45                                           1  \n",
       "4               77                                           0  \n",
       "..             ...                                         ...  \n",
       "743             38                                           0  \n",
       "744             52                                           0  \n",
       "745             62                                           0  \n",
       "746             39                                           0  \n",
       "747             72                                           0  \n",
       "\n",
       "[748 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_transfusion = pd.read_table('transfusion.data', sep=',')\n",
    "print(\"\\nDataset Transfusion:\")\n",
    "dataset_transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Transfusion Normalized:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.927899</td>\n",
       "      <td>7.623346</td>\n",
       "      <td>7.623346</td>\n",
       "      <td>2.615633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.175118</td>\n",
       "      <td>1.282738</td>\n",
       "      <td>1.282738</td>\n",
       "      <td>-0.257881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.051508</td>\n",
       "      <td>1.796842</td>\n",
       "      <td>1.796842</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.927899</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>0.439973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.051508</td>\n",
       "      <td>3.167784</td>\n",
       "      <td>3.167784</td>\n",
       "      <td>1.753579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>1.667904</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>0.152621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>1.420685</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>-0.602307</td>\n",
       "      <td>0.727324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>1.667904</td>\n",
       "      <td>-0.430940</td>\n",
       "      <td>-0.430940</td>\n",
       "      <td>1.137826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>3.645659</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>7.724778</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>-0.773675</td>\n",
       "      <td>1.548328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "0           -0.927899           7.623346               7.623346   \n",
       "1           -1.175118           1.282738               1.282738   \n",
       "2           -1.051508           1.796842               1.796842   \n",
       "3           -0.927899           2.482313               2.482313   \n",
       "4           -1.051508           3.167784               3.167784   \n",
       "..                ...                ...                    ...   \n",
       "743          1.667904          -0.602307              -0.602307   \n",
       "744          1.420685          -0.602307              -0.602307   \n",
       "745          1.667904          -0.430940              -0.430940   \n",
       "746          3.645659          -0.773675              -0.773675   \n",
       "747          7.724778          -0.773675              -0.773675   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "0         2.615633                                           1  \n",
       "1        -0.257881                                           1  \n",
       "2         0.029471                                           1  \n",
       "3         0.439973                                           1  \n",
       "4         1.753579                                           0  \n",
       "..             ...                                         ...  \n",
       "743       0.152621                                           0  \n",
       "744       0.727324                                           0  \n",
       "745       1.137826                                           0  \n",
       "746       0.193671                                           0  \n",
       "747       1.548328                                           0  \n",
       "\n",
       "[748 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfusion_normalised = dataset_transfusion.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "transfusion_normalised = normalizador.fit_transform(dataset_transfusion)\n",
    "dataset_transfusion['Recency (months)'] = transfusion_normalised[:,0]\n",
    "dataset_transfusion['Frequency (times)'] = transfusion_normalised[:,1]\n",
    "dataset_transfusion['Monetary (c.c. blood)'] = transfusion_normalised[:,2]\n",
    "dataset_transfusion['Time (months)'] = transfusion_normalised[:,3]\n",
    "\n",
    "print(\"\\nDataset Transfusion Normalized:\")\n",
    "dataset_transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transfusion features:\n",
      "\n",
      "[[-0.92789873  7.62334626  7.62334626  2.61563344]\n",
      " [-1.17511806  1.28273826  1.28273826 -0.2578809 ]\n",
      " [-1.0515084   1.79684161  1.79684161  0.02947053]\n",
      " ...\n",
      " [ 1.66790417 -0.43093957 -0.43093957  1.13782607]\n",
      " [ 3.64565877 -0.77367514 -0.77367514  0.19367135]\n",
      " [ 7.72477762 -0.77367514 -0.77367514  1.54832812]]\n"
     ]
    }
   ],
   "source": [
    "transfusion_values = dataset_transfusion.iloc[:,0:4].values\n",
    "print(\"\\nTransfusion features:\\n\")\n",
    "print(transfusion_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transfusion classes:\n",
      "\n",
      "[1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0\n",
      " 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Transfusion classes shape:\n",
      "(748,)\n"
     ]
    }
   ],
   "source": [
    "transfusion_classes = dataset_transfusion.iloc[:,4].values\n",
    "print(\"\\nTransfusion classes:\\n\")\n",
    "print(transfusion_classes)\n",
    "print(\"\\nTransfusion classes shape:\")\n",
    "print(transfusion_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold_treinamento = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=None) \n",
    "kfold_ajuste_parametros = model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore_decisao = tree.DecisionTreeClassifier()\n",
    "print(\"====== Started Decision Tree parameters tuning\")\n",
    "\n",
    "param_dist = {'max_depth':[3,4,5,6,7,8,9,10]}\n",
    "grid_search = GridSearchCV(arvore_decisao, param_grid=param_dist, cv=kfold_ajuste_parametros, scoring='accuracy', refit=False)\n",
    "grid_search.fit(digits.data, digits.target)\n",
    "decisionTreeBestParams = grid_search.best_params_\n",
    "print(\"Decision Tree: %s \\n\\n\" % decisionTreeBestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_vizinhos_proximos = KNeighborsClassifier()\n",
    "\n",
    "print(\"====== Started KNN parameters tuning\")\n",
    "\n",
    "param_dist = {'n_neighbors': list(np.arange(1, 15)), 'metric':['euclidean'], 'weights':['uniform', 'distance']} #ParÃ¢metros testados\n",
    "grid_search = GridSearchCV(knn_vizinhos_proximos, param_grid=param_dist, cv=kfold_ajuste_parametros, scoring='accuracy', refit=False)\n",
    "grid_search.fit(transfusion_values, transfusion_classes)\n",
    "knnBestParams = grid_search.best_params_\n",
    "print(\"KNN: %s \\n\\n\" % knnBestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(transfusion_classes.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(transfusion_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67109994\n",
      "Iteration 2, loss = 0.65596304\n",
      "Iteration 3, loss = 0.63588352\n",
      "Iteration 4, loss = 0.61414113\n",
      "Iteration 5, loss = 0.59462074\n",
      "Iteration 6, loss = 0.57794304\n",
      "Iteration 7, loss = 0.56464189\n",
      "Iteration 8, loss = 0.55413983\n",
      "Iteration 9, loss = 0.54772512\n",
      "Iteration 10, loss = 0.54208453\n",
      "Iteration 11, loss = 0.53894518\n",
      "Iteration 12, loss = 0.53635752\n",
      "Iteration 13, loss = 0.53474247\n",
      "Iteration 14, loss = 0.53368274\n",
      "Iteration 15, loss = 0.53316447\n",
      "Iteration 16, loss = 0.53229854\n",
      "Iteration 17, loss = 0.53193392\n",
      "Iteration 18, loss = 0.53155283\n",
      "Iteration 19, loss = 0.53107230\n",
      "Iteration 20, loss = 0.53059110\n",
      "Iteration 21, loss = 0.53014716\n",
      "Iteration 22, loss = 0.52972825\n",
      "Iteration 23, loss = 0.52928100\n",
      "Iteration 24, loss = 0.52879024\n",
      "Iteration 25, loss = 0.52833596\n",
      "Iteration 26, loss = 0.52780156\n",
      "Iteration 27, loss = 0.52740490\n",
      "Iteration 28, loss = 0.52683110\n",
      "Iteration 29, loss = 0.52630402\n",
      "Iteration 30, loss = 0.52584667\n",
      "Iteration 31, loss = 0.52532696\n",
      "Iteration 32, loss = 0.52483333\n",
      "Iteration 33, loss = 0.52432944\n",
      "Iteration 34, loss = 0.52384079\n",
      "Iteration 35, loss = 0.52331463\n",
      "Iteration 36, loss = 0.52283212\n",
      "Iteration 37, loss = 0.52232643\n",
      "Iteration 38, loss = 0.52177440\n",
      "Iteration 39, loss = 0.52130931\n",
      "Iteration 40, loss = 0.52083269\n",
      "Iteration 41, loss = 0.52024882\n",
      "Iteration 42, loss = 0.51972077\n",
      "Iteration 43, loss = 0.51917418\n",
      "Iteration 44, loss = 0.51870897\n",
      "Iteration 45, loss = 0.51814944\n",
      "Iteration 46, loss = 0.51762020\n",
      "Iteration 47, loss = 0.51712749\n",
      "Iteration 48, loss = 0.51657759\n",
      "Iteration 49, loss = 0.51604179\n",
      "Iteration 50, loss = 0.51551287\n",
      "Iteration 51, loss = 0.51501904\n",
      "Iteration 52, loss = 0.51450237\n",
      "Iteration 53, loss = 0.51391110\n",
      "Iteration 54, loss = 0.51339426\n",
      "Iteration 55, loss = 0.51284225\n",
      "Iteration 56, loss = 0.51231608\n",
      "Iteration 57, loss = 0.51178510\n",
      "Iteration 58, loss = 0.51127208\n",
      "Iteration 59, loss = 0.51069244\n",
      "Iteration 60, loss = 0.51014831\n",
      "Iteration 61, loss = 0.50961290\n",
      "Iteration 62, loss = 0.50909520\n",
      "Iteration 63, loss = 0.50856237\n",
      "Iteration 64, loss = 0.50801954\n",
      "Iteration 65, loss = 0.50744291\n",
      "Iteration 66, loss = 0.50691807\n",
      "Iteration 67, loss = 0.50640005\n",
      "Iteration 68, loss = 0.50582926\n",
      "Iteration 69, loss = 0.50530815\n",
      "Iteration 70, loss = 0.50479572\n",
      "Iteration 71, loss = 0.50423806\n",
      "Iteration 72, loss = 0.50371658\n",
      "Iteration 73, loss = 0.50317606\n",
      "Iteration 74, loss = 0.50265701\n",
      "Iteration 75, loss = 0.50211285\n",
      "Iteration 76, loss = 0.50159758\n",
      "Iteration 77, loss = 0.50102846\n",
      "Iteration 78, loss = 0.50050754\n",
      "Iteration 79, loss = 0.49998532\n",
      "Iteration 80, loss = 0.49947317\n",
      "Iteration 81, loss = 0.49897720\n",
      "Iteration 82, loss = 0.49847504\n",
      "Iteration 83, loss = 0.49794041\n",
      "Iteration 84, loss = 0.49736554\n",
      "Iteration 85, loss = 0.49683670\n",
      "Iteration 86, loss = 0.49638298\n",
      "Iteration 87, loss = 0.49586577\n",
      "Iteration 88, loss = 0.49528821\n",
      "Iteration 89, loss = 0.49484846\n",
      "Iteration 90, loss = 0.49429691\n",
      "Iteration 91, loss = 0.49378568\n",
      "Iteration 92, loss = 0.49331292\n",
      "Iteration 93, loss = 0.49280006\n",
      "Iteration 94, loss = 0.49229407\n",
      "Iteration 95, loss = 0.49177475\n",
      "Iteration 96, loss = 0.49130770\n",
      "Iteration 97, loss = 0.49084547\n",
      "Iteration 98, loss = 0.49035805\n",
      "Iteration 99, loss = 0.48984850\n",
      "Iteration 100, loss = 0.48935131\n",
      "Iteration 101, loss = 0.48889028\n",
      "Iteration 102, loss = 0.48843119\n",
      "Iteration 103, loss = 0.48795021\n",
      "Iteration 104, loss = 0.48748820\n",
      "Iteration 105, loss = 0.48698760\n",
      "Iteration 106, loss = 0.48653959\n",
      "Iteration 107, loss = 0.48608125\n",
      "Iteration 108, loss = 0.48561708\n",
      "Iteration 109, loss = 0.48513366\n",
      "Iteration 110, loss = 0.48464692\n",
      "Iteration 111, loss = 0.48422613\n",
      "Iteration 112, loss = 0.48382728\n",
      "Iteration 113, loss = 0.48339487\n",
      "Iteration 114, loss = 0.48288263\n",
      "Iteration 115, loss = 0.48247411\n",
      "Iteration 116, loss = 0.48201210\n",
      "Iteration 117, loss = 0.48159335\n",
      "Iteration 118, loss = 0.48118498\n",
      "Iteration 119, loss = 0.48073124\n",
      "Iteration 120, loss = 0.48033436\n",
      "Iteration 121, loss = 0.47990240\n",
      "Iteration 122, loss = 0.47946984\n",
      "Iteration 123, loss = 0.47911975\n",
      "Iteration 124, loss = 0.47864231\n",
      "Iteration 125, loss = 0.47826475\n",
      "Iteration 126, loss = 0.47789025\n",
      "Iteration 127, loss = 0.47748909\n",
      "Iteration 128, loss = 0.47709878\n",
      "Iteration 129, loss = 0.47668142\n",
      "Iteration 130, loss = 0.47630197\n",
      "Iteration 131, loss = 0.47598103\n",
      "Iteration 132, loss = 0.47558299\n",
      "Iteration 133, loss = 0.47519311\n",
      "Iteration 134, loss = 0.47478982\n",
      "Iteration 135, loss = 0.47445419\n",
      "Iteration 136, loss = 0.47408267\n",
      "Iteration 137, loss = 0.47371867\n",
      "Iteration 138, loss = 0.47338576\n",
      "Iteration 139, loss = 0.47303522\n",
      "Iteration 140, loss = 0.47266717\n",
      "Iteration 141, loss = 0.47235998\n",
      "Iteration 142, loss = 0.47200617\n",
      "Iteration 143, loss = 0.47164935\n",
      "Iteration 144, loss = 0.47133611\n",
      "Iteration 145, loss = 0.47101217\n",
      "Iteration 146, loss = 0.47066306\n",
      "Iteration 147, loss = 0.47032675\n",
      "Iteration 148, loss = 0.47003657\n",
      "Iteration 149, loss = 0.46972183\n",
      "Iteration 150, loss = 0.46940930\n",
      "Iteration 151, loss = 0.46911559\n",
      "Iteration 152, loss = 0.46888427\n",
      "Iteration 153, loss = 0.46848864\n",
      "Iteration 154, loss = 0.46818530\n",
      "Iteration 155, loss = 0.46791189\n",
      "Iteration 156, loss = 0.46765380\n",
      "Iteration 157, loss = 0.46738892\n",
      "Iteration 158, loss = 0.46707523\n",
      "Iteration 159, loss = 0.46677862\n",
      "Iteration 160, loss = 0.46651407\n",
      "Iteration 161, loss = 0.46624667\n",
      "Iteration 162, loss = 0.46595253\n",
      "Iteration 163, loss = 0.46573757\n",
      "Iteration 164, loss = 0.46543822\n",
      "Iteration 165, loss = 0.46521242\n",
      "Iteration 166, loss = 0.46500198\n",
      "Iteration 167, loss = 0.46470511\n",
      "Iteration 168, loss = 0.46447617\n",
      "Iteration 169, loss = 0.46423004\n",
      "Iteration 170, loss = 0.46396403\n",
      "Iteration 171, loss = 0.46375059\n",
      "Iteration 172, loss = 0.46353151\n",
      "Iteration 173, loss = 0.46327105\n",
      "Iteration 174, loss = 0.46304466\n",
      "Iteration 175, loss = 0.46282486\n",
      "Iteration 176, loss = 0.46259113\n",
      "Iteration 177, loss = 0.46238613\n",
      "Iteration 178, loss = 0.46218178\n",
      "Iteration 179, loss = 0.46196376\n",
      "Iteration 180, loss = 0.46173418\n",
      "Iteration 181, loss = 0.46154882\n",
      "Iteration 182, loss = 0.46136344\n",
      "Iteration 183, loss = 0.46113437\n",
      "Iteration 184, loss = 0.46098181\n",
      "Iteration 185, loss = 0.46074449\n",
      "Iteration 186, loss = 0.46056087\n",
      "Iteration 187, loss = 0.46036390\n",
      "Iteration 188, loss = 0.46020141\n",
      "Iteration 189, loss = 0.46000593\n",
      "Iteration 190, loss = 0.45982330\n",
      "Iteration 191, loss = 0.45969426\n",
      "Iteration 192, loss = 0.45951991\n",
      "Iteration 193, loss = 0.45931562\n",
      "Iteration 194, loss = 0.45913530\n",
      "Iteration 195, loss = 0.45897323\n",
      "Iteration 196, loss = 0.45882968\n",
      "Iteration 197, loss = 0.45862098\n",
      "Iteration 198, loss = 0.45847741\n",
      "Iteration 199, loss = 0.45832340\n",
      "Iteration 200, loss = 0.45817329\n",
      "Iteration 201, loss = 0.45802724\n",
      "Iteration 202, loss = 0.45786902\n",
      "Iteration 203, loss = 0.45769792\n",
      "Iteration 204, loss = 0.45756286\n",
      "Iteration 205, loss = 0.45742763\n",
      "Iteration 206, loss = 0.45729485\n",
      "Iteration 207, loss = 0.45714205\n",
      "Iteration 208, loss = 0.45699803\n",
      "Iteration 209, loss = 0.45687769\n",
      "Iteration 210, loss = 0.45672762\n",
      "Iteration 211, loss = 0.45660538\n",
      "Iteration 212, loss = 0.45646906\n",
      "Iteration 213, loss = 0.45635657\n",
      "Iteration 214, loss = 0.45623919\n",
      "Iteration 215, loss = 0.45610158\n",
      "Iteration 216, loss = 0.45601323\n",
      "Iteration 217, loss = 0.45587254\n",
      "Iteration 218, loss = 0.45575741\n",
      "Iteration 219, loss = 0.45564933\n",
      "Iteration 220, loss = 0.45549750\n",
      "Iteration 221, loss = 0.45539510\n",
      "Iteration 222, loss = 0.45529887\n",
      "Iteration 223, loss = 0.45517954\n",
      "Iteration 224, loss = 0.45509329\n",
      "Iteration 225, loss = 0.45496112\n",
      "Iteration 226, loss = 0.45484515\n",
      "Iteration 227, loss = 0.45476457\n",
      "Iteration 228, loss = 0.45467055\n",
      "Iteration 229, loss = 0.45454920\n",
      "Iteration 230, loss = 0.45444790\n",
      "Iteration 231, loss = 0.45434862\n",
      "Iteration 232, loss = 0.45429256\n",
      "Iteration 233, loss = 0.45418273\n",
      "Iteration 234, loss = 0.45407229\n",
      "Iteration 235, loss = 0.45402546\n",
      "Iteration 236, loss = 0.45390430\n",
      "Iteration 237, loss = 0.45381143\n",
      "Iteration 238, loss = 0.45374699\n",
      "Iteration 239, loss = 0.45362078\n",
      "Iteration 240, loss = 0.45355040\n",
      "Iteration 241, loss = 0.45347120\n",
      "Iteration 242, loss = 0.45338375\n",
      "Iteration 243, loss = 0.45330337\n",
      "Iteration 244, loss = 0.45322800\n",
      "Iteration 245, loss = 0.45317761\n",
      "Iteration 246, loss = 0.45310228\n",
      "Iteration 247, loss = 0.45299624\n",
      "Iteration 248, loss = 0.45293207\n",
      "Iteration 249, loss = 0.45284486\n",
      "Iteration 250, loss = 0.45277657\n",
      "Iteration 251, loss = 0.45271546\n",
      "Iteration 252, loss = 0.45268110\n",
      "Iteration 253, loss = 0.45256073\n",
      "Iteration 254, loss = 0.45249780\n",
      "Iteration 255, loss = 0.45243000\n",
      "Iteration 256, loss = 0.45235075\n",
      "Iteration 257, loss = 0.45229638\n",
      "Iteration 258, loss = 0.45224952\n",
      "Iteration 259, loss = 0.45221937\n",
      "Iteration 260, loss = 0.45210401\n",
      "Iteration 261, loss = 0.45204809\n",
      "Iteration 262, loss = 0.45201654\n",
      "Iteration 263, loss = 0.45190828\n",
      "Iteration 264, loss = 0.45189520\n",
      "Iteration 265, loss = 0.45180950\n",
      "Iteration 266, loss = 0.45176585\n",
      "Iteration 267, loss = 0.45170665\n",
      "Iteration 268, loss = 0.45163348\n",
      "Iteration 269, loss = 0.45158539\n",
      "Iteration 270, loss = 0.45153804\n",
      "Iteration 271, loss = 0.45148080\n",
      "Iteration 272, loss = 0.45144905\n",
      "Iteration 273, loss = 0.45139571\n",
      "Iteration 274, loss = 0.45131877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69234944\n",
      "Iteration 2, loss = 0.67910606\n",
      "Iteration 3, loss = 0.66185936\n",
      "Iteration 4, loss = 0.64246959\n",
      "Iteration 5, loss = 0.62515018\n",
      "Iteration 6, loss = 0.60944708\n",
      "Iteration 7, loss = 0.59652985\n",
      "Iteration 8, loss = 0.58654670\n",
      "Iteration 9, loss = 0.57947821\n",
      "Iteration 10, loss = 0.57354677\n",
      "Iteration 11, loss = 0.56967190\n",
      "Iteration 12, loss = 0.56690497\n",
      "Iteration 13, loss = 0.56555276\n",
      "Iteration 14, loss = 0.56353244\n",
      "Iteration 15, loss = 0.56255915\n",
      "Iteration 16, loss = 0.56177783\n",
      "Iteration 17, loss = 0.56113816\n",
      "Iteration 18, loss = 0.56046996\n",
      "Iteration 19, loss = 0.55984890\n",
      "Iteration 20, loss = 0.55936887\n",
      "Iteration 21, loss = 0.55880095\n",
      "Iteration 22, loss = 0.55827071\n",
      "Iteration 23, loss = 0.55771161\n",
      "Iteration 24, loss = 0.55717583\n",
      "Iteration 25, loss = 0.55662092\n",
      "Iteration 26, loss = 0.55606721\n",
      "Iteration 27, loss = 0.55552972\n",
      "Iteration 28, loss = 0.55504587\n",
      "Iteration 29, loss = 0.55450340\n",
      "Iteration 30, loss = 0.55393339\n",
      "Iteration 31, loss = 0.55346824\n",
      "Iteration 32, loss = 0.55291972\n",
      "Iteration 33, loss = 0.55240804\n",
      "Iteration 34, loss = 0.55193091\n",
      "Iteration 35, loss = 0.55142160\n",
      "Iteration 36, loss = 0.55094882\n",
      "Iteration 37, loss = 0.55044846\n",
      "Iteration 38, loss = 0.54993846\n",
      "Iteration 39, loss = 0.54946658\n",
      "Iteration 40, loss = 0.54899001\n",
      "Iteration 41, loss = 0.54850926\n",
      "Iteration 42, loss = 0.54804256\n",
      "Iteration 43, loss = 0.54757980\n",
      "Iteration 44, loss = 0.54715990\n",
      "Iteration 45, loss = 0.54667354\n",
      "Iteration 46, loss = 0.54623299\n",
      "Iteration 47, loss = 0.54572997\n",
      "Iteration 48, loss = 0.54528461\n",
      "Iteration 49, loss = 0.54483669\n",
      "Iteration 50, loss = 0.54440758\n",
      "Iteration 51, loss = 0.54395018\n",
      "Iteration 52, loss = 0.54353915\n",
      "Iteration 53, loss = 0.54308335\n",
      "Iteration 54, loss = 0.54266588\n",
      "Iteration 55, loss = 0.54221455\n",
      "Iteration 56, loss = 0.54183871\n",
      "Iteration 57, loss = 0.54136198\n",
      "Iteration 58, loss = 0.54094087\n",
      "Iteration 59, loss = 0.54050892\n",
      "Iteration 60, loss = 0.54008409\n",
      "Iteration 61, loss = 0.53968017\n",
      "Iteration 62, loss = 0.53924545\n",
      "Iteration 63, loss = 0.53883298\n",
      "Iteration 64, loss = 0.53839273\n",
      "Iteration 65, loss = 0.53799023\n",
      "Iteration 66, loss = 0.53755895\n",
      "Iteration 67, loss = 0.53712111\n",
      "Iteration 68, loss = 0.53672557\n",
      "Iteration 69, loss = 0.53633903\n",
      "Iteration 70, loss = 0.53588851\n",
      "Iteration 71, loss = 0.53549084\n",
      "Iteration 72, loss = 0.53510187\n",
      "Iteration 73, loss = 0.53465350\n",
      "Iteration 74, loss = 0.53426143\n",
      "Iteration 75, loss = 0.53382954\n",
      "Iteration 76, loss = 0.53340550\n",
      "Iteration 77, loss = 0.53298874\n",
      "Iteration 78, loss = 0.53257332\n",
      "Iteration 79, loss = 0.53216926\n",
      "Iteration 80, loss = 0.53176168\n",
      "Iteration 81, loss = 0.53133296\n",
      "Iteration 82, loss = 0.53095143\n",
      "Iteration 83, loss = 0.53053369\n",
      "Iteration 84, loss = 0.53007365\n",
      "Iteration 85, loss = 0.52969665\n",
      "Iteration 86, loss = 0.52930818\n",
      "Iteration 87, loss = 0.52886046\n",
      "Iteration 88, loss = 0.52845569\n",
      "Iteration 89, loss = 0.52803999\n",
      "Iteration 90, loss = 0.52765339\n",
      "Iteration 91, loss = 0.52724424\n",
      "Iteration 92, loss = 0.52683853\n",
      "Iteration 93, loss = 0.52640037\n",
      "Iteration 94, loss = 0.52600918\n",
      "Iteration 95, loss = 0.52560934\n",
      "Iteration 96, loss = 0.52518504\n",
      "Iteration 97, loss = 0.52476276\n",
      "Iteration 98, loss = 0.52436996\n",
      "Iteration 99, loss = 0.52397548\n",
      "Iteration 100, loss = 0.52358615\n",
      "Iteration 101, loss = 0.52314577\n",
      "Iteration 102, loss = 0.52278759\n",
      "Iteration 103, loss = 0.52234554\n",
      "Iteration 104, loss = 0.52199042\n",
      "Iteration 105, loss = 0.52155672\n",
      "Iteration 106, loss = 0.52119496\n",
      "Iteration 107, loss = 0.52076261\n",
      "Iteration 108, loss = 0.52034558\n",
      "Iteration 109, loss = 0.51995676\n",
      "Iteration 110, loss = 0.51958835\n",
      "Iteration 111, loss = 0.51916444\n",
      "Iteration 112, loss = 0.51876605\n",
      "Iteration 113, loss = 0.51839240\n",
      "Iteration 114, loss = 0.51805333\n",
      "Iteration 115, loss = 0.51758236\n",
      "Iteration 116, loss = 0.51717634\n",
      "Iteration 117, loss = 0.51680778\n",
      "Iteration 118, loss = 0.51642095\n",
      "Iteration 119, loss = 0.51604672\n",
      "Iteration 120, loss = 0.51572670\n",
      "Iteration 121, loss = 0.51531616\n",
      "Iteration 122, loss = 0.51486178\n",
      "Iteration 123, loss = 0.51449878\n",
      "Iteration 124, loss = 0.51412975\n",
      "Iteration 125, loss = 0.51375535\n",
      "Iteration 126, loss = 0.51336823\n",
      "Iteration 127, loss = 0.51298763\n",
      "Iteration 128, loss = 0.51259916\n",
      "Iteration 129, loss = 0.51225295\n",
      "Iteration 130, loss = 0.51189378\n",
      "Iteration 131, loss = 0.51151634\n",
      "Iteration 132, loss = 0.51111913\n",
      "Iteration 133, loss = 0.51079427\n",
      "Iteration 134, loss = 0.51039559\n",
      "Iteration 135, loss = 0.51001596\n",
      "Iteration 136, loss = 0.50973605\n",
      "Iteration 137, loss = 0.50935865\n",
      "Iteration 138, loss = 0.50898196\n",
      "Iteration 139, loss = 0.50863471\n",
      "Iteration 140, loss = 0.50827539\n",
      "Iteration 141, loss = 0.50790502\n",
      "Iteration 142, loss = 0.50757607\n",
      "Iteration 143, loss = 0.50720089\n",
      "Iteration 144, loss = 0.50688361\n",
      "Iteration 145, loss = 0.50653646\n",
      "Iteration 146, loss = 0.50619870\n",
      "Iteration 147, loss = 0.50586151\n",
      "Iteration 148, loss = 0.50556708\n",
      "Iteration 149, loss = 0.50518644\n",
      "Iteration 150, loss = 0.50485990\n",
      "Iteration 151, loss = 0.50456778\n",
      "Iteration 152, loss = 0.50423289\n",
      "Iteration 153, loss = 0.50392955\n",
      "Iteration 154, loss = 0.50356870\n",
      "Iteration 155, loss = 0.50327370\n",
      "Iteration 156, loss = 0.50300775\n",
      "Iteration 157, loss = 0.50263980\n",
      "Iteration 158, loss = 0.50233255\n",
      "Iteration 159, loss = 0.50204506\n",
      "Iteration 160, loss = 0.50172355\n",
      "Iteration 161, loss = 0.50144963\n",
      "Iteration 162, loss = 0.50112649\n",
      "Iteration 163, loss = 0.50082145\n",
      "Iteration 164, loss = 0.50053162\n",
      "Iteration 165, loss = 0.50025406\n",
      "Iteration 166, loss = 0.49998638\n",
      "Iteration 167, loss = 0.49969014\n",
      "Iteration 168, loss = 0.49942381\n",
      "Iteration 169, loss = 0.49913498\n",
      "Iteration 170, loss = 0.49886932\n",
      "Iteration 171, loss = 0.49862866\n",
      "Iteration 172, loss = 0.49833314\n",
      "Iteration 173, loss = 0.49811074\n",
      "Iteration 174, loss = 0.49779803\n",
      "Iteration 175, loss = 0.49751393\n",
      "Iteration 176, loss = 0.49729192\n",
      "Iteration 177, loss = 0.49702206\n",
      "Iteration 178, loss = 0.49678503\n",
      "Iteration 179, loss = 0.49654082\n",
      "Iteration 180, loss = 0.49634428\n",
      "Iteration 181, loss = 0.49605443\n",
      "Iteration 182, loss = 0.49581218\n",
      "Iteration 183, loss = 0.49558087\n",
      "Iteration 184, loss = 0.49536733\n",
      "Iteration 185, loss = 0.49509635\n",
      "Iteration 186, loss = 0.49489642\n",
      "Iteration 187, loss = 0.49467352\n",
      "Iteration 188, loss = 0.49443499\n",
      "Iteration 189, loss = 0.49429150\n",
      "Iteration 190, loss = 0.49400054\n",
      "Iteration 191, loss = 0.49380006\n",
      "Iteration 192, loss = 0.49361978\n",
      "Iteration 193, loss = 0.49336980\n",
      "Iteration 194, loss = 0.49317051\n",
      "Iteration 195, loss = 0.49299116\n",
      "Iteration 196, loss = 0.49277726\n",
      "Iteration 197, loss = 0.49259056\n",
      "Iteration 198, loss = 0.49240897\n",
      "Iteration 199, loss = 0.49219857\n",
      "Iteration 200, loss = 0.49199712\n",
      "Iteration 201, loss = 0.49181908\n",
      "Iteration 202, loss = 0.49162694\n",
      "Iteration 203, loss = 0.49146759\n",
      "Iteration 204, loss = 0.49130246\n",
      "Iteration 205, loss = 0.49110959\n",
      "Iteration 206, loss = 0.49093091\n",
      "Iteration 207, loss = 0.49075783\n",
      "Iteration 208, loss = 0.49062411\n",
      "Iteration 209, loss = 0.49042861\n",
      "Iteration 210, loss = 0.49028102\n",
      "Iteration 211, loss = 0.49012777\n",
      "Iteration 212, loss = 0.48997710\n",
      "Iteration 213, loss = 0.48978320\n",
      "Iteration 214, loss = 0.48966536\n",
      "Iteration 215, loss = 0.48949146\n",
      "Iteration 216, loss = 0.48933852\n",
      "Iteration 217, loss = 0.48920672\n",
      "Iteration 218, loss = 0.48905801\n",
      "Iteration 219, loss = 0.48894411\n",
      "Iteration 220, loss = 0.48878025\n",
      "Iteration 221, loss = 0.48864605\n",
      "Iteration 222, loss = 0.48848909\n",
      "Iteration 223, loss = 0.48839000\n",
      "Iteration 224, loss = 0.48823289\n",
      "Iteration 225, loss = 0.48813508\n",
      "Iteration 226, loss = 0.48799947\n",
      "Iteration 227, loss = 0.48789942\n",
      "Iteration 228, loss = 0.48774688\n",
      "Iteration 229, loss = 0.48762596\n",
      "Iteration 230, loss = 0.48751531\n",
      "Iteration 231, loss = 0.48740254\n",
      "Iteration 232, loss = 0.48726424\n",
      "Iteration 233, loss = 0.48715062\n",
      "Iteration 234, loss = 0.48706733\n",
      "Iteration 235, loss = 0.48691593\n",
      "Iteration 236, loss = 0.48687182\n",
      "Iteration 237, loss = 0.48671288\n",
      "Iteration 238, loss = 0.48662503\n",
      "Iteration 239, loss = 0.48652683\n",
      "Iteration 240, loss = 0.48642540\n",
      "Iteration 241, loss = 0.48630112\n",
      "Iteration 242, loss = 0.48620266\n",
      "Iteration 243, loss = 0.48612364\n",
      "Iteration 244, loss = 0.48603168\n",
      "Iteration 245, loss = 0.48591098\n",
      "Iteration 246, loss = 0.48582155\n",
      "Iteration 247, loss = 0.48574499\n",
      "Iteration 248, loss = 0.48564219\n",
      "Iteration 249, loss = 0.48556348\n",
      "Iteration 250, loss = 0.48548424\n",
      "Iteration 251, loss = 0.48539000\n",
      "Iteration 252, loss = 0.48531679\n",
      "Iteration 253, loss = 0.48523926\n",
      "Iteration 254, loss = 0.48516076\n",
      "Iteration 255, loss = 0.48506563\n",
      "Iteration 256, loss = 0.48498423\n",
      "Iteration 257, loss = 0.48490489\n",
      "Iteration 258, loss = 0.48487484\n",
      "Iteration 259, loss = 0.48477685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69660678\n",
      "Iteration 2, loss = 0.68225786\n",
      "Iteration 3, loss = 0.66179041\n",
      "Iteration 4, loss = 0.63923417\n",
      "Iteration 5, loss = 0.61918374\n",
      "Iteration 6, loss = 0.60225034\n",
      "Iteration 7, loss = 0.58810609\n",
      "Iteration 8, loss = 0.57671408\n",
      "Iteration 9, loss = 0.56821174\n",
      "Iteration 10, loss = 0.56236548\n",
      "Iteration 11, loss = 0.55801336\n",
      "Iteration 12, loss = 0.55508739\n",
      "Iteration 13, loss = 0.55261726\n",
      "Iteration 14, loss = 0.55144087\n",
      "Iteration 15, loss = 0.55011865\n",
      "Iteration 16, loss = 0.54943394\n",
      "Iteration 17, loss = 0.54867161\n",
      "Iteration 18, loss = 0.54794336\n",
      "Iteration 19, loss = 0.54740486\n",
      "Iteration 20, loss = 0.54681636\n",
      "Iteration 21, loss = 0.54621853\n",
      "Iteration 22, loss = 0.54568558\n",
      "Iteration 23, loss = 0.54502882\n",
      "Iteration 24, loss = 0.54454572\n",
      "Iteration 25, loss = 0.54389157\n",
      "Iteration 26, loss = 0.54337391\n",
      "Iteration 27, loss = 0.54280287\n",
      "Iteration 28, loss = 0.54220465\n",
      "Iteration 29, loss = 0.54162008\n",
      "Iteration 30, loss = 0.54107636\n",
      "Iteration 31, loss = 0.54055627\n",
      "Iteration 32, loss = 0.53999418\n",
      "Iteration 33, loss = 0.53950257\n",
      "Iteration 34, loss = 0.53895928\n",
      "Iteration 35, loss = 0.53845381\n",
      "Iteration 36, loss = 0.53795846\n",
      "Iteration 37, loss = 0.53743544\n",
      "Iteration 38, loss = 0.53691714\n",
      "Iteration 39, loss = 0.53644775\n",
      "Iteration 40, loss = 0.53597734\n",
      "Iteration 41, loss = 0.53544572\n",
      "Iteration 42, loss = 0.53500161\n",
      "Iteration 43, loss = 0.53448497\n",
      "Iteration 44, loss = 0.53405279\n",
      "Iteration 45, loss = 0.53360503\n",
      "Iteration 46, loss = 0.53311652\n",
      "Iteration 47, loss = 0.53270796\n",
      "Iteration 48, loss = 0.53226320\n",
      "Iteration 49, loss = 0.53183197\n",
      "Iteration 50, loss = 0.53137609\n",
      "Iteration 51, loss = 0.53097559\n",
      "Iteration 52, loss = 0.53053207\n",
      "Iteration 53, loss = 0.53011276\n",
      "Iteration 54, loss = 0.52971824\n",
      "Iteration 55, loss = 0.52930107\n",
      "Iteration 56, loss = 0.52888822\n",
      "Iteration 57, loss = 0.52848842\n",
      "Iteration 58, loss = 0.52815370\n",
      "Iteration 59, loss = 0.52773349\n",
      "Iteration 60, loss = 0.52733386\n",
      "Iteration 61, loss = 0.52693480\n",
      "Iteration 62, loss = 0.52664305\n",
      "Iteration 63, loss = 0.52619215\n",
      "Iteration 64, loss = 0.52582390\n",
      "Iteration 65, loss = 0.52545874\n",
      "Iteration 66, loss = 0.52508381\n",
      "Iteration 67, loss = 0.52474393\n",
      "Iteration 68, loss = 0.52439865\n",
      "Iteration 69, loss = 0.52403064\n",
      "Iteration 70, loss = 0.52367101\n",
      "Iteration 71, loss = 0.52332652\n",
      "Iteration 72, loss = 0.52296109\n",
      "Iteration 73, loss = 0.52265852\n",
      "Iteration 74, loss = 0.52234827\n",
      "Iteration 75, loss = 0.52196067\n",
      "Iteration 76, loss = 0.52163400\n",
      "Iteration 77, loss = 0.52128123\n",
      "Iteration 78, loss = 0.52095753\n",
      "Iteration 79, loss = 0.52064761\n",
      "Iteration 80, loss = 0.52028376\n",
      "Iteration 81, loss = 0.51995802\n",
      "Iteration 82, loss = 0.51962445\n",
      "Iteration 83, loss = 0.51932562\n",
      "Iteration 84, loss = 0.51900996\n",
      "Iteration 85, loss = 0.51871214\n",
      "Iteration 86, loss = 0.51835750\n",
      "Iteration 87, loss = 0.51807990\n",
      "Iteration 88, loss = 0.51776227\n",
      "Iteration 89, loss = 0.51745102\n",
      "Iteration 90, loss = 0.51715513\n",
      "Iteration 91, loss = 0.51683163\n",
      "Iteration 92, loss = 0.51658776\n",
      "Iteration 93, loss = 0.51627816\n",
      "Iteration 94, loss = 0.51593376\n",
      "Iteration 95, loss = 0.51566388\n",
      "Iteration 96, loss = 0.51534539\n",
      "Iteration 97, loss = 0.51503326\n",
      "Iteration 98, loss = 0.51476083\n",
      "Iteration 99, loss = 0.51448116\n",
      "Iteration 100, loss = 0.51419081\n",
      "Iteration 101, loss = 0.51391666\n",
      "Iteration 102, loss = 0.51363811\n",
      "Iteration 103, loss = 0.51333567\n",
      "Iteration 104, loss = 0.51307441\n",
      "Iteration 105, loss = 0.51278766\n",
      "Iteration 106, loss = 0.51248711\n",
      "Iteration 107, loss = 0.51220694\n",
      "Iteration 108, loss = 0.51195025\n",
      "Iteration 109, loss = 0.51166786\n",
      "Iteration 110, loss = 0.51139252\n",
      "Iteration 111, loss = 0.51113874\n",
      "Iteration 112, loss = 0.51084846\n",
      "Iteration 113, loss = 0.51059298\n",
      "Iteration 114, loss = 0.51031636\n",
      "Iteration 115, loss = 0.51004929\n",
      "Iteration 116, loss = 0.50981929\n",
      "Iteration 117, loss = 0.50951623\n",
      "Iteration 118, loss = 0.50926557\n",
      "Iteration 119, loss = 0.50902264\n",
      "Iteration 120, loss = 0.50872875\n",
      "Iteration 121, loss = 0.50850413\n",
      "Iteration 122, loss = 0.50822379\n",
      "Iteration 123, loss = 0.50799316\n",
      "Iteration 124, loss = 0.50773583\n",
      "Iteration 125, loss = 0.50748541\n",
      "Iteration 126, loss = 0.50723059\n",
      "Iteration 127, loss = 0.50697335\n",
      "Iteration 128, loss = 0.50671207\n",
      "Iteration 129, loss = 0.50647065\n",
      "Iteration 130, loss = 0.50629117\n",
      "Iteration 131, loss = 0.50601037\n",
      "Iteration 132, loss = 0.50572970\n",
      "Iteration 133, loss = 0.50549612\n",
      "Iteration 134, loss = 0.50524901\n",
      "Iteration 135, loss = 0.50503514\n",
      "Iteration 136, loss = 0.50477349\n",
      "Iteration 137, loss = 0.50455157\n",
      "Iteration 138, loss = 0.50431577\n",
      "Iteration 139, loss = 0.50406107\n",
      "Iteration 140, loss = 0.50384843\n",
      "Iteration 141, loss = 0.50360819\n",
      "Iteration 142, loss = 0.50340158\n",
      "Iteration 143, loss = 0.50315500\n",
      "Iteration 144, loss = 0.50293727\n",
      "Iteration 145, loss = 0.50270073\n",
      "Iteration 146, loss = 0.50245472\n",
      "Iteration 147, loss = 0.50223989\n",
      "Iteration 148, loss = 0.50202572\n",
      "Iteration 149, loss = 0.50178810\n",
      "Iteration 150, loss = 0.50158130\n",
      "Iteration 151, loss = 0.50138297\n",
      "Iteration 152, loss = 0.50116022\n",
      "Iteration 153, loss = 0.50093859\n",
      "Iteration 154, loss = 0.50072810\n",
      "Iteration 155, loss = 0.50051550\n",
      "Iteration 156, loss = 0.50031966\n",
      "Iteration 157, loss = 0.50008210\n",
      "Iteration 158, loss = 0.49987181\n",
      "Iteration 159, loss = 0.49965706\n",
      "Iteration 160, loss = 0.49947187\n",
      "Iteration 161, loss = 0.49924624\n",
      "Iteration 162, loss = 0.49906097\n",
      "Iteration 163, loss = 0.49885366\n",
      "Iteration 164, loss = 0.49864721\n",
      "Iteration 165, loss = 0.49844738\n",
      "Iteration 166, loss = 0.49822704\n",
      "Iteration 167, loss = 0.49802161\n",
      "Iteration 168, loss = 0.49782857\n",
      "Iteration 169, loss = 0.49764015\n",
      "Iteration 170, loss = 0.49747641\n",
      "Iteration 171, loss = 0.49726456\n",
      "Iteration 172, loss = 0.49704732\n",
      "Iteration 173, loss = 0.49686865\n",
      "Iteration 174, loss = 0.49668070\n",
      "Iteration 175, loss = 0.49648513\n",
      "Iteration 176, loss = 0.49628530\n",
      "Iteration 177, loss = 0.49610482\n",
      "Iteration 178, loss = 0.49592381\n",
      "Iteration 179, loss = 0.49573620\n",
      "Iteration 180, loss = 0.49554880\n",
      "Iteration 181, loss = 0.49536259\n",
      "Iteration 182, loss = 0.49517113\n",
      "Iteration 183, loss = 0.49501722\n",
      "Iteration 184, loss = 0.49483348\n",
      "Iteration 185, loss = 0.49466657\n",
      "Iteration 186, loss = 0.49448565\n",
      "Iteration 187, loss = 0.49429121\n",
      "Iteration 188, loss = 0.49412228\n",
      "Iteration 189, loss = 0.49394127\n",
      "Iteration 190, loss = 0.49378120\n",
      "Iteration 191, loss = 0.49359909\n",
      "Iteration 192, loss = 0.49343924\n",
      "Iteration 193, loss = 0.49326901\n",
      "Iteration 194, loss = 0.49310597\n",
      "Iteration 195, loss = 0.49294822\n",
      "Iteration 196, loss = 0.49278470\n",
      "Iteration 197, loss = 0.49260025\n",
      "Iteration 198, loss = 0.49245285\n",
      "Iteration 199, loss = 0.49228437\n",
      "Iteration 200, loss = 0.49212619\n",
      "Iteration 201, loss = 0.49196696\n",
      "Iteration 202, loss = 0.49178112\n",
      "Iteration 203, loss = 0.49163731\n",
      "Iteration 204, loss = 0.49149829\n",
      "Iteration 205, loss = 0.49132613\n",
      "Iteration 206, loss = 0.49118798\n",
      "Iteration 207, loss = 0.49101312\n",
      "Iteration 208, loss = 0.49087404\n",
      "Iteration 209, loss = 0.49072027\n",
      "Iteration 210, loss = 0.49055424\n",
      "Iteration 211, loss = 0.49049131\n",
      "Iteration 212, loss = 0.49026877\n",
      "Iteration 213, loss = 0.49010918\n",
      "Iteration 214, loss = 0.48997677\n",
      "Iteration 215, loss = 0.48982935\n",
      "Iteration 216, loss = 0.48966437\n",
      "Iteration 217, loss = 0.48958010\n",
      "Iteration 218, loss = 0.48940183\n",
      "Iteration 219, loss = 0.48924153\n",
      "Iteration 220, loss = 0.48910784\n",
      "Iteration 221, loss = 0.48900537\n",
      "Iteration 222, loss = 0.48884222\n",
      "Iteration 223, loss = 0.48870239\n",
      "Iteration 224, loss = 0.48856446\n",
      "Iteration 225, loss = 0.48842081\n",
      "Iteration 226, loss = 0.48832377\n",
      "Iteration 227, loss = 0.48813849\n",
      "Iteration 228, loss = 0.48803466\n",
      "Iteration 229, loss = 0.48790089\n",
      "Iteration 230, loss = 0.48774987\n",
      "Iteration 231, loss = 0.48764303\n",
      "Iteration 232, loss = 0.48750418\n",
      "Iteration 233, loss = 0.48735554\n",
      "Iteration 234, loss = 0.48724935\n",
      "Iteration 235, loss = 0.48710154\n",
      "Iteration 236, loss = 0.48701592\n",
      "Iteration 237, loss = 0.48687634\n",
      "Iteration 238, loss = 0.48673315\n",
      "Iteration 239, loss = 0.48662302\n",
      "Iteration 240, loss = 0.48648389\n",
      "Iteration 241, loss = 0.48637547\n",
      "Iteration 242, loss = 0.48625011\n",
      "Iteration 243, loss = 0.48613631\n",
      "Iteration 244, loss = 0.48601287\n",
      "Iteration 245, loss = 0.48590403\n",
      "Iteration 246, loss = 0.48577509\n",
      "Iteration 247, loss = 0.48564834\n",
      "Iteration 248, loss = 0.48555960\n",
      "Iteration 249, loss = 0.48541468\n",
      "Iteration 250, loss = 0.48531262\n",
      "Iteration 251, loss = 0.48519823\n",
      "Iteration 252, loss = 0.48508668\n",
      "Iteration 253, loss = 0.48497448\n",
      "Iteration 254, loss = 0.48487580\n",
      "Iteration 255, loss = 0.48477402\n",
      "Iteration 256, loss = 0.48464431\n",
      "Iteration 257, loss = 0.48454289\n",
      "Iteration 258, loss = 0.48444071\n",
      "Iteration 259, loss = 0.48431779\n",
      "Iteration 260, loss = 0.48422157\n",
      "Iteration 261, loss = 0.48411532\n",
      "Iteration 262, loss = 0.48401718\n",
      "Iteration 263, loss = 0.48391712\n",
      "Iteration 264, loss = 0.48378786\n",
      "Iteration 265, loss = 0.48368631\n",
      "Iteration 266, loss = 0.48359447\n",
      "Iteration 267, loss = 0.48352946\n",
      "Iteration 268, loss = 0.48338832\n",
      "Iteration 269, loss = 0.48328123\n",
      "Iteration 270, loss = 0.48319845\n",
      "Iteration 271, loss = 0.48308599\n",
      "Iteration 272, loss = 0.48298159\n",
      "Iteration 273, loss = 0.48289394\n",
      "Iteration 274, loss = 0.48280803\n",
      "Iteration 275, loss = 0.48273973\n",
      "Iteration 276, loss = 0.48260380\n",
      "Iteration 277, loss = 0.48251753\n",
      "Iteration 278, loss = 0.48241482\n",
      "Iteration 279, loss = 0.48235045\n",
      "Iteration 280, loss = 0.48224111\n",
      "Iteration 281, loss = 0.48214905\n",
      "Iteration 282, loss = 0.48205858\n",
      "Iteration 283, loss = 0.48196903\n",
      "Iteration 284, loss = 0.48186868\n",
      "Iteration 285, loss = 0.48178406\n",
      "Iteration 286, loss = 0.48168289\n",
      "Iteration 287, loss = 0.48161103\n",
      "Iteration 288, loss = 0.48152158\n",
      "Iteration 289, loss = 0.48142777\n",
      "Iteration 290, loss = 0.48134354\n",
      "Iteration 291, loss = 0.48126002\n",
      "Iteration 292, loss = 0.48117816\n",
      "Iteration 293, loss = 0.48109997\n",
      "Iteration 294, loss = 0.48100676\n",
      "Iteration 295, loss = 0.48093762\n",
      "Iteration 296, loss = 0.48085816\n",
      "Iteration 297, loss = 0.48074532\n",
      "Iteration 298, loss = 0.48065832\n",
      "Iteration 299, loss = 0.48059430\n",
      "Iteration 300, loss = 0.48050799\n",
      "Iteration 1, loss = 0.66819867\n",
      "Iteration 2, loss = 0.65656916\n",
      "Iteration 3, loss = 0.64128467\n",
      "Iteration 4, loss = 0.62412437\n",
      "Iteration 5, loss = 0.60843842\n",
      "Iteration 6, loss = 0.59537001\n",
      "Iteration 7, loss = 0.58520212\n",
      "Iteration 8, loss = 0.57704972\n",
      "Iteration 9, loss = 0.57143783\n",
      "Iteration 10, loss = 0.56657954\n",
      "Iteration 11, loss = 0.56373287\n",
      "Iteration 12, loss = 0.56187294\n",
      "Iteration 13, loss = 0.56009765\n",
      "Iteration 14, loss = 0.55900995\n",
      "Iteration 15, loss = 0.55819849\n",
      "Iteration 16, loss = 0.55735492\n",
      "Iteration 17, loss = 0.55656319\n",
      "Iteration 18, loss = 0.55593722\n",
      "Iteration 19, loss = 0.55527719\n",
      "Iteration 20, loss = 0.55466254\n",
      "Iteration 21, loss = 0.55397615\n",
      "Iteration 22, loss = 0.55336540\n",
      "Iteration 23, loss = 0.55276086\n",
      "Iteration 24, loss = 0.55211746\n",
      "Iteration 25, loss = 0.55150663\n",
      "Iteration 26, loss = 0.55082506\n",
      "Iteration 27, loss = 0.55023806\n",
      "Iteration 28, loss = 0.54963662\n",
      "Iteration 29, loss = 0.54906492\n",
      "Iteration 30, loss = 0.54840380\n",
      "Iteration 31, loss = 0.54781684\n",
      "Iteration 32, loss = 0.54724971\n",
      "Iteration 33, loss = 0.54664879\n",
      "Iteration 34, loss = 0.54611183\n",
      "Iteration 35, loss = 0.54565918\n",
      "Iteration 36, loss = 0.54499758\n",
      "Iteration 37, loss = 0.54435862\n",
      "Iteration 38, loss = 0.54380548\n",
      "Iteration 39, loss = 0.54327175\n",
      "Iteration 40, loss = 0.54272192\n",
      "Iteration 41, loss = 0.54216477\n",
      "Iteration 42, loss = 0.54160944\n",
      "Iteration 43, loss = 0.54107460\n",
      "Iteration 44, loss = 0.54051995\n",
      "Iteration 45, loss = 0.54002498\n",
      "Iteration 46, loss = 0.53949116\n",
      "Iteration 47, loss = 0.53893477\n",
      "Iteration 48, loss = 0.53844117\n",
      "Iteration 49, loss = 0.53796366\n",
      "Iteration 50, loss = 0.53739133\n",
      "Iteration 51, loss = 0.53684229\n",
      "Iteration 52, loss = 0.53634875\n",
      "Iteration 53, loss = 0.53584301\n",
      "Iteration 54, loss = 0.53531994\n",
      "Iteration 55, loss = 0.53480549\n",
      "Iteration 56, loss = 0.53429814\n",
      "Iteration 57, loss = 0.53380086\n",
      "Iteration 58, loss = 0.53329827\n",
      "Iteration 59, loss = 0.53279520\n",
      "Iteration 60, loss = 0.53235147\n",
      "Iteration 61, loss = 0.53182168\n",
      "Iteration 62, loss = 0.53134450\n",
      "Iteration 63, loss = 0.53083081\n",
      "Iteration 64, loss = 0.53035746\n",
      "Iteration 65, loss = 0.52986891\n",
      "Iteration 66, loss = 0.52936730\n",
      "Iteration 67, loss = 0.52890932\n",
      "Iteration 68, loss = 0.52839565\n",
      "Iteration 69, loss = 0.52794995\n",
      "Iteration 70, loss = 0.52743246\n",
      "Iteration 71, loss = 0.52695621\n",
      "Iteration 72, loss = 0.52650983\n",
      "Iteration 73, loss = 0.52601563\n",
      "Iteration 74, loss = 0.52557764\n",
      "Iteration 75, loss = 0.52506691\n",
      "Iteration 76, loss = 0.52461192\n",
      "Iteration 77, loss = 0.52416656\n",
      "Iteration 78, loss = 0.52367602\n",
      "Iteration 79, loss = 0.52320448\n",
      "Iteration 80, loss = 0.52273658\n",
      "Iteration 81, loss = 0.52229549\n",
      "Iteration 82, loss = 0.52181111\n",
      "Iteration 83, loss = 0.52136326\n",
      "Iteration 84, loss = 0.52089929\n",
      "Iteration 85, loss = 0.52041549\n",
      "Iteration 86, loss = 0.51998271\n",
      "Iteration 87, loss = 0.51953895\n",
      "Iteration 88, loss = 0.51908824\n",
      "Iteration 89, loss = 0.51861175\n",
      "Iteration 90, loss = 0.51814073\n",
      "Iteration 91, loss = 0.51770821\n",
      "Iteration 92, loss = 0.51726316\n",
      "Iteration 93, loss = 0.51680130\n",
      "Iteration 94, loss = 0.51638794\n",
      "Iteration 95, loss = 0.51591395\n",
      "Iteration 96, loss = 0.51546849\n",
      "Iteration 97, loss = 0.51501931\n",
      "Iteration 98, loss = 0.51458059\n",
      "Iteration 99, loss = 0.51413897\n",
      "Iteration 100, loss = 0.51373401\n",
      "Iteration 101, loss = 0.51322953\n",
      "Iteration 102, loss = 0.51280534\n",
      "Iteration 103, loss = 0.51237928\n",
      "Iteration 104, loss = 0.51195598\n",
      "Iteration 105, loss = 0.51151268\n",
      "Iteration 106, loss = 0.51105848\n",
      "Iteration 107, loss = 0.51065353\n",
      "Iteration 108, loss = 0.51021279\n",
      "Iteration 109, loss = 0.50976634\n",
      "Iteration 110, loss = 0.50933607\n",
      "Iteration 111, loss = 0.50891481\n",
      "Iteration 112, loss = 0.50848805\n",
      "Iteration 113, loss = 0.50805805\n",
      "Iteration 114, loss = 0.50761875\n",
      "Iteration 115, loss = 0.50718754\n",
      "Iteration 116, loss = 0.50678344\n",
      "Iteration 117, loss = 0.50638355\n",
      "Iteration 118, loss = 0.50594877\n",
      "Iteration 119, loss = 0.50554531\n",
      "Iteration 120, loss = 0.50510717\n",
      "Iteration 121, loss = 0.50467318\n",
      "Iteration 122, loss = 0.50425886\n",
      "Iteration 123, loss = 0.50386319\n",
      "Iteration 124, loss = 0.50344693\n",
      "Iteration 125, loss = 0.50302224\n",
      "Iteration 126, loss = 0.50265553\n",
      "Iteration 127, loss = 0.50224237\n",
      "Iteration 128, loss = 0.50188156\n",
      "Iteration 129, loss = 0.50144584\n",
      "Iteration 130, loss = 0.50103665\n",
      "Iteration 131, loss = 0.50061783\n",
      "Iteration 132, loss = 0.50020011\n",
      "Iteration 133, loss = 0.49984307\n",
      "Iteration 134, loss = 0.49944334\n",
      "Iteration 135, loss = 0.49901122\n",
      "Iteration 136, loss = 0.49863078\n",
      "Iteration 137, loss = 0.49827578\n",
      "Iteration 138, loss = 0.49786912\n",
      "Iteration 139, loss = 0.49749264\n",
      "Iteration 140, loss = 0.49711637\n",
      "Iteration 141, loss = 0.49674277\n",
      "Iteration 142, loss = 0.49634966\n",
      "Iteration 143, loss = 0.49597117\n",
      "Iteration 144, loss = 0.49559948\n",
      "Iteration 145, loss = 0.49520930\n",
      "Iteration 146, loss = 0.49487074\n",
      "Iteration 147, loss = 0.49447916\n",
      "Iteration 148, loss = 0.49412761\n",
      "Iteration 149, loss = 0.49375539\n",
      "Iteration 150, loss = 0.49336918\n",
      "Iteration 151, loss = 0.49300638\n",
      "Iteration 152, loss = 0.49267523\n",
      "Iteration 153, loss = 0.49230578\n",
      "Iteration 154, loss = 0.49195444\n",
      "Iteration 155, loss = 0.49158829\n",
      "Iteration 156, loss = 0.49129138\n",
      "Iteration 157, loss = 0.49092012\n",
      "Iteration 158, loss = 0.49058309\n",
      "Iteration 159, loss = 0.49021397\n",
      "Iteration 160, loss = 0.48993528\n",
      "Iteration 161, loss = 0.48953307\n",
      "Iteration 162, loss = 0.48921304\n",
      "Iteration 163, loss = 0.48887430\n",
      "Iteration 164, loss = 0.48854260\n",
      "Iteration 165, loss = 0.48822914\n",
      "Iteration 166, loss = 0.48789171\n",
      "Iteration 167, loss = 0.48757005\n",
      "Iteration 168, loss = 0.48725243\n",
      "Iteration 169, loss = 0.48692980\n",
      "Iteration 170, loss = 0.48663849\n",
      "Iteration 171, loss = 0.48629884\n",
      "Iteration 172, loss = 0.48598246\n",
      "Iteration 173, loss = 0.48567330\n",
      "Iteration 174, loss = 0.48537776\n",
      "Iteration 175, loss = 0.48506785\n",
      "Iteration 176, loss = 0.48478802\n",
      "Iteration 177, loss = 0.48448586\n",
      "Iteration 178, loss = 0.48417727\n",
      "Iteration 179, loss = 0.48386978\n",
      "Iteration 180, loss = 0.48359118\n",
      "Iteration 181, loss = 0.48336913\n",
      "Iteration 182, loss = 0.48303595\n",
      "Iteration 183, loss = 0.48278591\n",
      "Iteration 184, loss = 0.48245199\n",
      "Iteration 185, loss = 0.48219114\n",
      "Iteration 186, loss = 0.48191831\n",
      "Iteration 187, loss = 0.48162911\n",
      "Iteration 188, loss = 0.48137580\n",
      "Iteration 189, loss = 0.48110672\n",
      "Iteration 190, loss = 0.48082841\n",
      "Iteration 191, loss = 0.48056565\n",
      "Iteration 192, loss = 0.48032300\n",
      "Iteration 193, loss = 0.48004315\n",
      "Iteration 194, loss = 0.47978613\n",
      "Iteration 195, loss = 0.47952280\n",
      "Iteration 196, loss = 0.47936843\n",
      "Iteration 197, loss = 0.47902772\n",
      "Iteration 198, loss = 0.47880110\n",
      "Iteration 199, loss = 0.47856142\n",
      "Iteration 200, loss = 0.47834638\n",
      "Iteration 201, loss = 0.47811309\n",
      "Iteration 202, loss = 0.47784769\n",
      "Iteration 203, loss = 0.47761844\n",
      "Iteration 204, loss = 0.47736474\n",
      "Iteration 205, loss = 0.47714473\n",
      "Iteration 206, loss = 0.47691512\n",
      "Iteration 207, loss = 0.47670786\n",
      "Iteration 208, loss = 0.47648019\n",
      "Iteration 209, loss = 0.47625621\n",
      "Iteration 210, loss = 0.47603516\n",
      "Iteration 211, loss = 0.47583392\n",
      "Iteration 212, loss = 0.47565511\n",
      "Iteration 213, loss = 0.47540602\n",
      "Iteration 214, loss = 0.47519353\n",
      "Iteration 215, loss = 0.47500689\n",
      "Iteration 216, loss = 0.47477778\n",
      "Iteration 217, loss = 0.47458604\n",
      "Iteration 218, loss = 0.47440187\n",
      "Iteration 219, loss = 0.47421779\n",
      "Iteration 220, loss = 0.47404708\n",
      "Iteration 221, loss = 0.47382750\n",
      "Iteration 222, loss = 0.47362187\n",
      "Iteration 223, loss = 0.47345101\n",
      "Iteration 224, loss = 0.47327072\n",
      "Iteration 225, loss = 0.47306446\n",
      "Iteration 226, loss = 0.47292802\n",
      "Iteration 227, loss = 0.47272960\n",
      "Iteration 228, loss = 0.47254825\n",
      "Iteration 229, loss = 0.47234997\n",
      "Iteration 230, loss = 0.47217274\n",
      "Iteration 231, loss = 0.47202791\n",
      "Iteration 232, loss = 0.47186193\n",
      "Iteration 233, loss = 0.47169517\n",
      "Iteration 234, loss = 0.47150868\n",
      "Iteration 235, loss = 0.47137760\n",
      "Iteration 236, loss = 0.47121404\n",
      "Iteration 237, loss = 0.47102820\n",
      "Iteration 238, loss = 0.47087931\n",
      "Iteration 239, loss = 0.47072184\n",
      "Iteration 240, loss = 0.47058054\n",
      "Iteration 241, loss = 0.47041615\n",
      "Iteration 242, loss = 0.47031243\n",
      "Iteration 243, loss = 0.47011502\n",
      "Iteration 244, loss = 0.46995566\n",
      "Iteration 245, loss = 0.46983556\n",
      "Iteration 246, loss = 0.46974459\n",
      "Iteration 247, loss = 0.46955618\n",
      "Iteration 248, loss = 0.46939222\n",
      "Iteration 249, loss = 0.46927273\n",
      "Iteration 250, loss = 0.46913698\n",
      "Iteration 251, loss = 0.46899094\n",
      "Iteration 252, loss = 0.46887930\n",
      "Iteration 253, loss = 0.46875605\n",
      "Iteration 254, loss = 0.46859993\n",
      "Iteration 255, loss = 0.46847907\n",
      "Iteration 256, loss = 0.46836538\n",
      "Iteration 257, loss = 0.46824849\n",
      "Iteration 258, loss = 0.46811523\n",
      "Iteration 259, loss = 0.46799264\n",
      "Iteration 260, loss = 0.46788948\n",
      "Iteration 261, loss = 0.46776796\n",
      "Iteration 262, loss = 0.46762560\n",
      "Iteration 263, loss = 0.46752673\n",
      "Iteration 264, loss = 0.46740749\n",
      "Iteration 265, loss = 0.46731051\n",
      "Iteration 266, loss = 0.46719868\n",
      "Iteration 267, loss = 0.46707736\n",
      "Iteration 268, loss = 0.46695911\n",
      "Iteration 269, loss = 0.46686952\n",
      "Iteration 270, loss = 0.46674943\n",
      "Iteration 271, loss = 0.46664366\n",
      "Iteration 272, loss = 0.46654383\n",
      "Iteration 273, loss = 0.46644359\n",
      "Iteration 274, loss = 0.46634135\n",
      "Iteration 275, loss = 0.46627603\n",
      "Iteration 276, loss = 0.46616309\n",
      "Iteration 277, loss = 0.46605803\n",
      "Iteration 278, loss = 0.46595589\n",
      "Iteration 279, loss = 0.46585717\n",
      "Iteration 280, loss = 0.46576670\n",
      "Iteration 281, loss = 0.46569441\n",
      "Iteration 282, loss = 0.46558365\n",
      "Iteration 283, loss = 0.46550160\n",
      "Iteration 284, loss = 0.46544406\n",
      "Iteration 285, loss = 0.46533925\n",
      "Iteration 286, loss = 0.46526472\n",
      "Iteration 287, loss = 0.46515845\n",
      "Iteration 288, loss = 0.46507140\n",
      "Iteration 289, loss = 0.46498402\n",
      "Iteration 290, loss = 0.46491682\n",
      "Iteration 291, loss = 0.46483317\n",
      "Iteration 292, loss = 0.46474920\n",
      "Iteration 293, loss = 0.46467381\n",
      "Iteration 294, loss = 0.46457964\n",
      "Iteration 295, loss = 0.46450884\n",
      "Iteration 296, loss = 0.46443889\n",
      "Iteration 297, loss = 0.46436836\n",
      "Iteration 298, loss = 0.46428692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56928370\n",
      "Iteration 2, loss = 0.56681091\n",
      "Iteration 3, loss = 0.56324680\n",
      "Iteration 4, loss = 0.55916041\n",
      "Iteration 5, loss = 0.55532748\n",
      "Iteration 6, loss = 0.55233608\n",
      "Iteration 7, loss = 0.54952262\n",
      "Iteration 8, loss = 0.54764991\n",
      "Iteration 9, loss = 0.54592333\n",
      "Iteration 10, loss = 0.54462708\n",
      "Iteration 11, loss = 0.54374365\n",
      "Iteration 12, loss = 0.54299313\n",
      "Iteration 13, loss = 0.54238552\n",
      "Iteration 14, loss = 0.54200660\n",
      "Iteration 15, loss = 0.54159894\n",
      "Iteration 16, loss = 0.54123605\n",
      "Iteration 17, loss = 0.54080578\n",
      "Iteration 18, loss = 0.54047780\n",
      "Iteration 19, loss = 0.54021894\n",
      "Iteration 20, loss = 0.53993753\n",
      "Iteration 21, loss = 0.53953670\n",
      "Iteration 22, loss = 0.53917593\n",
      "Iteration 23, loss = 0.53885939\n",
      "Iteration 24, loss = 0.53854943\n",
      "Iteration 25, loss = 0.53820489\n",
      "Iteration 26, loss = 0.53790271\n",
      "Iteration 27, loss = 0.53754550\n",
      "Iteration 28, loss = 0.53725935\n",
      "Iteration 29, loss = 0.53688974\n",
      "Iteration 30, loss = 0.53657344\n",
      "Iteration 31, loss = 0.53624369\n",
      "Iteration 32, loss = 0.53591607\n",
      "Iteration 33, loss = 0.53559357\n",
      "Iteration 34, loss = 0.53529556\n",
      "Iteration 35, loss = 0.53495363\n",
      "Iteration 36, loss = 0.53467819\n",
      "Iteration 37, loss = 0.53431374\n",
      "Iteration 38, loss = 0.53402330\n",
      "Iteration 39, loss = 0.53372084\n",
      "Iteration 40, loss = 0.53343104\n",
      "Iteration 41, loss = 0.53308041\n",
      "Iteration 42, loss = 0.53278214\n",
      "Iteration 43, loss = 0.53244519\n",
      "Iteration 44, loss = 0.53213494\n",
      "Iteration 45, loss = 0.53186204\n",
      "Iteration 46, loss = 0.53155974\n",
      "Iteration 47, loss = 0.53123987\n",
      "Iteration 48, loss = 0.53089475\n",
      "Iteration 49, loss = 0.53058988\n",
      "Iteration 50, loss = 0.53027585\n",
      "Iteration 51, loss = 0.52998571\n",
      "Iteration 52, loss = 0.52971932\n",
      "Iteration 53, loss = 0.52938215\n",
      "Iteration 54, loss = 0.52907752\n",
      "Iteration 55, loss = 0.52877051\n",
      "Iteration 56, loss = 0.52847741\n",
      "Iteration 57, loss = 0.52818959\n",
      "Iteration 58, loss = 0.52786714\n",
      "Iteration 59, loss = 0.52756838\n",
      "Iteration 60, loss = 0.52725588\n",
      "Iteration 61, loss = 0.52699116\n",
      "Iteration 62, loss = 0.52666823\n",
      "Iteration 63, loss = 0.52636042\n",
      "Iteration 64, loss = 0.52607032\n",
      "Iteration 65, loss = 0.52576538\n",
      "Iteration 66, loss = 0.52547166\n",
      "Iteration 67, loss = 0.52519379\n",
      "Iteration 68, loss = 0.52501623\n",
      "Iteration 69, loss = 0.52458000\n",
      "Iteration 70, loss = 0.52430202\n",
      "Iteration 71, loss = 0.52401238\n",
      "Iteration 72, loss = 0.52370790\n",
      "Iteration 73, loss = 0.52344085\n",
      "Iteration 74, loss = 0.52313758\n",
      "Iteration 75, loss = 0.52288349\n",
      "Iteration 76, loss = 0.52253437\n",
      "Iteration 77, loss = 0.52224496\n",
      "Iteration 78, loss = 0.52195684\n",
      "Iteration 79, loss = 0.52166386\n",
      "Iteration 80, loss = 0.52139253\n",
      "Iteration 81, loss = 0.52110260\n",
      "Iteration 82, loss = 0.52081543\n",
      "Iteration 83, loss = 0.52052855\n",
      "Iteration 84, loss = 0.52021562\n",
      "Iteration 85, loss = 0.51997534\n",
      "Iteration 86, loss = 0.51965207\n",
      "Iteration 87, loss = 0.51936691\n",
      "Iteration 88, loss = 0.51908458\n",
      "Iteration 89, loss = 0.51880463\n",
      "Iteration 90, loss = 0.51853307\n",
      "Iteration 91, loss = 0.51822285\n",
      "Iteration 92, loss = 0.51796228\n",
      "Iteration 93, loss = 0.51768394\n",
      "Iteration 94, loss = 0.51740370\n",
      "Iteration 95, loss = 0.51711183\n",
      "Iteration 96, loss = 0.51684294\n",
      "Iteration 97, loss = 0.51654588\n",
      "Iteration 98, loss = 0.51627307\n",
      "Iteration 99, loss = 0.51598611\n",
      "Iteration 100, loss = 0.51574091\n",
      "Iteration 101, loss = 0.51545243\n",
      "Iteration 102, loss = 0.51518563\n",
      "Iteration 103, loss = 0.51489885\n",
      "Iteration 104, loss = 0.51460787\n",
      "Iteration 105, loss = 0.51434649\n",
      "Iteration 106, loss = 0.51409125\n",
      "Iteration 107, loss = 0.51382986\n",
      "Iteration 108, loss = 0.51358956\n",
      "Iteration 109, loss = 0.51328944\n",
      "Iteration 110, loss = 0.51297369\n",
      "Iteration 111, loss = 0.51272588\n",
      "Iteration 112, loss = 0.51243721\n",
      "Iteration 113, loss = 0.51218733\n",
      "Iteration 114, loss = 0.51191734\n",
      "Iteration 115, loss = 0.51165104\n",
      "Iteration 116, loss = 0.51138712\n",
      "Iteration 117, loss = 0.51113900\n",
      "Iteration 118, loss = 0.51089865\n",
      "Iteration 119, loss = 0.51060282\n",
      "Iteration 120, loss = 0.51037612\n",
      "Iteration 121, loss = 0.51012177\n",
      "Iteration 122, loss = 0.50984260\n",
      "Iteration 123, loss = 0.50956242\n",
      "Iteration 124, loss = 0.50936372\n",
      "Iteration 125, loss = 0.50905664\n",
      "Iteration 126, loss = 0.50887446\n",
      "Iteration 127, loss = 0.50854050\n",
      "Iteration 128, loss = 0.50829235\n",
      "Iteration 129, loss = 0.50805579\n",
      "Iteration 130, loss = 0.50777722\n",
      "Iteration 131, loss = 0.50755144\n",
      "Iteration 132, loss = 0.50730344\n",
      "Iteration 133, loss = 0.50704128\n",
      "Iteration 134, loss = 0.50680807\n",
      "Iteration 135, loss = 0.50655868\n",
      "Iteration 136, loss = 0.50631450\n",
      "Iteration 137, loss = 0.50606973\n",
      "Iteration 138, loss = 0.50582667\n",
      "Iteration 139, loss = 0.50559227\n",
      "Iteration 140, loss = 0.50533805\n",
      "Iteration 141, loss = 0.50510777\n",
      "Iteration 142, loss = 0.50487222\n",
      "Iteration 143, loss = 0.50465088\n",
      "Iteration 144, loss = 0.50439197\n",
      "Iteration 145, loss = 0.50418129\n",
      "Iteration 146, loss = 0.50398792\n",
      "Iteration 147, loss = 0.50370886\n",
      "Iteration 148, loss = 0.50346982\n",
      "Iteration 149, loss = 0.50324915\n",
      "Iteration 150, loss = 0.50300164\n",
      "Iteration 151, loss = 0.50280162\n",
      "Iteration 152, loss = 0.50256253\n",
      "Iteration 153, loss = 0.50235757\n",
      "Iteration 154, loss = 0.50214675\n",
      "Iteration 155, loss = 0.50190290\n",
      "Iteration 156, loss = 0.50169179\n",
      "Iteration 157, loss = 0.50146047\n",
      "Iteration 158, loss = 0.50129328\n",
      "Iteration 159, loss = 0.50100832\n",
      "Iteration 160, loss = 0.50083496\n",
      "Iteration 161, loss = 0.50059661\n",
      "Iteration 162, loss = 0.50039229\n",
      "Iteration 163, loss = 0.50019486\n",
      "Iteration 164, loss = 0.49997598\n",
      "Iteration 165, loss = 0.49976783\n",
      "Iteration 166, loss = 0.49956215\n",
      "Iteration 167, loss = 0.49938189\n",
      "Iteration 168, loss = 0.49916684\n",
      "Iteration 169, loss = 0.49893825\n",
      "Iteration 170, loss = 0.49874992\n",
      "Iteration 171, loss = 0.49854282\n",
      "Iteration 172, loss = 0.49835178\n",
      "Iteration 173, loss = 0.49814891\n",
      "Iteration 174, loss = 0.49795627\n",
      "Iteration 175, loss = 0.49776294\n",
      "Iteration 176, loss = 0.49755344\n",
      "Iteration 177, loss = 0.49737516\n",
      "Iteration 178, loss = 0.49721358\n",
      "Iteration 179, loss = 0.49700525\n",
      "Iteration 180, loss = 0.49681136\n",
      "Iteration 181, loss = 0.49663395\n",
      "Iteration 182, loss = 0.49644368\n",
      "Iteration 183, loss = 0.49625302\n",
      "Iteration 184, loss = 0.49606483\n",
      "Iteration 185, loss = 0.49589119\n",
      "Iteration 186, loss = 0.49574840\n",
      "Iteration 187, loss = 0.49552512\n",
      "Iteration 188, loss = 0.49536274\n",
      "Iteration 189, loss = 0.49517685\n",
      "Iteration 190, loss = 0.49506310\n",
      "Iteration 191, loss = 0.49484958\n",
      "Iteration 192, loss = 0.49464769\n",
      "Iteration 193, loss = 0.49453480\n",
      "Iteration 194, loss = 0.49435390\n",
      "Iteration 195, loss = 0.49415809\n",
      "Iteration 196, loss = 0.49402416\n",
      "Iteration 197, loss = 0.49383515\n",
      "Iteration 198, loss = 0.49369524\n",
      "Iteration 199, loss = 0.49352040\n",
      "Iteration 200, loss = 0.49335172\n",
      "Iteration 201, loss = 0.49318300\n",
      "Iteration 202, loss = 0.49303377\n",
      "Iteration 203, loss = 0.49288252\n",
      "Iteration 204, loss = 0.49272660\n",
      "Iteration 205, loss = 0.49256615\n",
      "Iteration 206, loss = 0.49247043\n",
      "Iteration 207, loss = 0.49225637\n",
      "Iteration 208, loss = 0.49219553\n",
      "Iteration 209, loss = 0.49196202\n",
      "Iteration 210, loss = 0.49182809\n",
      "Iteration 211, loss = 0.49166792\n",
      "Iteration 212, loss = 0.49152989\n",
      "Iteration 213, loss = 0.49147033\n",
      "Iteration 214, loss = 0.49125043\n",
      "Iteration 215, loss = 0.49111253\n",
      "Iteration 216, loss = 0.49097477\n",
      "Iteration 217, loss = 0.49081595\n",
      "Iteration 218, loss = 0.49071356\n",
      "Iteration 219, loss = 0.49058500\n",
      "Iteration 220, loss = 0.49040859\n",
      "Iteration 221, loss = 0.49030712\n",
      "Iteration 222, loss = 0.49015746\n",
      "Iteration 223, loss = 0.49003619\n",
      "Iteration 224, loss = 0.48990552\n",
      "Iteration 225, loss = 0.48976177\n",
      "Iteration 226, loss = 0.48965419\n",
      "Iteration 227, loss = 0.48951763\n",
      "Iteration 228, loss = 0.48942288\n",
      "Iteration 229, loss = 0.48927893\n",
      "Iteration 230, loss = 0.48915742\n",
      "Iteration 231, loss = 0.48902545\n",
      "Iteration 232, loss = 0.48889034\n",
      "Iteration 233, loss = 0.48881397\n",
      "Iteration 234, loss = 0.48867535\n",
      "Iteration 235, loss = 0.48859387\n",
      "Iteration 236, loss = 0.48842540\n",
      "Iteration 237, loss = 0.48831084\n",
      "Iteration 238, loss = 0.48821063\n",
      "Iteration 239, loss = 0.48808306\n",
      "Iteration 240, loss = 0.48797717\n",
      "Iteration 241, loss = 0.48787689\n",
      "Iteration 242, loss = 0.48776417\n",
      "Iteration 243, loss = 0.48765562\n",
      "Iteration 244, loss = 0.48753897\n",
      "Iteration 245, loss = 0.48745077\n",
      "Iteration 246, loss = 0.48739487\n",
      "Iteration 247, loss = 0.48726547\n",
      "Iteration 248, loss = 0.48713510\n",
      "Iteration 249, loss = 0.48703330\n",
      "Iteration 250, loss = 0.48692270\n",
      "Iteration 251, loss = 0.48682995\n",
      "Iteration 252, loss = 0.48673921\n",
      "Iteration 253, loss = 0.48663678\n",
      "Iteration 254, loss = 0.48654341\n",
      "Iteration 255, loss = 0.48645083\n",
      "Iteration 256, loss = 0.48635210\n",
      "Iteration 257, loss = 0.48625050\n",
      "Iteration 258, loss = 0.48618157\n",
      "Iteration 259, loss = 0.48605826\n",
      "Iteration 260, loss = 0.48598220\n",
      "Iteration 261, loss = 0.48591299\n",
      "Iteration 262, loss = 0.48580766\n",
      "Iteration 263, loss = 0.48571115\n",
      "Iteration 264, loss = 0.48562469\n",
      "Iteration 265, loss = 0.48555431\n",
      "Iteration 266, loss = 0.48547889\n",
      "Iteration 267, loss = 0.48537336\n",
      "Iteration 268, loss = 0.48526943\n",
      "Iteration 269, loss = 0.48520946\n",
      "Iteration 270, loss = 0.48513107\n",
      "Iteration 271, loss = 0.48503325\n",
      "Iteration 272, loss = 0.48499681\n",
      "Iteration 273, loss = 0.48487841\n",
      "Iteration 274, loss = 0.48484444\n",
      "Iteration 275, loss = 0.48471699\n",
      "Iteration 276, loss = 0.48464493\n",
      "Iteration 277, loss = 0.48455197\n",
      "Iteration 278, loss = 0.48448279\n",
      "Iteration 279, loss = 0.48442398\n",
      "Iteration 280, loss = 0.48436506\n",
      "Iteration 281, loss = 0.48426578\n",
      "Iteration 282, loss = 0.48420587\n",
      "Iteration 283, loss = 0.48412010\n",
      "Iteration 284, loss = 0.48406617\n",
      "Iteration 285, loss = 0.48397969\n",
      "Iteration 286, loss = 0.48390005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anapedroso/Documentos/IA-desempenho-algoritmos-classificacao/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(transfusion_values, transfusion_classes):\n",
    "    data_train, target_train = transfusion_values[train], transfusion_classes[train]\n",
    "    data_test, target_test = transfusion_values[test], transfusion_classes[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77       570\n",
      "           1       0.28      0.29      0.28       178\n",
      "\n",
      "    accuracy                           0.65       748\n",
      "   macro avg       0.52      0.53      0.53       748\n",
      "weighted avg       0.66      0.65      0.65       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[433 137]\n",
      " [126  52]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       570\n",
      "           1       0.24      0.29      0.26       178\n",
      "\n",
      "    accuracy                           0.62       748\n",
      "   macro avg       0.50      0.50      0.50       748\n",
      "weighted avg       0.64      0.62      0.63       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[411 159]\n",
      " [127  51]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       570\n",
      "           1       0.45      0.22      0.30       178\n",
      "\n",
      "    accuracy                           0.75       748\n",
      "   macro avg       0.62      0.57      0.57       748\n",
      "weighted avg       0.71      0.75      0.72       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[522  48]\n",
      " [138  40]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.87       570\n",
      "           1       0.59      0.27      0.37       178\n",
      "\n",
      "    accuracy                           0.78       748\n",
      "   macro avg       0.70      0.61      0.62       748\n",
      "weighted avg       0.75      0.78      0.75       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[536  34]\n",
      " [130  48]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86       570\n",
      "           1       0.56      0.22      0.31       178\n",
      "\n",
      "    accuracy                           0.77       748\n",
      "   macro avg       0.68      0.58      0.59       748\n",
      "weighted avg       0.74      0.77      0.73       748\n",
      "\n",
      "\n",
      "Matriz de confusÃ£o: \n",
      "[[539  31]\n",
      " [139  39]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(transfusion_classes, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusÃ£o: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(transfusion_classes, predicted_classes[classificador]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
