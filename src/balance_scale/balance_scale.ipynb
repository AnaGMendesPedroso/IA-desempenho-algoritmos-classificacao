{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Balance Scale:\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Class-Name  Left-Weight  Left-Distance  Right-Weight  Right-Distance\n0            B            1              1             1               1\n1            R            1              1             1               2\n2            R            1              1             1               3\n3            R            1              1             1               4\n4            R            1              1             1               5\n..         ...          ...            ...           ...             ...\n620          L            5              5             5               1\n621          L            5              5             5               2\n622          L            5              5             5               3\n623          L            5              5             5               4\n624          B            5              5             5               5\n\n[625 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class-Name</th>\n      <th>Left-Weight</th>\n      <th>Left-Distance</th>\n      <th>Right-Weight</th>\n      <th>Right-Distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>L</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>621</th>\n      <td>L</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>L</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>L</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>624</th>\n      <td>B</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>625 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "balanceScale = pd.read_table('balance-scale.data', sep=',')\n",
    "print(\"\\nDataset Balance Scale:\")\n",
    "balanceScale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Class-Name', 'Left-Weight', 'Left-Distance', 'Right-Weight',\n       'Right-Distance'],\n      dtype='object')"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanceScale.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "     Left-Weight  Left-Distance  Right-Weight  Right-Distance    0    1    2\n0              1              1             1               1  1.0  0.0  0.0\n1              1              1             1               2  0.0  0.0  1.0\n2              1              1             1               3  0.0  0.0  1.0\n3              1              1             1               4  0.0  0.0  1.0\n4              1              1             1               5  0.0  0.0  1.0\n..           ...            ...           ...             ...  ...  ...  ...\n620            5              5             5               1  0.0  1.0  0.0\n621            5              5             5               2  0.0  1.0  0.0\n622            5              5             5               3  0.0  1.0  0.0\n623            5              5             5               4  0.0  1.0  0.0\n624            5              5             5               5  1.0  0.0  0.0\n\n[625 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Left-Weight</th>\n      <th>Left-Distance</th>\n      <th>Right-Weight</th>\n      <th>Right-Distance</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>621</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>624</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>625 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanceScaleNormalized = balanceScale.values.copy()\n",
    "normalizador =  StandardScaler()\n",
    "\n",
    "# Codificar coluna que tenha caractere para numero e depois remover a coluna\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(balanceScale[['Class-Name']]).toarray())\n",
    "balanceScale = balanceScale.join(enc_df)\n",
    "balanceScale = balanceScale.drop(columns = ['Class-Name'])\n",
    "balanceScale\n",
    "\n",
    "# balanceScaleNormalized = normalizador.fit_transform(balanceScale)\n",
    "# balanceScale['Class-Name'] = balanceScaleNormalized[:,0]\n",
    "# balanceScale['Left-Weight'] = balanceScaleNormalized[:,1]\n",
    "# balanceScale['Left-Distance'] = balanceScaleNormalized[:,2]\n",
    "# balanceScale['Right-Weight'] = balanceScaleNormalized[:,3]\n",
    "# balanceScale['Right-Distance'] = balanceScaleNormalized[:,4]\n",
    "#\n",
    "# print(\"\\nDataset Balance Scale Normalized:\")\n",
    "# balanceScale\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balance Scale features:\n",
      "\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 2. 0.]\n",
      " [1. 1. 1. 3. 0.]\n",
      " ...\n",
      " [5. 5. 5. 3. 0.]\n",
      " [5. 5. 5. 4. 0.]\n",
      " [5. 5. 5. 5. 1.]]\n"
     ]
    }
   ],
   "source": [
    "balanceScaleValues = balanceScale.iloc[:,0:5].values\n",
    "print(\"\\nBalance Scale features:\\n\")\n",
    "print(balanceScaleValues)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balance Scale classes:\n",
      "\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1.]\n",
      "\n",
      "Balance Scale classes shape:\n",
      "(625,)\n"
     ]
    }
   ],
   "source": [
    "balanceScaleClasses = balanceScale.iloc[:,4].values\n",
    "print(\"\\nBalance Scale classes:\\n\")\n",
    "print(balanceScaleClasses)\n",
    "print(\"\\nBalance Scale classes shape:\")\n",
    "print(balanceScaleClasses.shape)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "arvore_decisao = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "vizinhos_proximos = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "naive_bayes_gaussian = GaussianNB()\n",
    "regressao_logistica = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='saga', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "rede_neural = MLPClassifier(hidden_layer_sizes=(5,), activation=\"logistic\", max_iter= 300, alpha=0.001, solver=\"sgd\", tol=1e-4, verbose=True, learning_rate_init=.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "predicted_classes = dict()\n",
    "predicted_classes['arvore_decisao'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['vizinhos_proximos'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['naive_bayes_gaussian'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['regressao_logistica'] = np.zeros(balanceScaleClasses.shape)\n",
    "predicted_classes['rede_neural'] = np.zeros(balanceScaleClasses.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63173028\n",
      "Iteration 2, loss = 0.60026911\n",
      "Iteration 3, loss = 0.55577056\n",
      "Iteration 4, loss = 0.50735210\n",
      "Iteration 5, loss = 0.46045838\n",
      "Iteration 6, loss = 0.41987167\n",
      "Iteration 7, loss = 0.38485362\n",
      "Iteration 8, loss = 0.35717595\n",
      "Iteration 9, loss = 0.33548313\n",
      "Iteration 10, loss = 0.31901997\n",
      "Iteration 11, loss = 0.30692295\n",
      "Iteration 12, loss = 0.29684170\n",
      "Iteration 13, loss = 0.29052391\n",
      "Iteration 14, loss = 0.28600121\n",
      "Iteration 15, loss = 0.28174844\n",
      "Iteration 16, loss = 0.27916865\n",
      "Iteration 17, loss = 0.27722391\n",
      "Iteration 18, loss = 0.27563980\n",
      "Iteration 19, loss = 0.27467011\n",
      "Iteration 20, loss = 0.27381200\n",
      "Iteration 21, loss = 0.27318269\n",
      "Iteration 22, loss = 0.27287749\n",
      "Iteration 23, loss = 0.27242075\n",
      "Iteration 24, loss = 0.27214715\n",
      "Iteration 25, loss = 0.27198579\n",
      "Iteration 26, loss = 0.27182084\n",
      "Iteration 27, loss = 0.27165425\n",
      "Iteration 28, loss = 0.27154512\n",
      "Iteration 29, loss = 0.27152954\n",
      "Iteration 30, loss = 0.27140283\n",
      "Iteration 31, loss = 0.27131524\n",
      "Iteration 32, loss = 0.27128204\n",
      "Iteration 33, loss = 0.27121716\n",
      "Iteration 34, loss = 0.27113408\n",
      "Iteration 35, loss = 0.27108524\n",
      "Iteration 36, loss = 0.27104347\n",
      "Iteration 37, loss = 0.27097353\n",
      "Iteration 38, loss = 0.27092304\n",
      "Iteration 39, loss = 0.27087907\n",
      "Iteration 40, loss = 0.27078571\n",
      "Iteration 41, loss = 0.27074041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61859155\n",
      "Iteration 2, loss = 0.58898474\n",
      "Iteration 3, loss = 0.54647175\n",
      "Iteration 4, loss = 0.49965405\n",
      "Iteration 5, loss = 0.45541143\n",
      "Iteration 6, loss = 0.41620277\n",
      "Iteration 7, loss = 0.38360045\n",
      "Iteration 8, loss = 0.35852236\n",
      "Iteration 9, loss = 0.33818894\n",
      "Iteration 10, loss = 0.32338509\n",
      "Iteration 11, loss = 0.31162578\n",
      "Iteration 12, loss = 0.30370642\n",
      "Iteration 13, loss = 0.29766334\n",
      "Iteration 14, loss = 0.29286369\n",
      "Iteration 15, loss = 0.28954024\n",
      "Iteration 16, loss = 0.28695100\n",
      "Iteration 17, loss = 0.28504139\n",
      "Iteration 18, loss = 0.28359877\n",
      "Iteration 19, loss = 0.28255455\n",
      "Iteration 20, loss = 0.28164122\n",
      "Iteration 21, loss = 0.28102605\n",
      "Iteration 22, loss = 0.28050716\n",
      "Iteration 23, loss = 0.28012919\n",
      "Iteration 24, loss = 0.27982767\n",
      "Iteration 25, loss = 0.27954643\n",
      "Iteration 26, loss = 0.27935306\n",
      "Iteration 27, loss = 0.27911660\n",
      "Iteration 28, loss = 0.27900520\n",
      "Iteration 29, loss = 0.27884818\n",
      "Iteration 30, loss = 0.27876546\n",
      "Iteration 31, loss = 0.27862534\n",
      "Iteration 32, loss = 0.27853377\n",
      "Iteration 33, loss = 0.27847273\n",
      "Iteration 34, loss = 0.27836492\n",
      "Iteration 35, loss = 0.27832236\n",
      "Iteration 36, loss = 0.27822771\n",
      "Iteration 37, loss = 0.27816861\n",
      "Iteration 38, loss = 0.27809838\n",
      "Iteration 39, loss = 0.27803402\n",
      "Iteration 40, loss = 0.27797439\n",
      "Iteration 41, loss = 0.27793314\n",
      "Iteration 42, loss = 0.27784547\n",
      "Iteration 43, loss = 0.27779716\n",
      "Iteration 44, loss = 0.27773920\n",
      "Iteration 45, loss = 0.27769204\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48259793\n",
      "Iteration 2, loss = 0.46144498\n",
      "Iteration 3, loss = 0.43221593\n",
      "Iteration 4, loss = 0.40251984\n",
      "Iteration 5, loss = 0.37510548\n",
      "Iteration 6, loss = 0.35139741\n",
      "Iteration 7, loss = 0.33235558\n",
      "Iteration 8, loss = 0.31785505\n",
      "Iteration 9, loss = 0.30656461\n",
      "Iteration 10, loss = 0.29829269\n",
      "Iteration 11, loss = 0.29156721\n",
      "Iteration 12, loss = 0.28674944\n",
      "Iteration 13, loss = 0.28313497\n",
      "Iteration 14, loss = 0.28021748\n",
      "Iteration 15, loss = 0.27834710\n",
      "Iteration 16, loss = 0.27648812\n",
      "Iteration 17, loss = 0.27544992\n",
      "Iteration 18, loss = 0.27437861\n",
      "Iteration 19, loss = 0.27377218\n",
      "Iteration 20, loss = 0.27320609\n",
      "Iteration 21, loss = 0.27267408\n",
      "Iteration 22, loss = 0.27235051\n",
      "Iteration 23, loss = 0.27212054\n",
      "Iteration 24, loss = 0.27182076\n",
      "Iteration 25, loss = 0.27165475\n",
      "Iteration 26, loss = 0.27146055\n",
      "Iteration 27, loss = 0.27141659\n",
      "Iteration 28, loss = 0.27127929\n",
      "Iteration 29, loss = 0.27117707\n",
      "Iteration 30, loss = 0.27106616\n",
      "Iteration 31, loss = 0.27104906\n",
      "Iteration 32, loss = 0.27095911\n",
      "Iteration 33, loss = 0.27089088\n",
      "Iteration 34, loss = 0.27084943\n",
      "Iteration 35, loss = 0.27078602\n",
      "Iteration 36, loss = 0.27076034\n",
      "Iteration 37, loss = 0.27070927\n",
      "Iteration 38, loss = 0.27065195\n",
      "Iteration 39, loss = 0.27063558\n",
      "Iteration 40, loss = 0.27055539\n",
      "Iteration 41, loss = 0.27050986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72999188\n",
      "Iteration 2, loss = 0.68581051\n",
      "Iteration 3, loss = 0.62477757\n",
      "Iteration 4, loss = 0.56062131\n",
      "Iteration 5, loss = 0.50075721\n",
      "Iteration 6, loss = 0.44821968\n",
      "Iteration 7, loss = 0.40705300\n",
      "Iteration 8, loss = 0.37441526\n",
      "Iteration 9, loss = 0.34947211\n",
      "Iteration 10, loss = 0.33100976\n",
      "Iteration 11, loss = 0.31612371\n",
      "Iteration 12, loss = 0.30671378\n",
      "Iteration 13, loss = 0.29830795\n",
      "Iteration 14, loss = 0.29306128\n",
      "Iteration 15, loss = 0.28849057\n",
      "Iteration 16, loss = 0.28529740\n",
      "Iteration 17, loss = 0.28327393\n",
      "Iteration 18, loss = 0.28119016\n",
      "Iteration 19, loss = 0.27983791\n",
      "Iteration 20, loss = 0.27863577\n",
      "Iteration 21, loss = 0.27785302\n",
      "Iteration 22, loss = 0.27730623\n",
      "Iteration 23, loss = 0.27664510\n",
      "Iteration 24, loss = 0.27639821\n",
      "Iteration 25, loss = 0.27595067\n",
      "Iteration 26, loss = 0.27571955\n",
      "Iteration 27, loss = 0.27547654\n",
      "Iteration 28, loss = 0.27531714\n",
      "Iteration 29, loss = 0.27519695\n",
      "Iteration 30, loss = 0.27498756\n",
      "Iteration 31, loss = 0.27490245\n",
      "Iteration 32, loss = 0.27479697\n",
      "Iteration 33, loss = 0.27470532\n",
      "Iteration 34, loss = 0.27461763\n",
      "Iteration 35, loss = 0.27453121\n",
      "Iteration 36, loss = 0.27446475\n",
      "Iteration 37, loss = 0.27441238\n",
      "Iteration 38, loss = 0.27434603\n",
      "Iteration 39, loss = 0.27428247\n",
      "Iteration 40, loss = 0.27423095\n",
      "Iteration 41, loss = 0.27418553\n",
      "Iteration 42, loss = 0.27415828\n",
      "Iteration 43, loss = 0.27409957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52797676\n",
      "Iteration 2, loss = 0.50447554\n",
      "Iteration 3, loss = 0.47243011\n",
      "Iteration 4, loss = 0.43814135\n",
      "Iteration 5, loss = 0.40578380\n",
      "Iteration 6, loss = 0.37748318\n",
      "Iteration 7, loss = 0.35441046\n",
      "Iteration 8, loss = 0.33569045\n",
      "Iteration 9, loss = 0.32105089\n",
      "Iteration 10, loss = 0.30989714\n",
      "Iteration 11, loss = 0.30153169\n",
      "Iteration 12, loss = 0.29501350\n",
      "Iteration 13, loss = 0.29019504\n",
      "Iteration 14, loss = 0.28643271\n",
      "Iteration 15, loss = 0.28402833\n",
      "Iteration 16, loss = 0.28193565\n",
      "Iteration 17, loss = 0.28048954\n",
      "Iteration 18, loss = 0.27938564\n",
      "Iteration 19, loss = 0.27839895\n",
      "Iteration 20, loss = 0.27778703\n",
      "Iteration 21, loss = 0.27726009\n",
      "Iteration 22, loss = 0.27695044\n",
      "Iteration 23, loss = 0.27665845\n",
      "Iteration 24, loss = 0.27642533\n",
      "Iteration 25, loss = 0.27616030\n",
      "Iteration 26, loss = 0.27598646\n",
      "Iteration 27, loss = 0.27584544\n",
      "Iteration 28, loss = 0.27573699\n",
      "Iteration 29, loss = 0.27564651\n",
      "Iteration 30, loss = 0.27551068\n",
      "Iteration 31, loss = 0.27541704\n",
      "Iteration 32, loss = 0.27532361\n",
      "Iteration 33, loss = 0.27521473\n",
      "Iteration 34, loss = 0.27513454\n",
      "Iteration 35, loss = 0.27503494\n",
      "Iteration 36, loss = 0.27494755\n",
      "Iteration 37, loss = 0.27484287\n",
      "Iteration 38, loss = 0.27477160\n",
      "Iteration 39, loss = 0.27467099\n",
      "Iteration 40, loss = 0.27457683\n",
      "Iteration 41, loss = 0.27448731\n",
      "Iteration 42, loss = 0.27440179\n",
      "Iteration 43, loss = 0.27428473\n",
      "Iteration 44, loss = 0.27421379\n",
      "Iteration 45, loss = 0.27409294\n",
      "Iteration 46, loss = 0.27400201\n",
      "Iteration 47, loss = 0.27390820\n",
      "Iteration 48, loss = 0.27384871\n",
      "Iteration 49, loss = 0.27370890\n",
      "Iteration 50, loss = 0.27360866\n",
      "Iteration 51, loss = 0.27350047\n",
      "Iteration 52, loss = 0.27340539\n",
      "Iteration 53, loss = 0.27333887\n",
      "Iteration 54, loss = 0.27321119\n",
      "Iteration 55, loss = 0.27311550\n",
      "Iteration 56, loss = 0.27301385\n",
      "Iteration 57, loss = 0.27294426\n",
      "Iteration 58, loss = 0.27280333\n",
      "Iteration 59, loss = 0.27269959\n",
      "Iteration 60, loss = 0.27259151\n",
      "Iteration 61, loss = 0.27249503\n",
      "Iteration 62, loss = 0.27238617\n",
      "Iteration 63, loss = 0.27228534\n",
      "Iteration 64, loss = 0.27217772\n",
      "Iteration 65, loss = 0.27207114\n",
      "Iteration 66, loss = 0.27196280\n",
      "Iteration 67, loss = 0.27185632\n",
      "Iteration 68, loss = 0.27176043\n",
      "Iteration 69, loss = 0.27164501\n",
      "Iteration 70, loss = 0.27154172\n",
      "Iteration 71, loss = 0.27142385\n",
      "Iteration 72, loss = 0.27130651\n",
      "Iteration 73, loss = 0.27118505\n",
      "Iteration 74, loss = 0.27107614\n",
      "Iteration 75, loss = 0.27096680\n",
      "Iteration 76, loss = 0.27086298\n",
      "Iteration 77, loss = 0.27072127\n",
      "Iteration 78, loss = 0.27060550\n",
      "Iteration 79, loss = 0.27048268\n",
      "Iteration 80, loss = 0.27035400\n",
      "Iteration 81, loss = 0.27025967\n",
      "Iteration 82, loss = 0.27012467\n",
      "Iteration 83, loss = 0.27000079\n",
      "Iteration 84, loss = 0.26987981\n",
      "Iteration 85, loss = 0.26976089\n",
      "Iteration 86, loss = 0.26964661\n",
      "Iteration 87, loss = 0.26951743\n",
      "Iteration 88, loss = 0.26938601\n",
      "Iteration 89, loss = 0.26925547\n",
      "Iteration 90, loss = 0.26915163\n",
      "Iteration 91, loss = 0.26900330\n",
      "Iteration 92, loss = 0.26887738\n",
      "Iteration 93, loss = 0.26873537\n",
      "Iteration 94, loss = 0.26860085\n",
      "Iteration 95, loss = 0.26847358\n",
      "Iteration 96, loss = 0.26833627\n",
      "Iteration 97, loss = 0.26819966\n",
      "Iteration 98, loss = 0.26805811\n",
      "Iteration 99, loss = 0.26796179\n",
      "Iteration 100, loss = 0.26777746\n",
      "Iteration 101, loss = 0.26763229\n",
      "Iteration 102, loss = 0.26750051\n",
      "Iteration 103, loss = 0.26733196\n",
      "Iteration 104, loss = 0.26721995\n",
      "Iteration 105, loss = 0.26704455\n",
      "Iteration 106, loss = 0.26692291\n",
      "Iteration 107, loss = 0.26673086\n",
      "Iteration 108, loss = 0.26658993\n",
      "Iteration 109, loss = 0.26643201\n",
      "Iteration 110, loss = 0.26626683\n",
      "Iteration 111, loss = 0.26610598\n",
      "Iteration 112, loss = 0.26596013\n",
      "Iteration 113, loss = 0.26578631\n",
      "Iteration 114, loss = 0.26561274\n",
      "Iteration 115, loss = 0.26546643\n",
      "Iteration 116, loss = 0.26527428\n",
      "Iteration 117, loss = 0.26509510\n",
      "Iteration 118, loss = 0.26493889\n",
      "Iteration 119, loss = 0.26477176\n",
      "Iteration 120, loss = 0.26457899\n",
      "Iteration 121, loss = 0.26440423\n",
      "Iteration 122, loss = 0.26423496\n",
      "Iteration 123, loss = 0.26404958\n",
      "Iteration 124, loss = 0.26385301\n",
      "Iteration 125, loss = 0.26369065\n",
      "Iteration 126, loss = 0.26349088\n",
      "Iteration 127, loss = 0.26329553\n",
      "Iteration 128, loss = 0.26311088\n",
      "Iteration 129, loss = 0.26291012\n",
      "Iteration 130, loss = 0.26271022\n",
      "Iteration 131, loss = 0.26254717\n",
      "Iteration 132, loss = 0.26231613\n",
      "Iteration 133, loss = 0.26213550\n",
      "Iteration 134, loss = 0.26191178\n",
      "Iteration 135, loss = 0.26171200\n",
      "Iteration 136, loss = 0.26150744\n",
      "Iteration 137, loss = 0.26129697\n",
      "Iteration 138, loss = 0.26108821\n",
      "Iteration 139, loss = 0.26087759\n",
      "Iteration 140, loss = 0.26069465\n",
      "Iteration 141, loss = 0.26045276\n",
      "Iteration 142, loss = 0.26023473\n",
      "Iteration 143, loss = 0.26000240\n",
      "Iteration 144, loss = 0.25978149\n",
      "Iteration 145, loss = 0.25954466\n",
      "Iteration 146, loss = 0.25931102\n",
      "Iteration 147, loss = 0.25909152\n",
      "Iteration 148, loss = 0.25884852\n",
      "Iteration 149, loss = 0.25861416\n",
      "Iteration 150, loss = 0.25837411\n",
      "Iteration 151, loss = 0.25814195\n",
      "Iteration 152, loss = 0.25790170\n",
      "Iteration 153, loss = 0.25765539\n",
      "Iteration 154, loss = 0.25743197\n",
      "Iteration 155, loss = 0.25718276\n",
      "Iteration 156, loss = 0.25692888\n",
      "Iteration 157, loss = 0.25668814\n",
      "Iteration 158, loss = 0.25642179\n",
      "Iteration 159, loss = 0.25618176\n",
      "Iteration 160, loss = 0.25591907\n",
      "Iteration 161, loss = 0.25565200\n",
      "Iteration 162, loss = 0.25538992\n",
      "Iteration 163, loss = 0.25513332\n",
      "Iteration 164, loss = 0.25488878\n",
      "Iteration 165, loss = 0.25463442\n",
      "Iteration 166, loss = 0.25435738\n",
      "Iteration 167, loss = 0.25407860\n",
      "Iteration 168, loss = 0.25380689\n",
      "Iteration 169, loss = 0.25355427\n",
      "Iteration 170, loss = 0.25327613\n",
      "Iteration 171, loss = 0.25302407\n",
      "Iteration 172, loss = 0.25271404\n",
      "Iteration 173, loss = 0.25243191\n",
      "Iteration 174, loss = 0.25215749\n",
      "Iteration 175, loss = 0.25186777\n",
      "Iteration 176, loss = 0.25160255\n",
      "Iteration 177, loss = 0.25130683\n",
      "Iteration 178, loss = 0.25101372\n",
      "Iteration 179, loss = 0.25073530\n",
      "Iteration 180, loss = 0.25045787\n",
      "Iteration 181, loss = 0.25015555\n",
      "Iteration 182, loss = 0.24987716\n",
      "Iteration 183, loss = 0.24958272\n",
      "Iteration 184, loss = 0.24929218\n",
      "Iteration 185, loss = 0.24901301\n",
      "Iteration 186, loss = 0.24870358\n",
      "Iteration 187, loss = 0.24841131\n",
      "Iteration 188, loss = 0.24811654\n",
      "Iteration 189, loss = 0.24781549\n",
      "Iteration 190, loss = 0.24752465\n",
      "Iteration 191, loss = 0.24721695\n",
      "Iteration 192, loss = 0.24693920\n",
      "Iteration 193, loss = 0.24663021\n",
      "Iteration 194, loss = 0.24633228\n",
      "Iteration 195, loss = 0.24600965\n",
      "Iteration 196, loss = 0.24572456\n",
      "Iteration 197, loss = 0.24541201\n",
      "Iteration 198, loss = 0.24512066\n",
      "Iteration 199, loss = 0.24480437\n",
      "Iteration 200, loss = 0.24450086\n",
      "Iteration 201, loss = 0.24420462\n",
      "Iteration 202, loss = 0.24387512\n",
      "Iteration 203, loss = 0.24355899\n",
      "Iteration 204, loss = 0.24325086\n",
      "Iteration 205, loss = 0.24294655\n",
      "Iteration 206, loss = 0.24262323\n",
      "Iteration 207, loss = 0.24229934\n",
      "Iteration 208, loss = 0.24200225\n",
      "Iteration 209, loss = 0.24166153\n",
      "Iteration 210, loss = 0.24135118\n",
      "Iteration 211, loss = 0.24103691\n",
      "Iteration 212, loss = 0.24074742\n",
      "Iteration 213, loss = 0.24036591\n",
      "Iteration 214, loss = 0.24003723\n",
      "Iteration 215, loss = 0.23971979\n",
      "Iteration 216, loss = 0.23938692\n",
      "Iteration 217, loss = 0.23907354\n",
      "Iteration 218, loss = 0.23870964\n",
      "Iteration 219, loss = 0.23837564\n",
      "Iteration 220, loss = 0.23803660\n",
      "Iteration 221, loss = 0.23772632\n",
      "Iteration 222, loss = 0.23740000\n",
      "Iteration 223, loss = 0.23701405\n",
      "Iteration 224, loss = 0.23667549\n",
      "Iteration 225, loss = 0.23632824\n",
      "Iteration 226, loss = 0.23598705\n",
      "Iteration 227, loss = 0.23567962\n",
      "Iteration 228, loss = 0.23529549\n",
      "Iteration 229, loss = 0.23491519\n",
      "Iteration 230, loss = 0.23454107\n",
      "Iteration 231, loss = 0.23419953\n",
      "Iteration 232, loss = 0.23383229\n",
      "Iteration 233, loss = 0.23346674\n",
      "Iteration 234, loss = 0.23309459\n",
      "Iteration 235, loss = 0.23273335\n",
      "Iteration 236, loss = 0.23235995\n",
      "Iteration 237, loss = 0.23200507\n",
      "Iteration 238, loss = 0.23167245\n",
      "Iteration 239, loss = 0.23126081\n",
      "Iteration 240, loss = 0.23090801\n",
      "Iteration 241, loss = 0.23052664\n",
      "Iteration 242, loss = 0.23016846\n",
      "Iteration 243, loss = 0.22981022\n",
      "Iteration 244, loss = 0.22941993\n",
      "Iteration 245, loss = 0.22902037\n",
      "Iteration 246, loss = 0.22864496\n",
      "Iteration 247, loss = 0.22826815\n",
      "Iteration 248, loss = 0.22787852\n",
      "Iteration 249, loss = 0.22749908\n",
      "Iteration 250, loss = 0.22712639\n",
      "Iteration 251, loss = 0.22672898\n",
      "Iteration 252, loss = 0.22637407\n",
      "Iteration 253, loss = 0.22594753\n",
      "Iteration 254, loss = 0.22559397\n",
      "Iteration 255, loss = 0.22516014\n",
      "Iteration 256, loss = 0.22477087\n",
      "Iteration 257, loss = 0.22436055\n",
      "Iteration 258, loss = 0.22399234\n",
      "Iteration 259, loss = 0.22356039\n",
      "Iteration 260, loss = 0.22316024\n",
      "Iteration 261, loss = 0.22276623\n",
      "Iteration 262, loss = 0.22234096\n",
      "Iteration 263, loss = 0.22194658\n",
      "Iteration 264, loss = 0.22152325\n",
      "Iteration 265, loss = 0.22110522\n",
      "Iteration 266, loss = 0.22070424\n",
      "Iteration 267, loss = 0.22028554\n",
      "Iteration 268, loss = 0.21985230\n",
      "Iteration 269, loss = 0.21943820\n",
      "Iteration 270, loss = 0.21901655\n",
      "Iteration 271, loss = 0.21856687\n",
      "Iteration 272, loss = 0.21813675\n",
      "Iteration 273, loss = 0.21769296\n",
      "Iteration 274, loss = 0.21730508\n",
      "Iteration 275, loss = 0.21682356\n",
      "Iteration 276, loss = 0.21638072\n",
      "Iteration 277, loss = 0.21595499\n",
      "Iteration 278, loss = 0.21550075\n",
      "Iteration 279, loss = 0.21504756\n",
      "Iteration 280, loss = 0.21460686\n",
      "Iteration 281, loss = 0.21416923\n",
      "Iteration 282, loss = 0.21370011\n",
      "Iteration 283, loss = 0.21324417\n",
      "Iteration 284, loss = 0.21279491\n",
      "Iteration 285, loss = 0.21234596\n",
      "Iteration 286, loss = 0.21188302\n",
      "Iteration 287, loss = 0.21142672\n",
      "Iteration 288, loss = 0.21098779\n",
      "Iteration 289, loss = 0.21049679\n",
      "Iteration 290, loss = 0.21001825\n",
      "Iteration 291, loss = 0.20957693\n",
      "Iteration 292, loss = 0.20910844\n",
      "Iteration 293, loss = 0.20861369\n",
      "Iteration 294, loss = 0.20822978\n",
      "Iteration 295, loss = 0.20766496\n",
      "Iteration 296, loss = 0.20720095\n",
      "Iteration 297, loss = 0.20672435\n",
      "Iteration 298, loss = 0.20624826\n",
      "Iteration 299, loss = 0.20575188\n",
      "Iteration 300, loss = 0.20526819\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(balanceScaleValues, balanceScaleClasses):\n",
    "    data_train, target_train = balanceScaleValues[train], balanceScaleClasses[train]\n",
    "    data_test, target_test = balanceScaleValues[test], balanceScaleClasses[test]\n",
    "\n",
    "    arvore_decisao = arvore_decisao.fit(data_train, target_train)\n",
    "    arvore_decisao_predicted = arvore_decisao.predict(data_test)\n",
    "    predicted_classes['arvore_decisao'][test] = arvore_decisao_predicted\n",
    "\n",
    "    vizinhos_proximos = vizinhos_proximos.fit(data_train, target_train)\n",
    "    vizinhos_proximos_predicted = vizinhos_proximos.predict(data_test)\n",
    "    predicted_classes['vizinhos_proximos'][test] = vizinhos_proximos_predicted\n",
    "\n",
    "    naive_bayes_gaussian = naive_bayes_gaussian.fit(data_train, target_train)\n",
    "    naive_bayes_gaussian_predicted = naive_bayes_gaussian.predict(data_test)\n",
    "    predicted_classes['naive_bayes_gaussian'][test] = naive_bayes_gaussian_predicted\n",
    "\n",
    "    regressao_logistica = regressao_logistica.fit(data_train, target_train)\n",
    "    regressao_logistica_predicted = regressao_logistica.predict(data_test)\n",
    "    predicted_classes['regressao_logistica'][test] = regressao_logistica_predicted\n",
    "\n",
    "    rede_neural = rede_neural.fit(data_train, target_train)\n",
    "    rede_neural_predicted = rede_neural.predict(data_test)\n",
    "    predicted_classes['rede_neural'][test] = rede_neural_predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================\n",
      "Resultados do classificador arvore_decisao\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       576\n",
      "         1.0       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           1.00       625\n",
      "   macro avg       1.00      1.00      1.00       625\n",
      "weighted avg       1.00      1.00      1.00       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[576   0]\n",
      " [  0  49]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador vizinhos_proximos\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97       576\n",
      "         1.0       1.00      0.37      0.54        49\n",
      "\n",
      "    accuracy                           0.95       625\n",
      "   macro avg       0.97      0.68      0.76       625\n",
      "weighted avg       0.95      0.95      0.94       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[576   0]\n",
      " [ 31  18]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador naive_bayes_gaussian\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       576\n",
      "         1.0       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           1.00       625\n",
      "   macro avg       1.00      1.00      1.00       625\n",
      "weighted avg       1.00      1.00      1.00       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[576   0]\n",
      " [  0  49]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador regressao_logistica\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       576\n",
      "         1.0       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           1.00       625\n",
      "   macro avg       1.00      1.00      1.00       625\n",
      "weighted avg       1.00      1.00      1.00       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[576   0]\n",
      " [  0  49]]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "Resultados do classificador rede_neural\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96       576\n",
      "         1.0       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.92       625\n",
      "   macro avg       0.46      0.50      0.48       625\n",
      "weighted avg       0.85      0.92      0.88       625\n",
      "\n",
      "\n",
      "Matriz de confusão: \n",
      "[[576   0]\n",
      " [ 49   0]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for classificador in predicted_classes.keys():\n",
    "    print(\"================================================================================================\")\n",
    "    print(\"Resultados do classificador %s\\n%s\\n\"\n",
    "          %(classificador, metrics.classification_report(balanceScaleClasses, predicted_classes[classificador])))\n",
    "    print(\"Matriz de confusão: \\n%s\\n\\n\\n\" % metrics.confusion_matrix(balanceScaleClasses, predicted_classes[classificador]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}